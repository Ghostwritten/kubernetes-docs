{"Overview.html":{"url":"Overview.html","title":"目录","keywords":"","body":"目录目录 目录 序言 工具 概念 升级 日志 网络 资源 Yum 部署 minikube 手动 对象 Deployment 技巧 开发 策略 原理 DaemonSet Volumes StatefulSet Job Runtimeclass 原理 PodSecurityPolicy ConfigMap Pod Probes SecurityContext secret Ingress Service NetworkPolicy RBAC 原理 Auditing 实战 存储 FlexVolume CSI 编写 CSI Volumes 原理 策略 OPA ResourceQuota QoS LimitRange 组件 Kubeadm 命令 Kube proxy Kubectl 命令 Kubelet 配置 垃圾回收 kube apiserver 安全 Falco default macros 规则 Apparmor 语法 Trivy 扫描应用 自定义策略 标签 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-16 11:19:19 "},"./":{"url":"./","title":"序言","keywords":"","body":"序言参考联系公众号序言 这是一本关于集合 kubernetes 的书。 参考 kubernetes 官网 宋净超的Kubernetes 中文指南 倪朋飞的Kubernetes 指南 深入剖析 Kubernetes 联系 Email: 1zoxun1@gmail.com WeChat: weke59 Youtube: BlackSwanGreen Ins: cnghostwritten Bilibili: LoveDeatRobots 公众号 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"工具/":{"url":"工具/","title":"工具","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"概念/":{"url":"概念/","title":"概念","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"升级/":{"url":"升级/","title":"升级","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"日志/":{"url":"日志/","title":"日志","keywords":"","body":"Kubernetes日志介绍1. 容器日志2. Kubernetes 日志2.1 应用（Pod）级别2.2 节点级别2.3 集群级别3. 日志位置Kubernetes日志介绍 tagsstart log tagsstop 1. 容器日志 Docker的日志分为两类，一类是Docker引擎日志；另一类是容器日志。引擎日志一般都交给了系统日志，不同的操作系统会放在不同的位置。本文主要介绍容器日志，容器日志可以理解是运行在容器内部的应用输出的日志，默认情况下，docker logs显示当前运行的容器的日志信息，内容包含 STOUT（标准输出）和STDERR（标准错误输出）。日志都会以json-file的格式存储于 /var/lib/docker/containers//-json.log，不过这种方式并不适合放到生产环境中。 . 默认方式下容器日志并不会限制日志文件的大小，容器会一直写日志，导致磁盘爆满，影响系统应用。（docker log-driver支持log文件的rotate） Docker Daemon收集容器的标准输出，当日志量过大时会导致Docker Daemon成为日志收集的瓶颈，日志的收集速度受限。 日志文件量过大时，利用docker logs -f查看时会直接将Docker Daemon阻塞住，造成docker ps等命令也不响应。 Docker提供了logging drivers配置，用户可以根据自己的需求去配置不同的log-driver，但是上述配置的日志收集也是通过Docker Daemon收集，收集日志的速度依然是瓶颈。 log-driver 日志收集速度 syslog 14.9 MB/s json-file 37.9 MB/s 能不能找到不通过Docker Daemon收集日志直接将日志内容重定向到文件并自动rotate的工具呢？答案是肯定的采用S6[2]基底镜像。 S6-log将CMD的标准输出重定向到/.../default/current，而不是发送到 Docker Daemon，这样就避免了Docker Daemon收集日志的性能瓶颈 2. Kubernetes 日志 Kubernetes日志收集方案分成三个级别 2.1 应用（Pod）级别 Pod级别的日志，默认是输出到标准输出和标志输入，实际上跟Docker容器的一致。使用kubectl logs pod-name -n namespace查看，具体参考：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs。 2.2 节点级别 Node级别的日志 , 通过配置容器的log-driver来进行管理，这种需要配合logrotare来进行，日志超过最大限制，自动进行rotate操作。 默认情况下，如果容器重启，kubelet 会保留被终止的容器日志。 如果 Pod 在工作节点被驱逐，该 Pod 中所有的容器也会被驱逐，包括容器日志。 节点级日志记录中，需要重点考虑实现日志的轮转，以此来保证日志不会消耗节点上全部可用空间。 Kubernetes 并不负责轮转日志，而是通过部署工具建立一个解决问题的方案。 例如，在用 kube-up.sh 部署的 Kubernetes 集群中，存在一个 logrotate，每小时运行一次。 你也可以设置容器运行时来自动地轮转应用日志。 例如，你可以找到关于 kube-up.sh 为 GCP 环境的 COS 镜像设置日志的详细信息， 脚本为 configure-helper 脚本。 当使用某 CRI 容器运行时 时，kubelet 要负责对日志进行轮换，并 管理日志目录的结构。kubelet 将此信息发送给 CRI 容器运行时，后者 将容器日志写入到指定的位置。kubelet 标志 container-log-max-size 和 container-log-max-files 可以用来配置每个日志文件的最大长度 和每个容器可以生成的日志文件个数上限。 当运行 kubectl logs 时， 节点上的 kubelet 处理该请求并直接读取日志文件，同时在响应中返回日志文件内容。 2.3 集群级别 集群级别的日志收集，有三种。 使用在每个节点上运行的节点级日志记录代理。 在应用程序的 Pod 中，包含专门记录日志的边车（Sidecar）容器。 将日志直接从应用程序中推送到日志记录后端。 2.3.1 节点代理方式 节点代理方式，在Node级别进行日志收集。一般使用DaemonSet部署在每个Node中。这种方式优点是耗费资源少，因为只需部署在节点，且对应用无侵入。缺点是只适合容器内应用日志必须都是标准输出。 logging agent一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。举个例子，我们可以通过 Fluentd 项目作为宿主机上的 logging-agent，然后把日志转发到远端的 ElasticSearch 里保存起来供将来进行检索。 优点：在于一个节点只需要部署一个 agent，并且不会对应用和 Pod 有任何侵入性。所以，这个方案，在社区里是最常用的一种。 缺点：这种方案的不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。 2.3.2 使用 sidecar 容器运行日志代理 也就是在Pod中跟随应用容器起一个日志处理容器，有两种形式： 一种是直接将应用容器的日志收集并输出到标准输出（叫做Streaming sidecar container），但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是sidecar的stdout和stderr对应的JSON文件。这对磁盘是很大的浪费，所以说，除非万不得已或者应用容器完全不可能被修改。 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} 在这种情况下，你用 kubectl logs 命令是看不到应用的任何日志的。而且我们前面讲解的、最常用的方案一，也是没办法使用的。那么这个时候，我们就可以为这个 Pod 添加两个 sidecar 容器，分别将上述两个日志文件里的内容重新以 stdout 和 stderr 的方式输出出来，这个 YAML 文件的写法如下所示： apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} 这时候，你就可以通过 kubectl logs 命令查看这两个 sidecar 容器的日志，间接看到应用的日志内容了，如下所示： kubectl logs counter count-log-1 输出为： 0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001 kubectl logs counter count-log-2 输出为： Mon Jan 1 00:00:00 UTC 2001 INFO 0 Mon Jan 1 00:00:01 UTC 2001 INFO 1 Mon Jan 1 00:00:02 UTC 2001 INFO 2 由于 sidecar 跟主容器之间是共享 Volume 的，所以这里的 sidecar 方案的额外性能损耗并不高，也就是多占用一点 CPU 和内存罢了。 但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件。这对磁盘是很大的浪费。所以说，除非万不得已或者应用容器完全不可能被修改，否则我还是建议你直接使用方案一，或者直接使用下面的第三种方案。 2.3.3 具有日志代理功能的边车容器 就是通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。也就是相当于把方案一里的 logging agent，放在了应用 Pod 里。这种方案的架构如下所示： 在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 logging-agent 还可以使用 fluentd，后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。一般来说，我们会把 fluentd 的输入源配置保存在一个 ConfigMap 里，如下所示： 第一个文件包含用来配置 fluentd 的 ConfigMap。 要进一步了解如何配置 fluentd，请参考 fluentd 官方文档。 apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluentd.conf: | type tail format none path /var/log/1.log pos_file /var/log/1.log.pos tag count.format1 type tail format none path /var/log/2.log pos_file /var/log/2.log.pos tag count.format2 type google_cloud 后，我们在应用 Pod 的定义里，就可以声明一个 Fluentd 容器作为 sidecar，专门负责将应用生成的 1.log 和 2.log 转发到 ElasticSearch 当中。。 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-agent image: k8s.gcr.io/fluentd-gcp:1.30 env: - name: FLUENTD_ARGS value: -c /etc/fluentd-config/fluentd.conf volumeMounts: - name: varlog mountPath: /var/log - name: config-volume mountPath: /etc/fluentd-config volumes: - name: varlog emptyDir: {} - name: config-volume configMap: name: fluentd-config 可以看到，这个 Fluentd 容器使用的输入源，就是通过引用我们前面编写的 ConfigMap 来指定的。这里我用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里。 需要注意的是，这种方案虽然部署简单，并且对宿主机非常友好，但是这个 sidecar 容器很可能会消耗较多的资源，甚至拖垮应用容器。并且，由于日志还是没有输出到 stdout 上，所以你通过 kubectl logs 是看不到任何日志输出的。 3. 日志位置 有两种类型：在容器中运行的和不在容器中运行的。例如： 在容器中运行的 kube-scheduler 和 kube-proxy。 不在容器中运行的 kubelet 和容器运行时。 在使用 systemd 机制的服务器上，kubelet 和容器容器运行时将日志写入到 journald 中。 如果没有 systemd，它们将日志写入到 /var/log 目录下的 .log 文件中。 容器中的系统组件通常将日志写到 /var/log 目录，绕过了默认的日志机制。 他们使用 klog 日志库。 你可以在日志开发文档 找到这些组件的日志告警级别约定。 和容器日志类似，/var/log 目录中的系统组件日志也应该被轮转。 通过脚本 kube-up.sh 启动的 Kubernetes 集群中，日志被工具 logrotate 执行每日轮转，或者日志大小超过 100MB 时触发轮转。 参考： kubernetes system-logs Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 09:35:05 "},"网络/":{"url":"网络/","title":"网络","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"资源/":{"url":"资源/","title":"资源","keywords":"","body":"Kubernetes 资源OverviewKubernetes 资源Overview Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-05 06:41:16 "},"资源/kubernetes-yum.html":{"url":"资源/kubernetes-yum.html","title":"Yum","keywords":"","body":"Kuberentes Yum1. linux yum2. kubernets yumDebian / UbuntuCentOS / RHEL / FedoraKuberentes Yum tagsstart 资源 tagsstop 1. linux yum 阿里云 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 2. kubernets yum Debian / Ubuntu 阿里云 apt-get update && apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl CentOS / RHEL / Fedora 阿里云 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 腾讯 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 查看kubelet kubeadm kubectl版本 yum list kubelet kubeadm kubectl --showduplicates|sort -r 安装指定版本 yum install -y kubelet-1.13.5 kubeadm-1.13.5 kubectl-1.13.5 查看安装的版本 kubeadm version kubectl version kubelet --version Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:57:35 "},"部署/":{"url":"部署/","title":"部署","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-05 06:41:16 "},"部署/kubernetes-minikube-deploy.html":{"url":"部署/kubernetes-minikube-deploy.html","title":"minikube","keywords":"","body":"Ubuntu Minikube deploy Kubernetes1. 关闭swap2. 安装依赖tools3. 添加阿里云镜像4. 安装 Docker、kubectl、kubeadm5. 配置阿里云 Docker 镜像加速器6. 安装 Minikube7. 检查8. 常用命令9.异常9.1 部署异常10. 部署应用程序11. 集群启动与停止Ubuntu Minikube deploy Kubernetes tagsstart kubernetes 部署 minikube tagsstop Life was like a box of chocolates, you never know what you're going to get. 生命就像一盒巧克力，结果往往出人意料 ——《阿甘正传》 1. 关闭swap swapoff -a 2. 安装依赖tools sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common conntrack 由于国内网络的不稳定性，我们需要将相关镜像源切换为国内阿里云的镜像 3. 添加阿里云镜像 curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update 4. 安装 Docker、kubectl、kubeadm #查看docker版本 $ sudo apt-cache madison docker.io docker.io | 19.03.6-0ubuntu1~18.04.3 | http://cn.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages docker.io | 19.03.6-0ubuntu1~18.04.2 | http://cn.archive.ubuntu.com/ubuntu bionic-security/universe amd64 Packages docker.io | 17.12.1-0ubuntu1 | http://cn.archive.ubuntu.com/ubuntu bionic/universe amd64 Packages sudo apt-get install -y docker.io kubectl kubeadm 5. 配置阿里云 Docker 镜像加速器 这里采用了阿里云的镜像加速器（需要阿里云账号进行登录），地址：阿里云 -> 容器镜像服务 -> 镜像工具 -> 镜像加速器 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 以上配置是阿里云镜像加速器中的配置，本文中这一块儿是直接从阿里云镜像加速器中的配置说明复制的，大家根据自己的情况在阿里云镜像加速器中去复制。 6. 安装 Minikube 目前最新 v1.24.0，查看最新版：https://github.com/kubernetes/minikube/releases 如果curl无法下载，也可以通过手动下载并上传到服务器的形式 curl -Lo minikube https://github.com/kubernetes/minikube/releases/download/v1.17.1/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ 使用 Minikube 创建 Kubernetes 格式1 minikube start --vm-driver=none --apiserver-ips= --image-mirror-country cn \\ --iso-url=https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.17.1.iso \\ --registry-mirror=https://xxxxxx.mirror.aliyuncs.com \\ --image-repository=https://registry.aliyuncs.com/google_containers 此处的 registry-mirror 是阿里云的镜像加速器的 mirror，换成你自己的即可; apiserver-ips 则是你服务器的IP，我的是https://q2hy3fzi.mirror.aliyuncs.com，因为如果需要远程访问的话，需要将服务器的IP进行暴露，同样换成你自己的服务器IP即可. 执行命令： minikube start --vm-driver=none --apiserver-ips=192.168.211.55 --image-mirror-country cn --iso-url=https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.17.1.iso --registry-mirror=https://q2hy3fzi.mirror.aliyuncs.com --image-repository=https://registry.aliyuncs.com/google_containers 输出内容： * minikube v1.17.1 on Ubuntu 18.04 * Using the none driver based on existing profile X The requested memory allocation of 1970MiB does not leave room for system overhead (total system memory: 1970MiB). You may face stability issues. * Suggestion: Start minikube with less memory allocated: 'minikube start --memory=1970mb' * Starting control plane node minikube in cluster minikube * Restarting existing none bare metal machine for \"minikube\" ... * OS release is Ubuntu 18.04.5 LTS * Preparing Kubernetes v1.20.2 on Docker 19.03.6 ... - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf - Generating certificates and keys ... - Booting up control plane ... - Configuring RBAC rules ... * Configuring local host environment ... * ! The 'none' driver is designed for experts who need to integrate with an existing VM * Most users should use the newer 'docker' driver instead, which does not require root! * For more information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/ * ! kubectl and minikube configuration will be stored in /root ! To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run: * - sudo mv /root/.kube /root/.minikube $HOME - sudo chown -R $USER $HOME/.kube $HOME/.minikube * * This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true * Verifying Kubernetes components... * Enabled addons: storage-provisioner, default-storageclass * Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default 如果不出意外的话，由 minikube 创建的单机 Kubernetes 环境就成功了。 7. 检查 root@spectre:~# kubectl get nodes NAME STATUS ROLES AGE VERSION spectre Ready control-plane,master 68s v1.20.2 root@spectre:~# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7f89b7bc75-774mp 1/1 Running 0 5m53s kube-system etcd-spectre 1/1 Running 0 6m7s kube-system kube-apiserver-spectre 1/1 Running 0 6m7s kube-system kube-controller-manager-spectre 1/1 Running 0 6m7s kube-system kube-proxy-swbhc 1/1 Running 0 5m53s kube-system kube-scheduler-spectre 1/1 Running 0 6m7s kube-system storage-provisioner 1/1 Running 0 6m6s root@spectre:~# kubectl cluster-info Kubernetes control plane is running at https://192.168.211.55:8443 KubeDNS is running at https://192.168.211.55:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 8. 常用命令 # 进入集群节点 minikube ssh # 查看节点 IP minikube ip # 停止集群 minikube stop # 删除集群 minikube delete 9.异常 9.1 部署异常 storage-provisioner Pod镜像名称错误 解决方法 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 minikube image load registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-minikube/storage-provisioner:v5 10. 部署应用程序 创建一个示例部署并在端口 8080 上公开它： kubectl create deployment hello-minikube --image=registry.aliyuncs.com/google_containers/echoserver:1.4 deployment.apps/hello-minikube created kubectl expose deployment hello-minikube --type=NodePort --port=8080 kubectl get services hello-minikube 11. 集群启动与停止 当我们需要修改docker配置等需要集群停止的需求可以执行命令： minikube stop 当修改完配置 minikube start 参考： kind 部署 kubernetes 集群 Minikube 在ubuntu 部署 Kubernetes Minikube 在 Centos 7 部署 Kubernetes kubeadm 部署 kubernetes 集群 更多Minikube细节请参考官方 kubernetes 快速学习手册 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:58:14 "},"部署/kubernetes-manually-deploy.html":{"url":"部署/kubernetes-manually-deploy.html","title":"手动","keywords":"","body":"kubernetes 手动部署1. 实践环境准备2. 软件版本3. 初始化操作3.1 配置hostname3.2 配置/etc/hosts3.3 关闭防火墙、selinux、swap3.4 加载br_netfilter3.5 配置内核参数3.6 配置CentOS YUM源3.7 安装依赖软件包3.8 时间同步配置3.9 配置节点间ssh互信3.10 初始化环境配置检查4.部署docker4.1 remove旧版本docker4.2 设置docker yum源4.3 列出docker版本4.4 安装docker 指定18.06.14.5 配置镜像加速器和docker数据存放路径4.6 启动docker5 安装kubeadm、kubelet、kubectl5.1 yum 安装 并启动6 镜像下载准备6.1 初始化获取要下载的镜像列表6.2 生成默认kubeadm.conf文件6.3 绕过墙下载镜像方法6.4 指定kubeadm安装的Kubernetes版本6.5 下载需要用到的镜像6.6 docker tag 镜像6.7 查看修改tag后的镜像列表7 部署master节点7.1 kubeadm init 初始化master节点7.2 验证测试8 部署calico网络 （在master上执行）8.1calico官方镜像 下载、标签修改、删除8.2 下载执行rbac-kdd.yaml文件8.3 下载、配置calico.yaml文件8.4 修改配置8.5 部署calico.yaml文件8.6 查看状态9 部署node节点9.1 下载安装镜像（在node上执行）9.2 把node加入集群里 （kubeadm init 生成）10 部署dashboard10.1 生成私钥和证书签名请求10.2 下载dashboard镜像、tag镜像（在全部节点上）10. 3 下载 kubernetes-dashboard.yaml 部署文件（在master上执行）10.4 修改配置10.5 创建dashboard的pod10.6 查看服务运行情况10.7 Dashboard BUG处理kubernetes 手动部署 tagsstart 部署 tagsstop Kubernetes 版本查阅地址： https://github.com/kubernetes/kubernetes/releases 1. 实践环境准备 服务器虚拟机准备 IP地址 节点角色 CPU Memory Hostname 192.168.211.11 master and etcd >=2c >=2G master 192.168.211.12 worker >=2c >=2G node 本实验我这里用的VM是vmware workstation创建的，每个给了2C 2G 20GB配置，大家根据自己的资 源情况，按照上面给的建议最低值创建即可 注意：hostname不能有大写字母，比如Master这样。 2. 软件版本 系统类型：centos 7.4 1708 内核版本: Linux 3.10.0-957.1.3.el7.x86_64 Kubernetes版 本：v1.18.1 docker版本：19.03.5 kubeadm版本：v1.18.1 kubectl版本：Client Version v1.18.1 Server Versionv1.18.0 kubelet版本：v1.18.1 3. 初始化操作 3.1 配置hostname hostnamectl set-hostname master hostnamectl set-hostname node 3.2 配置/etc/hosts cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.211.11 master 192.168.211.12 node EOF 3.3 关闭防火墙、selinux、swap setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config systmectl stop firewalld systmectl disable firewalld swapoff -a sed -i 's/.*swap.*/#&/' /etc/fstab 3.4 加载br_netfilter 平台CLI检查br_netfilter模块是否已加载，如果模块不可用则退出。需要此模块以启用透明伪装并促进虚拟可扩展LAN（VxLAN）流量，以在整个集群中的Kubernetes Pod之间进行通信。如果需要检查是否已加载，请运行： $ sudo lsmod|grep br_netfilter br_netfilter 24576 0 桥155648 2 br_netfilter，ebtable_broute 如果看到类似于所示的输出，则 br_netfilter模块已加载。内核模块通常在需要时加载，因此不太可能需要手动加载此模块。如有必要，您可以手动加载模块并通过运行以下命令将其添加为永久模块： sudo modprobe br_netfilter sudo sh -c 'echo \"br_netfilter\" > /etc/modules-load.d/br_netfilter.conf' 简而言之，开启内核ipv4转发需要加载br_netfilter模块 3.5 配置内核参数 cat > /etc/sysctl.d/k8s.conf > /etc/security/limits.conf echo \"* hard nofile 655360\" >> /etc/security/limits.conf echo \"* soft nproc 655360\" >> /etc/security/limits.conf echo \"* hard nproc 655360\" >> /etc/security/limits.conf echo \"* soft memlock unlimited\" >> /etc/security/limits.conf echo \"* hard memlock unlimited\" >> /etc/security/limits.conf echo \"DefaultLimitNOFILE=1024000\" >> /etc/systemd/system.conf echo \"DefaultLimitNPROC=1024000\" >> /etc/systemd/system.conf 执行看下最大文件打开数是否是655360 ，如果不是，尝试重新连接终端。 ulimit -Hn 3.6 配置CentOS YUM源 配置国内tencent yum源地址、epel源地址、国内Kubernetes源地址 yum install -y wget rm -rf /etc/yum.repos.d/* wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 3.7 安装依赖软件包 yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp bash-completion yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools vim libtool-ltdl 3.8 时间同步配置 Kubernetes是分布式的，各个节点系统时间需要同步对应上。 yum -y install chrony systemctl enable chronyd.service && systemctl start chronyd.service && systemctl status chronyd.service chronyc sources 运行date命令看下系统时间，过一会儿时间就会同步 [ ] 时间如果不同步会遇到什么？ 3.9 配置节点间ssh互信 配置ssh互信，那么节点之间就能无密访问，方便日后执行自动化部署 ssh-keygen # 每台机器执行这个命令， 一路回车即可 ssh-copy-id node # 到master上拷贝公钥到其他节点，这里需要输入 yes和密码 [ ] 如果只是master与其他节点做了互信，会有影响吗？ 3.10 初始化环境配置检查 重启，做完以上所有操作，最好reboot重启一遍 ping 每个节点hostname 看是否能ping通 ssh 对方hostname看互信是否无密码访问成功 执行date命令查看每个节点时间是否正确 执行 ulimit -Hn 看下最大文件打开数是否是655360 cat /etc/sysconfig/selinux |grep disabled 查看下每个节点selinux是否都是disabled状态 4.部署docker 4.1 remove旧版本docker yum remove -y docker docker-ce docker-common docker-selinux docker-engine 4.2 设置docker yum源 yum-config-manager --add-repo https://download.docker.com/linux/centos/dockerce.repo 4.3 列出docker版本 yum list docker-ce --showduplicates | sort -r 4.4 安装docker 指定18.06.1 yum install -y docker-ce-19.03.5-3.el7.x86_64 4.5 配置镜像加速器和docker数据存放路径 tee /etc/docker/daemon.json 4.6 启动docker systemctl daemon-reload && systemctl restart docker && systemctl enable docker && systemctl status docker 5 安装kubeadm、kubelet、kubectl 这一步是所有节点都得安装（包括node节点） 工具说明 kubeadm: 部署集群用的命令 kubelet: 在集群中每台机器上都要运行的组件，负责管理pod、容器的生命周期 kubectl: 集群管理工具 5.1 yum 安装 并启动 yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable kubelet && systemctl start kubelet 注意：kubelet 服务会暂时启动不了，先不用管它。 6 镜像下载准备 6.1 初始化获取要下载的镜像列表 使用kubeadm来搭建Kubernetes，那么就需要下载得到Kubernetes运行的对应基础镜像，比如：kubeproxy、kube-apiserver、kube-controller-manager等等 。那么有什么方法可以得知要下载哪些镜像 呢？从kubeadm v1.11+版本开始，增加了一个kubeadm config print-default 命令，可以让我们方便 的将kubeadm的默认配置输出到文件中，这个文件里就包含了搭建K8S对应版本需要的基础配置环境。另 外，我们也可以执行 kubeadm config images list 命令查看依赖需要安装的镜像列表。 $ kubeadm config images list W0417 00:57:50.788202 129664 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] k8s.gcr.io/kube-apiserver:v1.18.1 k8s.gcr.io/kube-controller-manager:v1.18.1 k8s.gcr.io/kube-scheduler:v1.18.1 k8s.gcr.io/kube-proxy:v1.18.1 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.3-0 k8s.gcr.io/coredns:1.6.7 注意：这个列表显示的tag名字和镜像版本号，从Kubernetes v1.12+开始，镜像名后面不带 amd64, arm, arm64, ppc64le 这样的标识了 6.2 生成默认kubeadm.conf文件 $ kubeadm config print init-defaults > kubeadm.conf 6.3 绕过墙下载镜像方法 注意这个配置文件默认会从google的镜像仓库地址k8s.gcr.io下载镜像，如果你没有科学上网，那么就会 下载不来。因此，我们通过下面的方法把地址改成国内的，比如用阿里的： sed -i \"s/imageRepository: .*/imageRepository: registry.aliyuncs.com\\/google_containers/g\" kubeadm.conf 6.4 指定kubeadm安装的Kubernetes版本 sed -i \"s/kubernetesVersion: .*/kubernetesVersion: v1.18.1/g\" kubeadm.conf 6.5 下载需要用到的镜像 kubeadm.conf修改好后，我们执行下面命令就可以自动从国内下载需要用到的镜像了： $ kubeadm config images pull --config kubeadm.conf [config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.18.1 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.18.1 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.18.1 [config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.18.1 [config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.2 [config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.3-0 [config/images] Pulled registry.aliyuncs.com/google_containers/coredns:1.6.7 自动下载v1.81需要用到的镜像，执行 docker images 可以看到下载好的镜像列表 $ docker images registry.aliyuncs.com/google_containers/kube-proxy v1.18.1 4e68534e24f6 8 days ago 117MB registry.aliyuncs.com/google_containers/kube-controller-manager v1.18.1 d1ccdd18e6ed 8 days ago 162MB registry.aliyuncs.com/google_containers/kube-apiserver v1.18.1 a595af0107f9 8 days ago 173MB registry.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 2 months ago 683kB registry.aliyuncs.com/google_containers/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB registry.aliyuncs.com/google_containers/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB 6.6 docker tag 镜像 镜像下载好后，我们还需要tag下载好的镜像，让下载好的镜像都是带有 k8s.gcr.io 标识的，目前我们从阿 里下载的镜像 标识都是，如果不打tag变成k8s.gcr.io，那么后面用kubeadm安装会出现问题，因为 kubeadm里面只认 google自身的模式。我们执行下面命令即可完成tag标识更换： $ cat tag.sh #!/bin/bash newtag=k8s.gcr.io for i in $(docker images | grep -v TAG |awk '{print $1 \":\" $2}') do image=$(echo $i | awk -F '/' '{print $3}') docker tag $i $newtag/$image docker rmi $i done $ bash tag.sh 6.7 查看修改tag后的镜像列表 k8s.gcr.io/kube-proxy v1.18.1 4e68534e24f6 8 days ago 117MB k8s.gcr.io/kube-controller-manager v1.18.1 d1ccdd18e6ed 8 days ago 162MB k8s.gcr.io/kube-apiserver v1.18.1 a595af0107f9 8 days ago 173MB k8s.gcr.io/kube-scheduler v1.18.1 6c9320041a7b 8 days ago 95.3MB k8s.gcr.io/pause 3.2 80d28bedfe5d 2 months ago 683kB k8s.gcr.io/coredns 1.6.7 67da37a9a360 2 months ago 43.8MB k8s.gcr.io/etcd 3.4.3-0 303ce5db0e90 5 months ago 288MB 注意：另外节点，重复上面的操作下载即可。 7 部署master节点 7.1 kubeadm init 初始化master节点 kubeadm init --kubernetes-version=v1.18.1 --pod-network-cidr=172.22.0.0/16 --apiserver-advertise-address=192.168.211.11 这里我们定义POD的网段为: 172.22.0.0/16 ，然后api server地址就是master本机IP地址 7.1.1 初始化报错 1.Switch cgroup driver, from cgroupfs to systemd [root@master ~]# kubeadm init --kubernetes-version=v1.18.0 --pod-network-cidr=172.22.0.0/16 --apiserver-advertise-address=192.168.211.11 W0416 17:58:21.037057 11830 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.0 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2 [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher 解决方法： $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], #添加此行 \"registry-mirrors\": [\"https://q2hy3fzi.mirror.aliyuncs.com\"], \"graph\": \"/tol/docker-data\" } $ systemctl restart docker 7.1.2 初始化成功 $ kubeadm init --kubernetes-version=v1.18.0 --pod-network-cidr=172.22.0.0/16 --apiserver-advertise-address=192.168.211.11 W0416 18:07:20.031658 12701 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.211.11] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.211.11 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.211.11 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" W0416 18:07:25.968262 12701 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" W0416 18:07:25.969958 12701 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 20.507370 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.18\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: 16l83a.e1tpcgkmze0i3fuy [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: #保存初始化node用到的命令 kubeadm join 192.168.211.11:6443 --token 16l83a.e1tpcgkmze0i3fuy \\ --discovery-token-ca-cert-hash sha256:233a3d9c6c0ed466642c08293e0bf2bb217359d414d3ccb0bf25afa1c00b7ca3 7.2 验证测试 配置kubectl命令 mkdir -p /root/.kube cp /etc/kubernetes/admin.conf /root/.kube/config 获取pods列表命令 其中coredns pod处于Pending状态，这个先不管 kubectl get pods --all-namespaces 查看集群的健康状态： $ kubectl get cs etcd-0 Healthy {\"health\":\"true\"} controller-manager Healthy ok scheduler Healthy ok 8 部署calico网络 （在master上执行） calico官网 calico详细介绍 calico介绍：Calico是一个纯三层的方案，其好处是它整合了各种云原生平台(Docker、Mesos 与 OpenStack 等)，每个 Kubernetes 节点上通过 Linux Kernel 现有的 L3 forwarding 功能来实现 vRouter 功能。 8.1calico官方镜像 下载、标签修改、删除 docker pull calico/node:v3.1.4 docker pull calico/cni:v3.1.4 docker pull calico/typha:v3.1.4 docker tag calico/node:v3.1.4 quay.io/calico/node:v3.1.4 docker tag calico/cni:v3.1.4 quay.io/calico/cni:v3.1.4 docker tag calico/typha:v3.1.4 quay.io/calico/typha:v3.1.4 docker rmi calico/node:v3.1.4 docker rmi calico/cni:v3.1.4 docker rmi calico/typha:v3.1.4 8.2 下载执行rbac-kdd.yaml文件 curl https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml -O kubectl apply -f rbac-kdd.yaml 8.3 下载、配置calico.yaml文件 curl https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/policy-only/1.7/calico.yaml -O 8.4 修改配置 8.4.1 修改ConfigMap 把ConfigMap 下的 typha_service_name 值由none变成 calico-typha kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # You must set a non-zero value for Typha replicas below. typha_service_name: \"calico-typha\" 8.4.2 修改Deployment 设置 Deployment 类目的 spec 下的replicas值 apiVersion: apps/v1 kind: Deployment metadata: name: calico-typha namespace: kube-system labels: k8s-app: calico-typha spec: # Number of Typha replicas. To enable Typha, set this to a non-zero value *and* set the # typha_service_name variable in the calico-config ConfigMap above. # # We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is essential # (when using the Kubernetes datastore). Use one replica for every 100-200 nodes. In # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade. replicas: 1 revisionHistoryLimit: 2 8.4.3 修改 定义POD网段 我们找到CALICO_IPV4POOL_CIDR，然后值修改成之前定义好的POD网段，我这里是172.22.0.0/16 - name: CALICO_IPV4POOL_CIDR value: \"172.22.0.0/16\" 8.4.4 开启bird模式 把 CALICO_NETWORKING_BACKEND 值设置为 bird ，这个值是设置BGP网络后端模式 - name: CALICO_NETWORKING_BACKEND value: \"bird\" 8.5 部署calico.yaml文件 上面参数设置调优完毕，我们执行下面命令彻底部署calico kubectl apply -f calico.yaml 8.6 查看状态 kubectl get pods --all-namespaces 这里calico-typha 没起来，那是因为我们的node 计算节点还没启动和安装。 9 部署node节点 9.1 下载安装镜像（在node上执行） node上也是需要下载安装一些镜像的，需要下载的镜像为：kube-proxy:v1.13、pause:3.1、caliconode:v3.1.4、calico-cni:v3.1.4、calico-typha:v3.1.4 docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.13.0 docker pull registry.aliyuncs.com/google_containers/pause:3.1 docker pull calico/node:v3.1.4 docker pull calico/cni:v3.1.4 docker pull calico/typha:v3.1.4 docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.13.0 k8s.gcr.io/kubeproxy:v1.13.0 docker tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1 docker tag calico/node:v3.1.4 quay.io/calico/node:v3.1.4 docker tag calico/cni:v3.1.4 quay.io/calico/cni:v3.1.4 docker tag calico/typha:v3.1.4 quay.io/calico/typha:v3.1.4 docker rmi registry.aliyuncs.com/google_containers/kube-proxy:v1.13.0 docker rmi registry.aliyuncs.com/google_containers/pause:3.1 docker rmi calico/node:v3.1.4 docker rmi calico/cni:v3.1.4 docker rmi calico/typha:v3.1.4 9.2 把node加入集群里 （kubeadm init 生成） [root@node ~]# kubeadm join 192.168.211.11:6443 --token 16l83a.e1tpcgkmze0i3fuy \\ > --discovery-token-ca-cert-hash sha256:233a3d9c6c0ed466642c08293e0bf2bb217359d414d3ccb0bf25afa1c00b7ca3 W0416 21:07:26.704823 21506 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set. [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.18\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 两个节点运行的参数命令一样，运行完后，我们在master节点上运行 kubectl get nodes 命令查看node 是否正常 kubectl get nodes 到此，集群的搭建完成了90%，剩下一个是搭建dashboard 10 部署dashboard 部署dashboard之前，我们需要生成证书，不然后面会https访问登录不了。 10.1 生成私钥和证书签名请求 [root@master ~]# mkdir -p /etc/kubernetes/certs [root@master ~]# cd /etc/kubernetes/certs [root@master certs]# ls [root@master certs]# openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048 Generating RSA private key, 2048 bit long modulus .................+++ ..................................+++ e is 65537 (0x10001) [root@master certs]# ll 总用量 4 -rw-r--r--. 1 root root 1751 4月 16 21:15 dashboard.pass.key [root@master certs]# openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.key writing RSA key [root@master certs]# ll 总用量 8 -rw-r--r--. 1 root root 1679 4月 16 21:16 dashboard.key -rw-r--r--. 1 root root 1751 4月 16 21:15 dashboard.pass.key [root@master certs]# rm -rf dashboard.pass.key [root@master certs]# openssl req -new -key dashboard.key -out dashboard.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]: State or Province Name (full name) []: Locality Name (eg, city) [Default City]: Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server's hostname) []: Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: [root@master certs]# ll 总用量 8 -rw-r--r--. 1 root root 952 4月 16 21:18 dashboard.csr -rw-r--r--. 1 root root 1679 4月 16 21:16 dashboard.key 生成SSL证书 openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt dashboard.crt文件是适用于仪表板和dashboard.key私钥的证书。 $ ll /etc/kubernetes/certs/ 总用量 12 -rw-r--r--. 1 root root 1103 4月 16 21:20 dashboard.crt -rw-r--r--. 1 root root 952 4月 16 21:18 dashboard.csr -rw-r--r--. 1 root root 1679 4月 16 21:16 dashboard.key 创建secret kubectl create secret generic kubernetes-dashboard-certs --from-file=/etc/kubernetes/certs -n kube-system 注意/etc/kubernetes/certs 是之前创建crt、csr、key 证书文件存放的路径 10.2 下载dashboard镜像、tag镜像（在全部节点上） $ vim dashboard.sh #!/bin/bash tag=v2.0.0-rc7 docker pull registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:$tag docker tag registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v$tag k8s.gcr.io/kubernetes-dashboard:$tag docker rmi registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:$tag $ bash dashboard.sh 10. 3 下载 kubernetes-dashboard.yaml 部署文件（在master上执行） curl https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O 10.4 修改配置 10.4.1 修改镜像版本 sed -i \"s/kubernetes-dashboard-amd64:v1.10.0/kubernetes-dashboard:$tag/g\" kubernetes-dashboard.yaml 10.4.2 把Secret 注释 因为上面我们已经生成了密钥认证了，我们用我们自己生成的。 #apiVersion: v1 #kind: Secret #metadata: # labels: # k8s-app: kubernetes-dashboard # name: kubernetes-dashboard-certs # namespace: kubernetes-dashboard #type: Opaque # #--- 10.4.3 配置443端口映射到外部主机30005上 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30005 #添加 type: NodePort #添加 selector: k8s-app: kubernetes-dashboard 10.5 创建dashboard的pod kubectl create -f kubernetes-dashboard.yaml 10.6 查看服务运行情况 kubectl get deployment kubernetes-dashboard -n kube-system kubectl --namespace kubesystem get pods -o wide kubectl get services kubernetes-dashboard -n kube-system netstat -ntlp|grep 30005 10.7 Dashboard BUG处理 点击跳过 $ vim kube-dashboard-access.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 执行让kube-dashboard-access.yaml kubectl create -f kube-dashboard-access.yaml 然后重新登录界面刷新下，解决问题，如果产生一下问题。 问题 Server Error 404. It will give you 3 seconds to redirect back to the login page. Kubernetes dashboard the server could not find the requested resource 问题原因：dashboard版本过低所致。 将dashboard版本升级 Deleted version 1.10.1 and switched to v2.0.0-beta8 然后重新登录界面刷新下。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:58:03 "},"对象/":{"url":"对象/","title":"对象","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"对象/Deployment/":{"url":"对象/Deployment/","title":"Deployment","keywords":"","body":"Kubernetes Deployment Overview1. 简介2. 创建 Deployment2.1 编排yaml2.2 定义描述2.3 运行与查看2.4 Pod-template-hash label3. 更新 Deployment3.1 Rollover（多个 rollout 并行）Label selector 更新4. 回滚 Deployment4.1 检查 Deployment 上线历史4.2 回滚到之前的修订版本5. 缩放 Deployment5.1 比例缩放6. 暂停、恢复 Deployment7. Deployment 状态7.1 进行中的 Deployment(Progressing)7.2 完成的 Deployment(Complete)7.3 失败的 Deployment(Failed)8. 编排 Deployment8.1 Pod template8.2 Replicas8.3 spec.selector8.4 策略8.5 进度期限秒数8.6 最短就绪时间8.7 Rollback To8.8 Revision8.9 修订历史限制8.10 paused（暂停的）Kubernetes Deployment Overview tagsstart Deployment tagsstop Deployment 结构示意图 deployment排查拓扑图 1. 简介 一个 Deployment 为 Pods 和 ReplicaSets 提供声明式的更新能力。 你负责描述 Deployment 中的 目标状态，而 Deployment 控制器（Controller） 以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。 2. 创建 Deployment 下面是 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 nginx Pods： 2.1 编排yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 2.2 定义描述 创建名为 nginx-deployment（由 .metadata.name 字段标明）的 Deployment。 该 Deployment 创建三个（由 replicas 字段标明）Pod 副本。 selector 字段定义 Deployment 如何查找要管理的 Pods。 在这里，你选择在 Pod 模板中定义的标签（app: nginx）。 不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。 spec.selector.matchLabels 字段是 {key,value} 键值对映射。 在 matchLabels 映射中的每个 {key,value} 映射等效于 matchExpressions 中的一个元素， 即其 key 字段是 “key”，operator 为 “In”，values 数组仅包含 “value”。 在 matchLabels 和 matchExpressions 中给出的所有条件都必须满足才能匹配。 template 字段包含以下子字段： Pod 被使用 labels 字段打上 app: nginx 标签。 Pod 模板规约（即 .template.spec 字段）指示 Pods 运行一个 nginx 容器， 该容器运行版本为 1.14.2 的 nginx Docker Hub镜像。 创建一个容器并使用 name 字段将其命名为 nginx。 2.3 运行与查看 $ kubectl apply -f nginx-deployment.yaml $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 说明： 你可以设置 --record 标志将所执行的命令写入资源注解 kubernetes.io/change-cause 中。这对于以后的检查是有用的。例如，要查看针对每个 Deployment 修订版本所执行过的命令。 在检查集群中的 Deployment 时，所显示的字段有： NAME 列出了集群中 Deployment 的名称。 READY 显示应用程序的可用的 副本 数。显示的模式是“就绪个数/期望个数”。 UP-TO-DATE 显示为了达到期望状态已经更新的副本数。 AVAILABLE 显示应用可供用户使用的副本数。 AGE 显示应用程序运行的时间。 请注意期望副本数是根据 .spec.replicas 字段设置 3。 查看 Deployment 上线状态 $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \"nginx-deployment\" successfully rolled out 几秒钟后再次运行 #注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板） 并且可用 $ kubectl get deployments AME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 18s 查看 Deployment 创建的 ReplicaSet（rs) $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ReplicaSet 输出中包含以下字段： NAME 列出名字空间中 ReplicaSet 的名称； DESIRED 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态； CURRENT 显示当前运行状态中的副本个数； READY 显示应用中有多少副本可以为用户提供服务； AGE 显示应用已经运行的时间长度。 注意 ReplicaSet 的名称始终被格式化为[Deployment名称]-[随机字符串]。 其中的随机字符串是使用 pod-template-hash 作为种子随机生成的。 Deployment 控制器将 pod-template-hash 标签添加到 Deployment 所创建或收留的 每个 ReplicaSet 。此标签可确保 Deployment 的子 ReplicaSets 不重叠。 标签是通过对 ReplicaSet 的 PodTemplate 进行哈希处理。 所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet 可能拥有的任何现有 Pod 中。 要查看每个 Pod 自动生成的标签，运行 $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 $ k expose deployment nginx-deployment 80 $ k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 51d nginx-deployment ClusterIP 10.101.106.248 80/TCP 8s $ curl 10.101.106.248:80 Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 2.4 Pod-template-hash label 注意：这个 label 不是用户指定的！ 注意上面示例输出中的 pod label 里的 pod-template-hash label。当 Deployment 创建或者接管 ReplicaSet 时，Deployment controller 会自动为 Pod 添加 pod-template-hash label。这样做的目的是防止 Deployment 的子 ReplicaSet 的 pod 名字重复。通过将 ReplicaSet 的 PodTemplate 进行哈希散列，使用生成的哈希值作为 label 的值，并添加到 ReplicaSet selector 里、 pod template label 和 ReplicaSet 管理中的 Pod 上。 3. 更新 Deployment 说明： 仅当 Deployment Pod 模板（即 .spec.template）发生改变时，例如模板的标签或容器镜像被更新， 才会触发Deployment 上线。 其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。 按照以下步骤更新 Deployment： 先来更新 nginx Pod 以使用 nginx:1.16.1 镜像，而不是 nginx:1.14.2 镜像。 $ kubectl --record deployment.apps/nginx-deployment set image \\ deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 deployment.apps/nginx-deployment image updated 或者 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record deployment.apps/nginx-deployment image updated 或者，可以 edit Deployment 并将 .spec.template.spec.containers[0].image 从 nginx:1.14.2 更改至 nginx:1.16.1。 $ kubectl edit deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment edited 要查看上线状态，运行： $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... or deployment \"nginx-deployment\" successfully rolled out $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 36s $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s $ kubectl describe deployments Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Environment: Mounts: Volumes: Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板即可。 Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods 75% 处于运行状态（最大不可用比例为 25%）。 Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。 默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。 例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods， 并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现。 在足够数量的旧 Pods 被杀死前并没有创建新 Pods。它确保至少 2 个 Pod 可用，同时 最多总共 4 个 Pod 可用。 可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（nginx-deployment-2035384211） 并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet （nginx-deployment-1564180365），并将其扩容为 1，然后将旧 ReplicaSet 缩容到 2， 以便至少有 2 个 Pod 可用且最多创建 4 个 Pod。 然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。 最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。 3.1 Rollover（多个 rollout 并行） 每当 Deployment controller 观测到有新的 deployment 被创建时，如果没有已存在的 ReplicaSet 来创建期望个数的 Pod 的话，就会创建出一个新的 ReplicaSet 来做这件事。已存在的 ReplicaSet 控制 label 与 .spec.selector 匹配但是 template 跟 .spec.template 不匹配的 Pod 缩容。最终，新的 ReplicaSet 将会扩容出 .spec.replicas 指定数目的 Pod，旧的 ReplicaSet 会缩容到 0。 如果您更新了一个的已存在并正在进行中的 Deployment，每次更新 Deployment 都会创建一个新的 ReplicaSet 并扩容它，同时回滚之前扩容的 ReplicaSet —— 将它添加到旧的 ReplicaSet 列表中，开始缩容。 例如，假如您创建了一个有 5 个 niginx:1.7.9 replica 的 Deployment，但是当还只有 3 个 nginx:1.7.9 的 replica 创建出来的时候您就开始更新含有 5 个 nginx:1.9.1 replica 的 Deployment。在这种情况下，Deployment 会立即杀掉已创建的 3 个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod。它不会等到所有的 5 个 nginx:1.7.9 的 Pod 都创建完成后才开始改变航道。 Label selector 更新 我们通常不鼓励更新 label selector，我们建议事先规划好您的 selector。 任何情况下，只要您想要执行 label selector 的更新，请一定要谨慎并确认您已经预料到所有可能因此导致的后果。 增添 selector 需要同时在 Deployment 的 spec 中更新新的 label，否则将返回校验错误。此更改是不可覆盖的，这意味着新的 selector 不会选择使用旧 selector 创建的 ReplicaSet 和 Pod，从而导致所有旧版本的 ReplicaSet 都被丢弃，并创建新的 ReplicaSet。 更新 selector，即更改 selector key 的当前值，将导致跟增添 selector 同样的后果。 删除 selector，即删除 Deployment selector 中的已有的 key，不需要对 Pod template label 做任何更改，现有的 ReplicaSet 也不会成为孤儿，但是请注意，删除的 label 仍然存在于现有的 Pod 和 ReplicaSet 中。 4. 回滚 Deployment 有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。 默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚 （你可以通过修改修订历史记录限制来更改这一约束）。 Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。 这意味着仅当 Deployment 的 Pod 模板（.spec.template）发生更改时，才会创建新修订版本 -- 例如，模板的标签或容器镜像发生变化。 其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。 这是为了方便同时执行手动缩放或自动缩放。 换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。 假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为 nginx:1.161 而不是 nginx:1.16.1： $ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true deployment.apps/nginx-deployment image updated 此上线进程会出现停滞。你可以通过检查上线状态来验证： $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 1 out of 3 new replicas have been updated... 你可以看到旧的副本有两个（nginx-deployment-1564180365 和 nginx-deployment-2035384211）， 新的副本有 1 个（nginx-deployment-3066724191）： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s 查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中 $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s $ kubectl describe deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.91 Port: 80/TCP Host Port: 0/TCP Environment: Mounts: Volumes: Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 说明： Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。 这行为取决于所指定的rollingUpdate 参数（具体为 maxUnavailable）。 默认情况下，Kubernetes 将此值设置为 25%。 4.1 检查 Deployment 上线历史 首先，检查 Deployment 修订历史： kubectl rollout history deployment.v1.apps/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true CHANGE-CAUSE 的内容是从 Deployment 的 kubernetes.io/change-cause 注解复制过来的。 复制动作发生在修订版本创建时。你可以通过以下方式设置 CHANGE-CAUSE 消息： 使用 kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.9.1\" 为 Deployment添加注解。 追加 --record 命令行标志以保存正在更改资源的 kubectl 命令。 手动编辑资源的清单。 要查看修订历史的详细信息，运行： $ kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2 deployments \"nginx-deployment\" revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: No volumes. 4.2 回滚到之前的修订版本 按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。 假定现在你已决定撤消当前上线并回滚到以前的修订版本： $ kubectl rollout undo deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment 或者，你也可以通过使用 --to-revision 来回滚到特定修订版本： $ kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2 deployment.apps/nginx-deployment 现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment 控制器生成了 回滚到修订版本 2 的 DeploymentRollback 事件。 检查回滚是否成功以及 Deployment 是否正在运行，运行： $ kubectl get deployment nginx-deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 30m $ kubectl describe deployment nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment: Mounts: Volumes: Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment \"nginx-deployment\" to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0 5. 缩放 Deployment $ kubectl scale deployment.v1.apps/nginx-deployment --replicas=10 deployment.apps/nginx-deployment scaled 假设集群启用了Pod 的水平自动缩放， 你可以为 Deployment 设置自动缩放器，并基于现有 Pods 的 CPU 利用率选择 要运行的 Pods 个数下限和上限。 $ kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80 deployment.apps/nginx-deployment scaled 5.1 比例缩放 RollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。 当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时， Deployment 控制器会平衡现有的活跃状态的 ReplicaSets（含 Pods 的 ReplicaSets）中的额外副本， 以降低风险。这称为 比例缩放（Proportional Scaling）。 例如，你正在运行一个 10 个副本的 Deployment，其 maxSurge=3，maxUnavailable=2。 $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE VAILABLE AGE nginx-deployment 10 10 10 10 50s 更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。 $ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag deployment.apps/nginx-deployment image updated 镜像更新使用 ReplicaSet nginx-deployment-1989198191 启动新的上线过程， 但由于上面提到的 maxUnavailable 要求，该进程被阻塞了。检查上线状态： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m 然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。 Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本 都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。 较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到 副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。 具有零副本的 ReplicaSets 不会被扩容。 在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。 假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。 要确认这一点，请运行： $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m 上线状态确认了副本是如何被添加到每个 ReplicaSet 的。 $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m 6. 暂停、恢复 Deployment 你可以在触发一个或多个更新之前暂停 Deployment，然后再恢复其执行。 这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。 例如，对于一个刚刚创建的 Deployment： 获取 Deployment 信息： $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m 使用如下指令暂停运行： $ kubectl rollout pause deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment paused 接下来更新 Deployment 镜像： $ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 deployment.apps/nginx-deployment image updated 注意没有新的上线被触发： $ kubectl rollout history deployment.v1.apps/nginx-deployment deployments \"nginx\" REVISION CHANGE-CAUSE 1 获取上线状态确保 Deployment 更新已经成功： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m 你可以根据需要执行很多更新操作，例如，可以要使用的资源： $ kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi deployment.apps/nginx-deployment resource requirements updated 暂停 Deployment 之前的初始状态将继续发挥作用，但新的更新在 Deployment 被 暂停期间不会产生任何效果。 最终，恢复 Deployment 执行并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新： $ kubectl rollout resume deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment resumed #观察上线的状态 $ kubectl get rs -w NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s #获取最近上线的状态 $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s 说明： 你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。 7. Deployment 状态 Deployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于 Progressing（进行中），可能是 Complete（已完成），也可能是 Failed（失败）以至于无法继续进行。 7.1 进行中的 Deployment(Progressing) 执行下面的任务期间，Kubernetes 标记 Deployment 为 进行中（Progressing）： Deployment 创建新的 ReplicaSet Deployment 正在为其最新的 ReplicaSet 扩容 Deployment 正在为其旧有的 ReplicaSet(s) 缩容 新的 Pods 已经就绪或者可用（就绪至少持续了 MinReadySeconds 秒）。 你可以使用 kubectl rollout status 监视 Deployment 的进度。 7.2 完成的 Deployment(Complete) 当 Deployment 具有以下特征时，Kubernetes 将其标记为 完成（Complete）： 与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。 与 Deployment 关联的所有副本都可用。 未运行 Deployment 的旧副本。 你可以使用 kubectl rollout status 检查 Deployment 是否已完成。 如果上线成功完成，kubectl rollout status 返回退出代码 0。 $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment \"nginx-deployment\" successfully rolled out $ echo $? 0 7.3 失败的 Deployment(Failed) 你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫(Failed)，一直处于未完成状态。 造成此情况一些可能因素如下： 配额（Quota）不足 就绪探测（Readiness Probe）失败 镜像拉取错误 权限不足 限制范围（Limit Ranges）问题 应用程序运行时的配置错误 检测此状况的一种方法是在 Deployment 规约中指定截止时间参数： （[.spec.progressDeadlineSeconds]（#progress-deadline-seconds））。 .spec.progressDeadlineSeconds 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态） 标示 Deployment 进展停滞之前，需要等待所给的时长。 以下 kubectl 命令设置规约中的 progressDeadlineSeconds，从而告知控制器 在 10 分钟后报告 Deployment 没有进展： $ kubectl patch deployment.v1.apps/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}' deployment.apps/nginx-deployment patched 超过截止时间后，Deployment 控制器将添加具有以下属性的 DeploymentCondition 到 Deployment 的 .status.conditions 中： Type=Progressing Status=False Reason=ProgressDeadlineExceeded 参考 Kubernetes API 约定 获取更多状态状况相关的信息。 除了报告 Reason=ProgressDeadlineExceeded 状态之外，Kubernetes 对已停止的 Deployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。 例如，将 Deployment 回滚到其以前的版本。 如果你暂停了某个 Deployment，Kubernetes 不再根据指定的截止时间检查 Deployment 进展。 你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。 Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短， 也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。 如果描述 Deployment，你将会注意到以下部分： $ kubectl describe deployment nginx-deployment Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate $ kubectl get deployment nginx-deployment -o yaml status: availableReplicas: 2 conditions: - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set \"nginx-deployment-4262182780\" is progressing. reason: ReplicaSetUpdated status: \"True\" type: Progressing - lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2' reason: FailedCreate status: \"True\" type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因： Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate 可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额 来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作， Deployment 状态会更新为成功状况（Status=True and Reason=NewReplicaSetAvailable）。 Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable Type=Available 加上 Status=True 意味着 Deployment 具有最低可用性。 最低可用性由 Deployment 策略中的参数指定。 Type=Progressing 加上 Status=True 表示 Deployment 处于上线过程中，并且正在运行， 或者已成功完成进度，最小所需新副本处于可用。 请参阅对应状况的 Reason 了解相关细节。 在我们的案例中 Reason=NewReplicaSetAvailable 表示 Deployment 已完成。 你可以使用 kubectl rollout status 检查 Deployment 是否未能取得进展。 如果 Deployment 已超过进度限期，kubectl rollout status 返回非零退出代码。 $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment \"nginx\" exceeded its progress deadline $ echo $? 1 8. 编排 Deployment 8.1 Pod template .spec 中只有 .spec.template 和 .spec.selector 是必需的字段 .spec.template 是一个 Pod 模板。它和 Pod 的语法规则完全相同。 只是这里它是嵌套的，因此不需要 apiVersion 或 kind。 除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。对于标签，请确保不要与其他控制器重叠。请参考选择算符。 8.2 Replicas .spec.replicas 是指定所需 Pod 的可选字段。它的默认值是1 8.3 spec.selector .spec.selector 是指定本 Deployment 的 Pod标签选择算符的必需字段。 .spec.selector 必须匹配 .spec.template.metadata.labels，否则请求会被 API 拒绝。 在 API apps/v1版本中，.spec.selector 和 .metadata.labels 如果没有设置的话， 不会被默认设置为 .spec.template.metadata.labels，所以需要明确进行设置。 同时在apps/v1版本中，Deployment 创建后 .spec.selector 是不可变的 8.4 策略 .spec.strategy 策略指定用于用新 Pods 替换旧 Pods 的策略。 .spec.strategy.type 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。 重新创建 Deployment 如果 .spec.strategy.type==Recreate，在创建新 Pods 之前，所有现有的 Pods 会被杀死。 滚动更新 Deployment Deployment 会在 .spec.strategy.type==RollingUpdate时，采取 滚动更新的方式更新 Pods。你可以指定 maxUnavailable 和 maxSurge 来控制滚动更新 过程。 最大峰值 .spec.strategy.rollingUpdate.maxSurge 是一个可选字段，用来指定可以创建的超出 期望 Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。 如果 MaxUnavailable 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。 此字段的默认值为 25%。 例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容， 同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。 8.5 进度期限秒数 .spec.progressDeadlineSeconds 是一个可选字段，用于指定系统在报告 Deployment 进展失败 之前等待 Deployment 取得进展的秒数。 这类报告会在资源状态中体现为 Type=Progressing、Status=False、 Reason=ProgressDeadlineExceeded。Deployment 控制器将持续重试 Deployment。 将来，一旦实现了自动回滚，Deployment 控制器将在探测到这样的条件时立即回滚 Deployment。 如果指定，则此字段值需要大于 .spec.minReadySeconds 取值 8.6 最短就绪时间 .spec.minReadySeconds 是一个可选字段，用于指定新创建的 Pod 在没有任意容器崩溃情况下的最小就绪时间， 只有超出这个时间 Pod 才被视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。 要了解何时 Pod 被视为就绪，可参考容器探针 8.7 Rollback To .spec.rollbackTo 是一个可以选配置项，用来配置 Deployment 回退的配置。设置该参数将触发回退操作，每次回退完成后，该值就会被清除。 8.8 Revision .spec.rollbackTo.revision 是一个可选配置项，用来指定回退到的 revision。默认是 0，意味着回退到上一个 revision。 8.9 修订历史限制 Deployment 的修订历史记录存储在它所控制的 ReplicaSets 中。 .spec.revisionHistoryLimit 是一个可选字段，用来设定出于会滚目的所要保留的旧 ReplicaSet 数量。 这些旧 ReplicaSet 会消耗 etcd 中的资源，并占用 kubectl get rs 的输出。 每个 Deployment 修订版本的配置都存储在其 ReplicaSets 中；因此，一旦删除了旧的 ReplicaSet， 将失去回滚到 Deployment 的对应修订版本的能力。 默认情况下，系统保留 10 个旧 ReplicaSet，但其理想值取决于新 Deployment 的频率和稳定性。 更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet。 在这种情况下，无法撤消新的 Deployment 上线，因为它的修订历史被清除了。 8.10 paused（暂停的） .spec.paused 是用于暂停和恢复 Deployment 的可选布尔字段。 暂停的 Deployment 和未暂停的 Deployment 的唯一区别是，Deployment 处于暂停状态时， PodTemplateSpec 的任何修改都不会触发新的上线。 Deployment 在创建时是默认不会处于暂停状态。 参考： Kuberenetes Deployments Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"对象/Deployment/kubernetes-deployment-skill.html":{"url":"对象/Deployment/kubernetes-deployment-skill.html","title":"技巧","keywords":"","body":"Deployment 技巧1. patch 管理 deployment1.2 patch 以 yaml 格式合并更新 Deployment1.2 合并类的 patch 的说明1.3 patch 以 yaml 格式替换更新 Deployment1.4 patch 以 json 格式合并更新 Deployment1.5 patch 其他方式更新 Deployment2. kustomize 管理 deploymentDeployment 技巧 tagsstart Deployment tagsstop 1. patch 管理 deployment 1.2 patch 以 yaml 格式合并更新 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: patch-demo spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: patch-demo-ctr image: nginx tolerations: - effect: NoSchedule key: dedicated value: test-team 创建 Deployment： $ kubectl create -f deployment-patch.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE patch-demo-28633765-670qr 1/1 Running 0 23s patch-demo-28633765-j5qs3 1/1 Running 0 23s 把运行的 Pod 的名字记下来。稍后，你将看到这些 Pod 被终止并被新的 Pod 替换。 此时，每个 Pod 都有一个运行 nginx 镜像的容器。现在假设你希望每个 Pod 有两个容器：一个运行 nginx，另一个运行 redis。 创建一个名为 patch-file-containers.yaml 的文件。内容如下: spec: template: spec: containers: - name: patch-demo-ctr-2 image: redis 修补你的 Deployment： kubectl patch deployment patch-demo --patch \"$(cat patch-file-containers.yaml)\" 查看修补后的 Deployment： kubectl get deployment patch-demo --output yaml 输出显示 Deployment 中的 PodSpec 有两个容器: containers: - image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... - image: nginx imagePullPolicy: Always name: patch-demo-ctr ... 查看与 patch Deployment 相关的 Pod: $ kubectl get pods NAME READY STATUS RESTARTS AGE patch-demo-1081991389-2wrn5 2/2 Running 0 1m patch-demo-1081991389-jmg7b 2/2 Running 0 1m 输出显示正在运行的 Pod 与以前运行的 Pod 有不同的名称。Deployment 终止了旧的 Pod，并创建了两个 符合更新的部署规范的新 Pod。2/2 表示每个 Pod 有两个容器: $ kubectl get pods NAME READY STATUS RESTARTS AGE patch-demo-1081991389-2wrn5 2/2 Running 0 1m patch-demo-1081991389-jmg7b 2/2 Running 0 1m 仔细查看其中一个 patch-demo Pod: $ kubectl get pod --output yaml containers: - image: redis ... - image: nginx ... 1.2 合并类的 patch 的说明 在前面的练习中所做的 patch 称为策略性合并 patch（Strategic Merge Patch)。 请注意，patch 没有替换containers 列表。相反，它向列表中添加了一个新 Container。换句话说， patch 中的列表与现有列表合并。当你在列表中使用策略性合并 patch 时，并不总是这样。 在某些情况下，列表是替换的，而不是合并的。 对于策略性合并 patch，列表可以根据其 patch 策略进行替换或合并。 patch 策略由 Kubernetes 源代码中字段标记中的 patchStrategy 键的值指定。 例如，PodSpec 结构体的 Containers 字段的 patchStrategy 为 merge： type PodSpec struct { ... Containers []Container `json:\"containers\" patchStrategy:\"merge\" patchMergeKey:\"name\" ...` 你还可以在 OpenApi spec 规范中看到 patch 策略： \"io.k8s.api.core.v1.PodSpec\": { ... \"containers\": { \"description\": \"List of containers belonging to the pod. ... }, \"x-kubernetes-patch-merge-key\": \"name\", \"x-kubernetes-patch-strategy\": \"merge\" }, 你可以在 Kubernetes API 文档 中看到 patch 策略。 创建一个名为 patch-file-tolerations.yaml 的文件。内容如下: spec: template: spec: tolerations: - effect: NoSchedule key: disktype value: ssd 对 Deployment 执行 patch 操作： $ kubectl patch deployment patch-demo --patch \"$(cat patch-file-containers.yaml)\" $ kubectl get deployment patch-demo --output yaml containers: - image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... - image: nginx imagePullPolicy: Always name: patch-demo-ctr ... tolerations: - effect: NoSchedule key: disktype value: ssd 请注意，PodSpec 中的 tolerations 列表被替换，而不是合并。这是因为 PodSpec 的 tolerations 的字段标签中没有 patchStrategy 键。所以策略合并 patch 操作使用默认的 patch 策略，也就是 replace。 type PodSpec struct { ... Tolerations []Toleration `json:\"tolerations,omitempty\" protobuf:\"bytes,22,opt,name=tolerations\"` 1.3 patch 以 yaml 格式替换更新 Deployment 策略性合并 patch 不同于 JSON 合并 patch。 使用 JSON 合并 patch，如果你想更新列表，你必须指定整个新列表。新的列表完全取代现有的列表。 kubectl patch 命令有一个 type 参数，你可以将其设置为以下值之一: | Parameter value | Merge type | |-----------------|----------------------------| | json | JSON Patch, RFC 6902 | | merge | JSON Merge Patch, RFC 7386 | | strategic | Strategic merge patch | 有关 JSON patch 和 JSON 合并 patch 的比较，查看 JSON patch 和 JSON 合并 patch。 type 参数的默认值是 strategic。在前面的练习中，我们做了一个策略性的合并 patch。 下一步，在相同的部署上执行 JSON 合并 patch。创建一个名为 patch-file-2 的文件。内容如下: spec: template: spec: containers: - name: patch-demo-ctr-3 image: gcr.io/google-samples/node-hello:1.0 在 patch 命令中，将 type 设置为 merge： kubectl patch deployment patch-demo --type merge --patch \"$(cat patch-file-2.yaml)\" 查看 patch 部署： kubectl get deployment patch-demo --output yaml patch 中指定的容器列表只有一个容器。 输出显示您的一个容器列表替换了现有的容器列表 spec: containers: - image: gcr.io/google-samples/node-hello:1.0 ... name: patch-demo-ctr-3 列表中运行的 Pod： $ kubectl get pods NAME READY STATUS RESTARTS AGE patch-demo-1307768864-69308 1/1 Running 0 1m patch-demo-1307768864-c86dc 1/1 Running 0 1m 1.4 patch 以 json 格式合并更新 Deployment kubectl patch 命令使用 YAML 或 JSON。它可以将 patch 作为文件，也可以直接在命令行中使用。 创建一个文件名称是 patch-file.json 内容如下： { \"spec\": { \"template\": { \"spec\": { \"containers\": [ { \"name\": \"patch-demo-ctr-2\", \"image\": \"redis\" } ] } } } } 以下命令是相同的： kubectl patch deployment patch-demo --patch \"$(cat patch-file.yaml)\" 1.5 patch 其他方式更新 Deployment kubectl patch deployment patch-demo --patch 'spec:\\n template:\\n spec:\\n containers:\\n - name: patch-demo-ctr-2\\n image: redis' kubectl patch deployment patch-demo --patch \"$(cat patch-file.json)\" kubectl patch deployment patch-demo --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"patch-demo-ctr-2\",\"image\": \"redis\"}]}}}}' 2. kustomize 管理 deployment kustomize (一) 管理yaml部署入门hello world Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:50:37 "},"对象/Deployment/kubernetes-client-go-deployment.html":{"url":"对象/Deployment/kubernetes-client-go-deployment.html","title":"开发","keywords":"","body":"kubernetes client-go 管理 deployment1. deployment控制器实现流程：2. 操作deployment查看创建2.1 go.mod2.2 client.gokubernetes client-go 管理 deployment tagsstart client-go Deployment tagsstop 1. deployment控制器实现流程： Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态； Deployment 对象的 Replicas 字段的值就是期望状态； Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod 可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。我们社区交流也称为“控制循环”， 2. 操作deployment查看创建 注意： Apps/v1beta1 1.16版本以上不再支持，而是Apps/v1 deployments, err := clientset.Appv1().Deployments(\"\").List(metav1.ListOptions{}) 2.1 go.mod module createdeployment go 1.13 require ( github.com/evanphx/json-patch v4.9.0+incompatible // indirect github.com/fsnotify/fsnotify v1.4.9 // indirect github.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7 // indirect github.com/golang/protobuf v1.4.2 // indirect github.com/googleapis/gnostic v0.4.0 // indirect github.com/gregjones/httpcache v0.0.0-20190611155906-901d90724c79 // indirect github.com/imdario/mergo v0.3.11 // indirect github.com/json-iterator/go v1.1.10 // indirect github.com/pkg/errors v0.9.1 // indirect golang.org/x/net v0.0.0-20200707034311-ab3426394381 // indirect golang.org/x/sys v0.0.0-20200622214017-ed371f2e16b4 // indirect golang.org/x/text v0.3.3 // indirect golang.org/x/time v0.0.0-20200630173020-3af7569d3a1e // indirect google.golang.org/protobuf v1.24.0 // indirect k8s.io/apimachinery v0.17.0 k8s.io/client-go v0.17.0 k8s.io/gengo v0.0.0-20200413195148-3a45101e95ac // indirect k8s.io/klog/v2 v2.2.0 // indirect k8s.io/utils v0.0.0-20201110183641-67b214c5f920 // indirect sigs.k8s.io/structured-merge-diff/v4 v4.0.1 // indirect ) 2.2 client.go package main import ( \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/kubernetes\" appsv1 \"k8s.io/api/apps/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" apiv1 \"k8s.io/api/core/v1\" \"k8s.io/client-go/kubernetes/typed/apps/v1\" \"flag\" \"fmt\" \"encoding/json\" ) func main() { //kubelet.kubeconfig 是文件对应地址 kubeconfig := flag.String(\"kubeconfig\", \"/root/.kube/config\", \"(optional) absolute path to the kubeconfig file\") flag.Parse() // 解析到config config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) if err != nil { panic(err.Error()) } // 创建连接 clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } deploymentsClient := clientset.AppsV1().Deployments(\"default\") //创建deployment createDeployment(deploymentsClient) //监听deployment startWatchDeployment(deploymentsClient) } //监听Deployment变化 func startWatchDeployment(deploymentsClient v1.DeploymentInterface) { w, _ := deploymentsClient.Watch(metav1.ListOptions{}) for { select { case e, _ := 参考资料： https://godoc.org/k8s.io/client-go/kubernetes#Clientset https://godoc.org/k8s.io/client-go/kubernetes/typed/core/v1 https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/volume/persistentvolume/pv_controller_base.go https://github.com/openshift/origin/blob/master/vendor/k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go https://www.bookstack.cn/read/huweihuang-kubernetes-notes/develop-client-go.md Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:08:58 "},"对象/Deployment/kubernetes-deployment-strategies.html":{"url":"对象/Deployment/kubernetes-deployment-strategies.html","title":"策略","keywords":"","body":"Deployment 策略1. 简介2. 重建(Recreate) - 最好在开发环境3. 滚动更新(rolling-update)4. 蓝/绿(blue/green) - 最好用来验证 API 版本问题5. 金丝雀(Canary) - 让部分用户参与测试6. A/B测试(A/B testing) - 最适合部分用户的功能测试7. 总结Deployment 策略 tagsstart Deployment tagsstop 1. 简介 在Kubernetes中有几种不同的方式发布应用，所以为了让应用在升级期间依然平稳提供服务，选择一个正确的发布策略就非常重要了。 选择正确的部署策略是要依赖于我们的业务需求的，下面我们列出了一些可能会使用到的策略： 重建(recreate)：停止旧版本部署新版本 滚动更新(rolling-update)：一个接一个地以滚动更新方式发布新版本 蓝绿(blue/green)：新版本与旧版本一起存在，然后切换流量 金丝雀(canary)：将新版本面向一部分用户发布，然后继续全量发布 A/B测(a/b testing)：以精确的方式（HTTP 头、cookie、权重等）向部分用户发布新版本。A/B测实际上是一种基于数据统计做出业务决策的技术。在 Kubernetes 中并不原生支持，需要额外的一些高级组件来完成改设置（比如Istio、Linkerd、Traefik、或者自定义 Nginx/Haproxy 等）。 你可以在Kubernetes集群上来对上面的这些策略进行测试，下面的仓库中有需要使用到的资源清单：https://github.com/ContainerSolutions/k8s-deployment-strategies 接下来我们来介绍下每种策略，看看在什么场景下面适合哪种策略。 2. 重建(Recreate) - 最好在开发环境 策略定义为Recreate的Deployment，会终止所有正在运行的实例，然后用较新的版本来重新创建它们。 spec: replicas: 3 strategy: type: Recreate 重新创建策略是一个虚拟部署，包括关闭版本A，然后在关闭版本A后部署版本B. 此技术意味着服务的停机时间取决于应用程序的关闭和启动持续时间。 我们这里创建两个相关的资源清单文件，app-v1.yaml： apiVersion: v1 kind: Service metadata: name: my-app labels: app: my-app spec: type: NodePort ports: - name: http port: 80 targetPort: http selector: app: my-app --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 3 selector: matchLabels: app: my-app strategy: type: Recreate selector: matchLabels: app: my-app template: metadata: labels: app: my-app version: v1.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v1.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 app-v2.yaml 文件内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 3 strategy: type: Recreate selector: matchLabels: app: my-app template: metadata: labels: app: my-app version: v2.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v2.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 上面两个资源清单文件中的 Deployment 定义几乎是一直的，唯一不同的是定义的环境变量VERSION值不同，接下来按照下面的步骤来验证Recreate策略： 版本1提供服务 删除版本1 部署版本2 等待所有副本准备就绪 首先部署第一个应用： $ kubectl apply -f app-v1.yaml service \"my-app\" created deployment.apps \"my-app\" created 测试版本1是否部署成功： $ kubectl get pods -l app=my-app NAME READY STATUS RESTARTS AGE my-app-7b4874cd75-m5kct 1/1 Running 0 19m my-app-7b4874cd75-pc444 1/1 Running 0 19m my-app-7b4874cd75-tlctl 1/1 Running 0 19m $ kubectl get svc my-app NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-app NodePort 10.108.238.76 80:32532/TCP 5m $ curl http://127.0.0.1:32532 Host: my-app-7b4874cd75-pc444, Version: v1.0.0 可以看到版本1的应用正常运行了。为了查看部署的运行情况，打开一个新终端并运行以下命令： $ watch kubectl get po -l app=my-app 然后部署版本2的应用： $ kubectl apply -f app-v2.yaml 这个时候可以观察上面新开的终端中的 Pod 列表的变化，可以看到之前的3个 Pod 都会先处于Terminating状态，并且3个 Pod 都被删除后才开始创建新的 Pod。 然后测试第二个版本应用的部署进度： $ while sleep 0.1; do curl http://127.0.0.1:32532; done curl: (7) Failed connect to 127.0.0.1:32532; Connection refused curl: (7) Failed connect to 127.0.0.1:32532; Connection refused ...... Host: my-app-f885c8d45-sp44p, Version: v2.0.0 Host: my-app-f885c8d45-t8g7g, Version: v2.0.0 Host: my-app-f885c8d45-sp44p, Version: v2.0.0 ...... 可以看到最开始的阶段服务都是处于不可访问的状态，然后到第二个版本的应用部署成功后才正常访问，可以看到现在访问的数据是版本2了。 最后，可以执行下面的命令来清空上面的资源对象： $ kubectl delete all -l app=my-app 结论: 应用状态全部更新 停机时间取决于应用程序的关闭和启动消耗的时间 3. 滚动更新(rolling-update) 滚动更新通过逐个替换实例来逐步部署新版本的应用，直到所有实例都被替换完成为止。它通常遵循以下过程：在负载均衡器后面使用版本 A 的实例池，然后部署版本 B 的一个实例，当服务准备好接收流量时(Readiness Probe 正常)，将该实例添加到实例池中，然后从实例池中删除一个版本 A 的实例并关闭，如下图所示： 下图是滚动更新过程应用接收流量的示意图： 下面是 Kubernetes 中通过 Deployment 来进行滚动更新的关键参数： spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 # 一次可以添加多少个Pod maxUnavailable: 1 # 滚动更新期间最大多少个Pod不可用 现在仍然使用上面的 app-v1.yaml 这个资源清单文件，新建一个定义滚动更新的资源清单文件 app-v2-rolling-update.yaml，文件内容如下: apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 10 # maxUnavailable设置为0可以完全确保在滚动更新期间服务不受影响，还可以使用百分比的值来进行设置。 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 selector: matchLabels: app: my-app template: metadata: labels: app: my-app version: v2.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v2.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe # 初始延迟设置高点可以更好地观察滚动更新过程 initialDelaySeconds: 15 periodSeconds: 5 上面的资源清单中我们在环境变量中定义了版本2，然后通过设置strategy.type=RollingUpdate来定义该 Deployment 使用滚动更新的策略来更新应用，接下来我们按下面的步骤来验证滚动更新策略： 版本1提供服务 部署版本2 等待直到所有副本都被版本2替换完成 同样，首先部署版本1应用： $ kubectl apply -f app-v1.yaml service \"my-app\" created deployment.apps \"my-app\" created 测试版本1是否部署成功： $ kubectl get pods -l app=my-app NAME READY STATUS RESTARTS AGE my-app-7b4874cd75-h8c4d 1/1 Running 0 47s my-app-7b4874cd75-p4l8f 1/1 Running 0 47s my-app-7b4874cd75-qnt7p 1/1 Running 0 47s $ kubectl get svc my-app NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-app NodePort 10.109.99.184 80:30486/TCP 1m $ curl http://127.0.0.1:30486 Host: my-app-7b4874cd75-qnt7p, Version: v1.0.0 同样，在一个新终端中执行下面命令观察 Pod 变化： $ watch kubectl get pod -l app=my-app 然后部署滚动更新版本2应用： $ kubectl apply -f app-v2-rolling-update.yaml deployment.apps \"my-app\" configured 这个时候在上面的 watch 终端中可以看到多了很多 Pod，还在创建当中，并没有一开始就删除之前的 Pod，同样，这个时候执行下面命令，测试应用状态： $ while sleep 0.1; do curl http://127.0.0.1:30486; done Host: my-app-7b4874cd75-vrlj7, Version: v1.0.0 ...... Host: my-app-7b4874cd75-vrlj7, Version: v1.0.0 Host: my-app-6b5479d97f-2fk24, Version: v2.0.0 Host: my-app-7b4874cd75-p4l8f, Version: v1.0.0 ...... Host: my-app-6b5479d97f-s5ctz, Version: v2.0.0 Host: my-app-7b4874cd75-5ldqx, Version: v1.0.0 ...... Host: my-app-6b5479d97f-5z6ww, Version: v2.0.0 们可以看到上面的应用并没有出现不可用的情况，最开始访问到的都是版本1的应用，然后偶尔会出现版本2的应用，直到最后全都变成了版本2的应用，而这个时候看上面 watch 终端中 Pod 已经全部变成10个版本2的应用了，我们可以看到这就是一个逐步替换的过程。 如果在滚动更新过程中发现新版本应用有问题，我们可以通过下面的命令来进行一键回滚： $ kubectl rollout undo deploy my-app deployment.apps \"my-app\" 如果你想保持两个版本的应用都存在，那么我们也可以执行 pause 命令来暂停更新： $ kubectl rollout pause deploy my-app deployment.apps \"my-app\" paused 这个时候我们再去循环访问我们的应用就可以看到偶尔会出现版本1的应用信息了。 如果新版本应用程序没问题了，也可以继续恢复更新： $ kubectl rollout resume deploy my-app deployment.apps \"my-app\" resumed 最后，可以执行下面的命令来清空上面的资源对象： $ kubectl delete all -l app=my-app 结论： 版本在实例之间缓慢替换 rollout/rollback 可能需要一定时间 无法控制流量 4. 蓝/绿(blue/green) - 最好用来验证 API 版本问题 蓝/绿发布是版本2 与版本1 一起发布，然后流量切换到版本2，也称为红/黑部署。蓝/绿发布与滚动更新不同，版本2(绿) 与版本1(蓝)一起部署，在测试新版本满足要求后，然后更新更新 Kubernetes 中扮演负载均衡器角色的 Service 对象，通过替换 label selector 中的版本标签来将流量发送到新版本，如下图所示： 下面是蓝绿发布策略下应用方法的示例图： 在 Kubernetes 中，我们可以用两种方法来实现蓝绿发布，通过单个 Service 对象或者 Ingress 控制器来实现蓝绿发布，实际操作都是类似的，都是通过 label 标签去控制。 实现蓝绿发布的关键点就在于 Service 对象中 label selector 标签的匹配方法，比如我们重新定义版本1 的资源清单文件 app-v1-single-svc.yaml，文件内容如下： apiVersion: v1 kind: Service metadata: name: my-app labels: app: my-app spec: type: NodePort ports: - name: http port: 80 targetPort: http # 注意这里我们匹配 app 和 version 标签，当要切换流量的时候，我们更新 version 标签的值，比如：v2.0.0 selector: app: my-app version: v1.0.0 --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v1 labels: app: my-app spec: replicas: 3 selector: matchLabels: app: my-app version: v1.0.0 template: metadata: labels: app: my-app version: v1.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v1.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 上面定义的资源对象中，最重要的就是 Service 中 label selector 的定义： selector: app: my-app version: v1.0.0 版本2 的应用定义和以前一样，新建文件 app-v2-single-svc.yaml，文件内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v2 labels: app: my-app spec: replicas: 3 selector: matchLabels: app: my-app version: v2.0.0 template: metadata: labels: app: my-app version: v2.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v2.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 然后按照下面的步骤来验证使用单个 Service 对象实现蓝/绿部署的策略： 版本1 应用提供服务 部署版本2 应用 等到版本2 应用全部部署完成 切换入口流量从版本1 到版本2 关闭版本1 应用 首先，部署版本1 应用： $ kubectl apply -f app-v1-single-svc.yaml service \"my-app\" created deployment.apps \"my-app-v1\" created 测试版本1 应用是否部署成功： $ kubectl get pods -l app=my-app NAME READY STATUS RESTARTS AGE my-app-v1-7b4874cd75-7xh6s 1/1 Running 0 41s my-app-v1-7b4874cd75-dmq8f 1/1 Running 0 41s my-app-v1-7b4874cd75-t64z7 1/1 Running 0 41s $ kubectl get svc -l app=my-app NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-app NodePort 10.106.184.144 80:31539/TCP 50s $ curl http://127.0.0.1:31539 Host: my-app-v1-7b4874cd75-7xh6s, Version: v1.0.0 同样，新开一个终端，执行如下命令观察 Pod 变化： $ watch kubectl get pod -l app=my-app 然后部署版本2 应用： $ kubectl apply -f app-v2-single-svc.yaml deployment.apps \"my-app-v2\" created 然后在上面 watch 终端中可以看到会多3个my-app-v2开头的 Pod，待这些 Pod 部署成功后，我们再去访问当前的应用： $ while sleep 0.1; do curl http://127.0.0.1:31539; done Host: my-app-v1-7b4874cd75-dmq8f, Version: v1.0.0 Host: my-app-v1-7b4874cd75-dmq8f, Version: v1.0.0 ...... 我们会发现访问到的都是版本1 的应用，和我们刚刚部署的版本2 没有任何关系，这是因为我们 Service 对象中通过 label selector 匹配的是version=v1.0.0这个标签，我们可以通过修改 Service 对象的匹配标签，将流量路由到标签version=v2.0.0的 Pod 去： $ kubectl patch service my-app -p '{\"spec\":{\"selector\":{\"version\":\"v2.0.0\"}}}' service \"my-app\" patched 然后再去访问应用，可以发现现在都是版本2 的信息了： $ while sleep 0.1; do curl http://127.0.0.1:31539; done Host: my-app-v2-f885c8d45-r5m6z, Version: v2.0.0 Host: my-app-v2-f885c8d45-r5m6z, Version: v2.0.0 ...... 如果你需要回滚到版本1，同样只需要更改 Service 的匹配标签即可： $ kubectl patch service my-app -p '{\"spec\":{\"selector\":{\"version\":\"v1.0.0\"}}}' 如果新版本已经完全符合我们的需求了，就可以删除版本1 的应用了： $ kubectl delete deploy my-app-v1 最后，同样，执行如下命令清理上述资源对象： $ kubectl delete all -l app=my-app 结论： 实时部署/回滚 避免版本问题，因为一次更改是整个应用的改变 需要两倍的资源 在发布到生产之前，应该对整个应用进行适当的测试 5. 金丝雀(Canary) - 让部分用户参与测试 金丝雀部署是让部分用户访问到新版本应用，在 Kubernetes 中，可以使用两个具有相同 Pod 标签的 Deployment 来实现金丝雀部署。新版本的副本和旧版本的一起发布。在一段时间后如果没有检测到错误，则可以扩展新版本的副本数量并删除旧版本的应用。 如果需要按照具体的百分比来进行金丝雀发布，需要尽可能的启动多的 Pod 副本，这样计算流量百分比的时候才方便，比如，如果你想将 1% 的流量发送到版本 B，那么我们就需要有一个运行版本 B 的 Pod 和 99 个运行版本 A 的 Pod，当然如果你对具体的控制策略不在意的话也就无所谓了，如果你需要更精确的控制策略，建议使用服务网格（如 Istio），它们可以更好地控制流量。 在下面的例子中，我们使用 Kubernetes 原生特性来实现一个穷人版的金丝雀发布，如果你想要对流量进行更加细粒度的控制，请使用豪华版本的 Istio。下面是金丝雀发布的应用请求示意图： 接下来我们按照下面的步骤来验证金丝雀策略： 10个副本的版本1 应用提供服务 版本2 应用部署1个副本（意味着小于10%的流量） 等待足够的时间来确认版本2 应用足够稳定没有任何错误信息 将版本2 应用扩容到10个副本 等待所有实例完成 关闭版本1 应用 首先，创建版本1 的应用资源清单，app-v1-canary.yaml，内容如下： apiVersion: v1 kind: Service metadata: name: my-app labels: app: my-app spec: type: NodePort ports: - name: http port: 80 targetPort: http selector: app: my-app --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v1 labels: app: my-app spec: replicas: 10 selector: matchLabels: app: my-app version: v1.0.0 template: metadata: labels: app: my-app version: v1.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v1.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 其中核心的部分也是 Service 对象中的 label selector 标签，不在具有版本相关的标签了，然后定义版本2 的资源清单文件，app-v2-canary.yaml，文件内容如下： apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v2 labels: app: my-app spec: replicas: 1 selector: matchLabels: app: my-app version: v2.0.0 template: metadata: labels: app: my-app version: v2.0.0 annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9101\" spec: containers: - name: my-app image: containersol/k8s-deployment-strategies ports: - name: http containerPort: 8080 - name: probe containerPort: 8086 env: - name: VERSION value: v2.0.0 livenessProbe: httpGet: path: /live port: probe initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: probe periodSeconds: 5 版本1 和版本2 的 Pod 都具有一个共同的标签app=my-app，所以对应的 Service 会匹配两个版本的 Pod。 首先，部署版本1 应用： $ kubectl apply -f app-v1-canary.yaml service \"my-app\" created deployment.apps \"my-app-v1\" created 然后测试版本1 应用是否正确部署了： $ kubectl get svc -l app=my-app NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-app NodePort 10.105.133.213 80:30760/TCP 47s $ curl http://127.0.0.1:30760 Host: my-app-v1-7b4874cd75-tsh2s, Version: v1.0.0 同样，新开一个终端，查看 Pod 的变化： $ watch kubectl get po 然后部署版本2 应用： $ kubectl apply -f app-v2-canary.yaml deployment.apps \"my-app-v2\" created 然后在 watch 终端页面可以看到多了一个 Pod，现在一共 11 个 Pod，其中只有1 个 Pod 运行新版本应用，然后同样可以循环访问该应用，查看是否会有版本2 的应用信息： $ while sleep 0.1; do curl http://127.0.0.1:30760; done Host: my-app-v1-7b4874cd75-bhxbp, Version: v1.0.0 Host: my-app-v1-7b4874cd75-wmcqc, Version: v1.0.0 Host: my-app-v1-7b4874cd75-tsh2s, Version: v1.0.0 Host: my-app-v1-7b4874cd75-ml58j, Version: v1.0.0 Host: my-app-v1-7b4874cd75-spsdv, Version: v1.0.0 Host: my-app-v2-f885c8d45-mc2fx, Version: v2.0.0 ...... 正常情况下可以看到大部分都是返回的版本1 的应用信息，偶尔会出现版本2 的应用信息，这就证明我们的金丝雀发布成功了，待确认了版本2 的这个应用没有任何问题后，可以将版本2 应用扩容到10 个副本： $ kubectl scale --replicas=10 deploy my-app-v2 deployment.extensions \"my-app-v2\" scaled 其实这个时候访问应用的话新版本和旧版本的流量分配是1:1了，确认了版本2 正常后，就可以删除版本1 的应用了： $ kubectl delete deploy my-app-v1 deployment.extensions \"my-app-v1\" deleted 最终留下的是 10 个新版本的 Pod 了，到这里我们的整个金丝雀发布就完成了。 同样，最后，执行下面的命令删除上面的资源对象： $ kubectl delete all -l app=my-app 结论： 部分用户获取新版本 方便错误和性能监控 快速回滚 发布较慢 流量精准控制很浪费（99％A / 1％B = 99 Pod A，1 Pod B） 如果你对新功能的发布没有信心，建议使用金丝雀发布的策略。 6. A/B测试(A/B testing) - 最适合部分用户的功能测试 A/B 测试实际上是一种基于统计信息而非部署策略来制定业务决策的技术，与业务结合非常紧密。但是它们也是相关的，也可以使用金丝雀发布来实现。 除了基于权重在版本之间进行流量控制之外，A/B 测试还可以基于一些其他参数（比如 Cookie、User Agent、地区等等）来精确定位给定的用户群，该技术广泛用于测试一些功能特性的效果，然后按照效果来进行确定。 我们经常可以在今日头条的客户端中就会发现有大量的 A/B 测试，同一个地区的用户看到的客户端有很大不同。 要使用这些细粒度的控制，仍然还是建议使用 Istio，可以根据权重或 HTTP 头等来动态请求路由控制流量转发。 下面是使用 Istio 进行规则设置的示例，因为 Istio 还不太稳定，以下示例规则将来可能会更改： route: - tags: version: v1.0.0 weight: 90 - tags: version: v2.0.0 weight: 10 关于在 Istio 中具体如何做 A/B 测试，我们这里就不再详细介绍了，我们在istio-book文档中有相关的介绍。 结论： 几个版本并行运行 完全控制流量分配 特定的一个访问错误难以排查，需要分布式跟踪 Kubernetes 没有直接的支持，需要其他额外的工具 7. 总结 发布应用有许多种方法，当发布到开发/测试环境的时候，重建或者滚动更新通常是一个不错的选择。在生产环境，滚动更新或者蓝绿发布比较合适，但是新版本的提前测试是非常有必要的。如果你对新版本的应用不是很有信心的话，那应该使用金丝雀发布，将用户的影响降到最低。最后，如果你的公司需要在特定的用户群体中进行新功能的测试，例如，移动端用户请求路由到版本 A，桌面端用户请求路由到版本 B，那么你就看使用A/B 测试，通过使用 Kubernetes 服务网关的配置，可以根据某些请求参数来确定用户应路由的服务。 参考： Kubernetes deployment strategies Kubernetes 部署策略详解 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:50:49 "},"对象/Deployment/kubernetes-deployment-principle.html":{"url":"对象/Deployment/kubernetes-deployment-principle.html","title":"原理","keywords":"","body":"Deployment 原理1. deployment控制器实现流程2. 控制定义（期望）与被控制对象（模板）3. ReplicaSet4. 滚动更新Deployment 原理 tagsstart 原理 Deployment tagsstop 1. deployment控制器实现流程 Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态； Deployment 对象的 Replicas 字段的值就是期望状态； Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod 可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。我们社区交流也称为“控制循环”，比如，现在有一种待编排的对象 X，它有一个对应的控制器。那么，我就可以用一段 Go 语言风格的伪代码，为你描述这个控制循环： for { 实际状态 := 获取集群中对象X的实际状态（Actual State） 期望状态 := 获取集群中对象X的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } } 在具体实现中，实际状态往往来自于 Kubernetes 集群本身。比如， kubelet 通过心跳汇报的容器状态和节点状态； 监控系统中保存的应用监控数据； 控制器主动收集的它自己感兴趣的信息。 这些都是常见的实际状态的来源。而期望状态，一般来自于用户提交的 YAML 文件。比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。 增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。 2. 控制定义（期望）与被控制对象（模板） 其实，像 Deployment 这种控制器的设计原理，就是我们前面提到过的，“用一种对象管理另一种对象”的“艺术”。其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。 像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。这个概念非常重要，因为后面我要讲解到的大多数控制器，都会使用 PodTemplate 来统一定义它所要管理的 Pod。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。至此，我们就可以对 Deployment 以及其他类似的控制器，做一个简单总结了： 如上图所示，类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。 这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。 那么，对于我们这个 nginx-deployment 来说，它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗？或者说，nginx-deployment 所直接控制的，就是 Pod 对象么？不是，是ReplicaSet 3. ReplicaSet Deployment 看似简单，但实际上，它实现了 Kubernetes 项目中一个非常重要的功能：Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）。这个功能，是从 PaaS 时代开始，一个平台级项目就必须具备的编排能力。 举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。 ReplicaSet 的结构非常简单，我们可以通过这个 YAML 文件查看一下： apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 从这个 YAML 文件中，我们可以看到，一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的。不难发现，它的定义其实是 Deployment 的一个子集。 更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。 明白了这个原理，我再来和你一起分析一个如下所示的 Deployment： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 可以看到，这就是一个我们常用的 nginx-deployment，它定义的 Pod 副本个数是 3（spec.replicas=3）。那么，在具体的实现上，这个 Deployment，与 ReplicaSet，以及 Pod 的关系是怎样的呢？我们可以用一张图把它描述出来： 通过这张图，我们就很清楚地看到，一个定义了 replicas=3 的 Deployment，与它的 ReplicaSet，以及 Pod 的关系，实际上是一种“层层控制”的关系。 其中，ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义。 而在此基础上，Deployment 同样通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。其中，“水平扩展 / 收缩”非常容易实现，Deployment Controller 只需要修改它所控制的 ReplicaSet 的 Pod 副本个数就可以了。 比如，把这个值从 3 改成 4，那么 Deployment 所对应的 ReplicaSet，就会根据修改后的值自动创建一个新的 Pod。这就是“水平扩展”了；“水平收缩”则反之。而用户想要执行这个操作的指令也非常简单，就是 kubectl scale，比如： $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled 那么，“滚动更新”又是什么意思，是如何实现的呢？ 4. 滚动更新 首先，我们来创建这个 nginx-deployment： $ kubectl create -f nginx-deployment.yaml --record 注意，在这里，我额外加了一个--record 参数。它的作用，是记录下你每次操作所执行的命令，以方便后面查看。然后，我们来检查一下 nginx-deployment 创建后的状态信息： $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 在返回结果中，我们可以看到四个状态字段，它们的含义如下所示。 DESIRED：用户期望的 Pod 副本个数（spec.replicas 的值）； CURRENT：当前处于 Running 状态的 Pod 的个数； UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致； AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。 可以看到，只有这个 AVAILABLE 字段，描述的才是用户所期望的最终状态。而 Kubernetes 项目还为我们提供了一条指令，让我们可以实时查看 Deployment 对象的状态变化。这个指令就是 kubectl rollout status： $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out 在这个返回结果中，“2 out of 3 new replicas have been updated”意味着已经有 2 个 Pod 进入了 UP-TO-DATE 状态。继续等待一会儿，我们就能看到这个 Deployment 的 3 个 Pod，就进入到了 AVAILABLE 状态： NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 20s 此时，你可以尝试查看一下这个 Deployment 所控制的 ReplicaSet： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3167673210 3 3 3 20s 如上所示，在用户提交了一个 Deployment 对象后，Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet。这个 ReplicaSet 的名字，则是由 Deployment 的名字和一个随机字符串共同组成。 这个随机字符串叫作 pod-template-hash，在我们这个例子里就是：3167673210。ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里，从而保证这些 Pod 不会与集群里的其他 Pod 混淆。 而 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 中是一致的。所以，相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段。 这个时候，如果我们修改了 Deployment 的 Pod 模板，“滚动更新”就会被自动触发。修改 Deployment 有很多方法。比如，我可以直接使用 kubectl edit 指令编辑 Etcd 里的 API 对象。 $ kubectl edit deployment/nginx-deployment ... spec: containers: - name: nginx image: nginx:1.9.1 # 1.7.9 -> 1.9.1 ports: - containerPort: 80 ... deployment.extensions/nginx-deployment edited 这个 kubectl edit 指令，会帮你直接打开 nginx-deployment 的 API 对象。然后，你就可以修改这里的 Pod 模板部分了。比如，在这里，我将 nginx 镜像的版本升级到了 1.9.1。 备注：kubectl edit 并不神秘，它不过是把 API 对象的内容下载到了本地文件，让你修改完成后再提交上去。 kubectl edit 指令编辑完成后，保存退出，Kubernetes 就会立刻触发“滚动更新”的过程。你还可以通过 kubectl rollout status 指令查看 nginx-deployment 的状态变化： $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.extensions/nginx-deployment successfully rolled out 这时，你可以通过查看 Deployment 的 Events，看到这个“滚动更新”的流程： $ kubectl describe deployment nginx-deployment ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 0 可以看到，首先，当你修改了 Deployment 里的 Pod 定义之后，Deployment Controller 会使用这个修改后的 Pod 模板，创建一个新的 ReplicaSet（hash=1764197365），这个新的 ReplicaSet 的初始 Pod 副本数是：0。 然后，在 Age=24 s 的位置，Deployment Controller 开始将这个新的 ReplicaSet 所控制的 Pod 副本数从 0 个变成 1 个，即：“水平扩展”出一个副本。 紧接着，在 Age=22 s 的位置，Deployment Controller 又将旧的 ReplicaSet（hash=3167673210）所控制的旧 Pod 副本数减少一个，即：“水平收缩”成两个副本。 如此交替进行，新 ReplicaSet 管理的 Pod 副本数，从 0 个变成 1 个，再变成 2 个，最后变成 3 个。而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个，再变成 1 个，最后变成 0 个。这样，就完成了这一组 Pod 的版本升级过程。 像这样，将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 3 3 3 6s nginx-deployment-3167673210 0 0 0 30s 其中，旧 ReplicaSet（hash=3167673210）已经被“水平收缩”成了 0 个副本。这种“滚动更新”的好处是显而易见的。 比如，在升级刚开始的时候，集群里只有 1 个新版本的 Pod。如果这时，新版本 Pod 有问题启动不起来，那么“滚动更新”就会停止，从而允许开发和运维人员介入。而在这个过程中，由于应用本身还有两个旧版本的 Pod 在线，所以服务并不会受到太大的影响。 当然，这也就要求你一定要使用 Pod 的 Health Check 机制检查应用的运行状态，而不是简单地依赖于容器的 Running 状态。要不然的话，虽然容器已经变成 Running 了，但服务很有可能尚未启动，“滚动更新”的效果也就达不到了。 而为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。 所以，在上面这个 Deployment 的例子中，它有 3 个 Pod 副本，那么控制器在“滚动更新”的过程中永远都会确保至少有 2 个 Pod 处于可用状态，至多只有 4 个 Pod 同时存在于集群中。这个策略，是 Deployment 对象的一个字段，名叫 RollingUpdateStrategy，如下所示： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: ... strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 在上面这个 RollingUpdateStrategy 的配置中，maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。 同时，这两个配置还可以用前面我们介绍的百分比形式来表示，比如：maxUnavailable=50%，指的是我们最多可以一次删除“50%*DESIRED 数量”个 Pod。结合以上讲述，现在我们可以扩展一下 Deployment、ReplicaSet 和 Pod 的关系图了。 如上所示，Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet 的属性。而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod 数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。通过这样的多个 ReplicaSet 对象，Kubernetes 项目就实现了对多个“应用版本”的描述。而明白了“应用版本和 ReplicaSet 一一对应”的设计思想之后，我就可以为你讲解一下Deployment 对应用进行版本控制的具体原理了。 这一次，我会使用一个叫 kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。我们一起来实践一下： $ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 deployment.extensions/nginx-deployment image updated 由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。这时，我们来检查一下 ReplicaSet 的状态，如下所示： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 2 2 2 24s nginx-deployment-3167673210 0 0 0 35s nginx-deployment-2156724341 2 2 0 7s 通过这个返回结果，我们可以看到，新版本的 ReplicaSet（hash=2156724341）的“水平扩展”已经停止。而且此时，它已经创建了两个 Pod，但是它们都没有进入 READY 状态。这当然是因为这两个 Pod 都拉取不到有效的镜像。 与此同时，旧版本的 ReplicaSet（hash=1764197365）的“水平收缩”，也自动停止了。此时，已经有一个旧 Pod 被删除，还剩下两个旧 Pod。那么问题来了， 我们如何让这个 Deployment 的 3 个 Pod，都回滚到以前的旧版本呢？我们只需要执行一条 kubectl rollout undo 命令，就能把整个 Deployment 回滚到上一个版本： $ kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment 很容易想到，在具体操作上，Deployment 的控制器，其实就是让这个旧 ReplicaSet（hash=1764197365）再次“扩展”成 3 个 Pod，而让新的 ReplicaSet（hash=2156724341）重新“收缩”到 0 个 Pod。 更进一步地，如果我想回滚到更早之前的版本，要怎么办呢？ 首先，我需要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本。而由于我们在创建这个 Deployment 的时候，指定了--ecord 参数，所以我们创建这些版本时执行的 kubectl 命令，都会被记录下来。这个操作的输出如下所示： $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 可以看到，我们前面执行的创建和更新操作，分别对应了版本 1 和版本 2，而那次失败的更新操作，则对应的是版本 3。 当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示： $ kubectl rollout history deployment/nginx-deployment --revision=2 然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了。这个指令的用法如下： $ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment 这样，Deployment Controller 还会按照“滚动更新”的方式，完成对 Deployment 的降级操作。不过，你可能已经想到了一个问题：我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源呢？ 没错。所以，Kubernetes 项目还提供了一个指令，使得我们对 Deployment 的多次更新操作，最后 只生成一个 ReplicaSet。 具体的做法是，在更新 Deployment 前，你要先执行一条 kubectl rollout pause 指令。它的用法如下所示： $ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused 这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。 所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示： $ kubectl rollout resume deployment/nginx-deployment deployment.extensions/nginx-deployment resumed 在这个 kubectl rollout resume 指令执行之前，在 kubectl rollout pause 指令之后的这段时间里，我们对 Deployment 进行的所有修改，最后只会触发一次“滚动更新”。当然，我们可以通过检查 ReplicaSet 状态的变化，来验证一下 kubectl rollout pause 和 kubectl rollout resume 指令的执行效果，如下所示： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-1764197365 0 0 0 2m nginx-3196763511 3 3 3 28s 通过返回结果，我们可以看到，只有一个 hash=3196763511 的 ReplicaSet 被创建了出来。不过，即使你像上面这样小心翼翼地控制了 ReplicaSet 的生成数量，随着应用版本的不断增加，Kubernetes 中还是会为同一个 Deployment 保存很多很多不同的 ReplicaSet。那么，我们又该如何控制这些“历史”ReplicaSet 的数量呢？ 很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。 参考： kubernetes deployment Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:50:20 "},"对象/Kubernetes-DaemonSet.html":{"url":"对象/Kubernetes-DaemonSet.html","title":"DaemonSet","keywords":"","body":"Kubernetes DaemonSet1. 简介2. 创建 DaemonSet3. 配置说明3.1 必需字段3.2 Pod 模板3.3 Pod 选择算符3.4 仅在某些节点上运行 Pod4. Daemon Pods 是如何被调度的4.1 通过默认调度器调度4.2 污点和容忍度5. Daemon Pods 通信6. DaemonSet更新7. DaemonSet 滚动更新7.1 DaemonSet 更新策略7.2 执行滚动更新8. DaemonSet 回滚8.1 显示DaemonSet 回滚到的历史修订版本（revision）8.2 回滚到指定版本8.3 监视 DaemonSet 回滚进度8.4 理解 DaemonSet 修订版本Kubernetes DaemonSet tagsstart DaemonSet 对象 tagsstop 有时候如果你爱一个人，就要形同陌路。 ——《银翼杀手2049》 1. 简介 DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 DaemonSet 的一些典型用法： 在每个节点上运行集群守护进程 在每个节点上运行日志收集守护进程 在每个节点上运行监控守护进程 一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。 2. 创建 DaemonSet 你可以在 YAML 文件中描述 DaemonSet。 例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet： apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can't run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 基于 YAML 文件创建 DaemonSet： kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml 3. 配置说明 3.1 必需字段 DaemonSet 需要 apiVersion、kind 和 metadata 字段 DaemonSet 对象的名称必须是一个合法的 DNS 子域名。 DaemonSet 也需要一个 .spec 配置段3.2 Pod 模板 .spec 中唯一必需的字段是 .spec.template。 .spec.template 是一个 Pod 模板。 除了它是嵌套的，因而不具有 apiVersion 或 kind 字段之外，它与 Pod 具有相同的 schema。 除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 Pod 选择算符）。 在 DaemonSet 中的 Pod 模板必须具有一个值为 Always 的 RestartPolicy。 当该值未指定时，默认是 Always。 3.3 Pod 选择算符 .spec.selector 字段表示 Pod 选择算符，它与 Job 的 .spec.selector 的作用是相同的。 从 Kubernetes 1.8 开始，您必须指定与 .spec.template 的标签匹配的 Pod 选择算符。 用户不指定 Pod 选择算符时，该字段不再有默认值。 选择算符的默认值生成结果与 kubectl apply 不兼容。 此外，一旦创建了 DaemonSet，它的 .spec.selector 就不能修改。 修改 Pod 选择算符可能导致 Pod 意外悬浮，并且这对用户来说是费解的。 spec.selector 是一个对象，如下两个字段组成： matchLabels - 与 ReplicationController 的 .spec.selector 的作用相同。 matchExpressions - 允许构建更加复杂的选择器，可以通过指定 key、value 列表以及将 key 和 value列表关联起来的 operator。 当上述两个字段都指定时，结果会按逻辑与（AND）操作处理。 如果指定了 .spec.selector，必须与 .spec.template.metadata.labels 相匹配。 如果与后者不匹配，则 DeamonSet 会被 API 拒绝。 3.4 仅在某些节点上运行 Pod 如果指定了 .spec.template.spec.nodeSelector，DaemonSet 控制器将在能够与 Node 选择算符 匹配的节点上创建 Pod。 类似这种情况，可以指定 .spec.template.spec.affinity，之后 DaemonSet 控制器 将在能够与节点亲和性 匹配的节点上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。 4. Daemon Pods 是如何被调度的 4.1 通过默认调度器调度 DaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。 通常，运行 Pod 的节点由 Kubernetes 调度器选择。 不过，DaemonSet Pods 由 DaemonSet 控制器创建和调度。这就带来了以下问题： Pod 行为的不一致性：正常 Pod 在被创建后等待调度时处于 Pending 状态， DaemonSet Pods 创建后不会处于Pending 状态下。这使用户感到困惑。 Pod 抢占 由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占 的情况下制定调度决策。 ScheduleDaemonSetPods 允许您使用默认调度器而不是 DaemonSet 控制器来调度 DaemonSets， 方法是将 NodeAffinity 条件而不是 .spec.nodeName 条件添加到 DaemonSet Pods。 默认调度器接下来将 Pod 绑定到目标主机。 如果 DaemonSet Pod 的节点亲和性配置已存在，则被替换。 DaemonSet 控制器仅在创建或修改 DaemonSet Pod 时执行这些操作， 并且不会更改 DaemonSet 的 spec.template。 此外，系统会自动添加 node.kubernetes.io/unschedulable：NoSchedule 容忍度到 DaemonSet Pods。在调度 DaemonSet Pod 时，默认调度器会忽略 unschedulable 节点。 4.2 污点和容忍度 node.kubernetes.io/not-ready NoExecute 1.13+ 当出现类似网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。 node.kubernetes.io/unreachable NoExecute 1.13+ 当出现类似于网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。 node.kubernetes.io/disk-pressure NoSchedule 1.8+ node.kubernetes.io/memory-pressure NoSchedule 1.8+ node.kubernetes.io/unschedulable NoSchedule 1.12+ DaemonSet Pod 能够容忍默认调度器所设置的 unschedulable 属性. node.kubernetes.io/network-unavailable NoSchedule 1.12+ DaemonSet 在使用宿主网络时，能够容忍默认调度器所设置的 network-unavailable 属性。 。 5. Daemon Pods 通信 与 DaemonSet 中的 Pod 进行通信的几种可能模式如下： 推送（Push）：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。 这些服务没有客户端。 NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。 DNS：创建具有相同 Pod 选择算符的 无头服务， 通过使用 endpoints 资源或从 DNS 中检索到多个 A 记录来发现DaemonSet。 Service：创建具有相同 Pod 选择算符的服务，并使用该服务随机访问到某个节点上的 守护进程（没有办法访问到特定节点）。 6. DaemonSet更新 如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod， 同时删除不匹配的节点上的 Pod。 你可以修改 DaemonSet 创建的 Pod。不过并非 Pod 的所有字段都可更新。 下次当某节点（即使具有相同的名称）被创建时，DaemonSet 控制器还会使用最初的模板。 您可以删除一个 DaemonSet。如果使用 kubectl 并指定 --cascade=false 选项， 则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet， 新的 DaemonSet 会收养已有的 Pod。 如果有 Pod 需要被替换，DaemonSet 会根据其 updateStrategy 来替换。 7. DaemonSet 滚动更新 Kubernetes 1.6 或者更高版本中才支持 DaemonSet 滚动更新功能 7.1 DaemonSet 更新策略 DaemonSet 有两种更新策略： OnDelete: 使用 OnDelete 更新策略时，在更新 DaemonSet 模板后，只有当你手动删除老的 DaemonSet pods 之后，新的 DaemonSet Pod 才会被自动创建。跟 Kubernetes 1.6 以前的版本类似。 RollingUpdate: 这是默认的更新策略。使用 RollingUpdate 更新策略时，在更新 DaemonSet 模板后， 老的DaemonSet pods 将被终止，并且将以受控方式自动创建新的 DaemonSet pods。 更新期间，最多只能有DaemonSet 的一个 Pod 运行于每个节点上。 7.2 执行滚动更新 启用 DaemonSet 的滚动更新功能，必须设置 .spec.updateStrategy.type 为 RollingUpdate。 你可能想设置 .spec.updateStrategy.rollingUpdate.maxUnavailable (默认为 1) 和 .spec.minReadySeconds (默认为 0)。 7.2.1 创建带有 RollingUpdate 更新策略的 DaemonSet 下面的 YAML 包含一个 DaemonSet，其更新策略为 'RollingUpdate'： apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 #滚动升级时允许的最大Unavailable的pod个数 template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can't run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 检查了 DaemonSet 清单中更新策略的设置之后，创建 DaemonSet： kubectl create -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml 另一种方式是如果你希望使用 kubectl apply 来更新 DaemonSet 的话，也可以 使用 kubectl apply 来创建 DaemonSet： kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml 7.2.2 检查 DaemonSet 的滚动更新策略 首先，检查 DaemonSet 的更新策略，确保已经将其设置为 RollingUpdate: kubectl get ds/fluentd-elasticsearch -o go-template='{{.spec.updateStrategy.type}}{{\"\\n\"}}' -n kube-system 如果还没在系统中创建 DaemonSet，请使用以下命令检查 DaemonSet 的清单： kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml --dry-run=client -o go-template='{{.spec.updateStrategy.type}}{{\"\\n\"}}' 两个命令的输出都应该为： RollingUpdate 如果输出不是 RollingUpdate，请返回并相应地修改 DaemonSet 对象或者清单。 7.2.3 更新 DaemonSet 模板 对 RollingUpdate DaemonSet 的 .spec.template 的任何更新都将触发滚动更新。 这可以通过几个不同的 kubectl 命令来完成。 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can't run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 声明式命令 如果你使用 配置文件 来更新 DaemonSet，请使用 kubectl apply: kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml 指令式命令 如果你使用 指令式命令 来更新 DaemonSets，请使用kubectl edit： kubectl edit ds/fluentd-elasticsearch -n kube-system 只更新容器镜像 如果你只需要更新 DaemonSet 模板里的容器镜像，比如，.spec.template.spec.containers[*].image, 请使用 kubectl set image: kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system 监视滚动更新状态 最后，观察 DaemonSet 最新滚动更新的进度： kubectl rollout status ds/fluentd-elasticsearch -n kube-system 当滚动更新完成时，输出结果如下： daemonset \"fluentd-elasticsearch\" successfully rolled out 8. DaemonSet 回滚 8.1 显示DaemonSet 回滚到的历史修订版本（revision） 如果只想回滚到最后一个版本，可以跳过这一步。 $ kubectl rollout history daemonset daemonsets \"\" REVISION CHANGE-CAUSE 1 ... 2 ... ... 在创建时，DaemonSet 的变化原因从 kubernetes.io/change-cause 注解（annotation） 复制到其修订版本中。用户可以在 kubectl 命令中设置 --record=true， 将执行的命令记录在变化原因注解中。 查看指定版本的详细信息： $ kubectl rollout history daemonset --revision=1 daemonsets \"\" with revision #1 Pod Template: Labels: foo=bar Containers: app: Image: ... Port: ... Environment: ... Mounts: ... Volumes: ... 8.2 回滚到指定版本 说明： 如果 --to-revision 参数未指定，将选中最近的版本。 # 在 --to-revision 中指定你从步骤 1 中获取的修订版本 $ kubectl rollout undo daemonset --to-revision= daemonset \"\" rolled back 8.3 监视 DaemonSet 回滚进度 $ kubectl rollout status ds/ daemonset \"\" successfully rolled out 8.4 理解 DaemonSet 修订版本 在前面的 kubectl rollout history 步骤中，你获得了一个修订版本列表，每个修订版本都存储在名为 ControllerRevision 的资源中。 要查看每个修订版本中保存的内容，可以找到 DaemonSet 修订版本的原生资源： $ kubectl get controllerrevision -l = NAME CONTROLLER REVISION AGE - DaemonSet/ 1 1h - DaemonSet/ 2 1h 每个 ControllerRevision 中存储了相应 DaemonSet 版本的注解和模板。 kubectl rollout undo 选择特定的 ControllerRevision，并用 ControllerRevision 中存储的模板代替 DaemonSet 的模板。 kubectl rollout undo 相当于通过其他命令（如 kubectl edit 或 kubectl apply） 将 DaemonSet 模板更新至先前的版本。 说明： 注意 DaemonSet 修订版本只会正向变化。也就是说，回滚完成后，所回滚到的 ControllerRevision 版本号(.revision 字段) 会增加。 例如，如果用户在系统中有版本 1 和版本 2，并从版本 2 回滚到版本 1， 带有.revision: 1 的ControllerRevision 将变为 .revision: 3。 参考： Kubernetes DaemonSet google cloud DaemonSet What is a DaemonSet? How To Use & Manage Kubernetes DaemonSets Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-16 07:20:46 "},"对象/kubernetes-PersistentVolume.html":{"url":"对象/kubernetes-PersistentVolume.html","title":"Volumes","keywords":"","body":"Kubernetes Volumes 理解| Persistent Volume, Persistent Volume Claim & Storage Class1. 共享存储机制概述2. PV2.1 PV的关键配置参数2.2 PV生命周期的各个阶段2.3 如何删除Terminating的pv3. PVC4. PV和PVC的生命周期4.1 资源供应5. StorageClass5.1 StorageClass的关键配置参数5.2 厂商与类型5.3 设置默认的StorageClass总结Kubernetes Volumes 理解| Persistent Volume, Persistent Volume Claim & Storage Class tagsstart 存储 对象 tagsstop 1. 共享存储机制概述 Kubernetes对于有状态的容器应用或者对数据需要持久化的应用，不仅需要将容器内的目录挂载到宿主机的目录或者emptyDir临时存储卷，而且需要更加可靠的存储来保存应用产生的重要数据，以便容器应用在重建之后仍然可以使用之前的数据。不过，存储资源和计算资源（CPU/内存）的管理方式完全不同。为了能够屏蔽底层存储实现的细节，让用户方便使用，同时让管理员方便管理，Kubernetes从1.0版本就引入PersistentVolume（PV）和PersistentVolumeClaim（PVC）两个资源对象来实现对存储的管理子系统。PV是对底层网络共享存储的抽象，将共享存储定义为一种“资源”，比如Node也是一种容器应用可以“消费”的资源。PV由管理员创建和配置，它与共享存储的具体实现直接相关，例如GlusterFS、iSCSI、RBD或GCE或AWS公有云提供的共享存储，通过插件式的机制完成与共享存储的对接，以供应用访问和使用。PVC则是用户对存储资源的一个“申请”。就像Pod“消费”Node的资源一样，PVC能够“消费”PV资源。PVC可以申请特定的存储空间和访问模式。使用PVC“申请”到一定的存储空间仍然不能满足应用对存储设备的各种需求。通常应用程序都会对存储设备的特性和性能有不同的要求，包括读写速度、并发性能、数据冗余等更高的要求，Kubernetes从1.4版本开始引入了一个新的资源对象StorageClass，用于标记存储资源的特性和性能。到1.6版本时，StorageClass和动态资源供应的机制得到了完善，实现了存储卷的按需创建，在共享存储的自动化管理进程中实现了重要的一步。 通过StorageClass的定义，管理员可以将存储资源定义为某种类别（Class），正如存储设备对于自身的配置描述（Profile），例如“快速存储”“慢速存储”“有数据冗余”“无数据冗余”等。用户根据StorageClass的描述就能够直观地得知各种存储资源的特性，就可以根据应用对存资源的需求去申请存储资源了。Kubernetes从1.9版本开始引入容器存储接口Container Storage Interface（CSI）机制，目标是在Kubernetes和外部存储系统之间建立一套标准的存储管理接口，通过该接口为容器提供存储服务，类似于CRI（容器运行时接口）和CNI（容器网络接口）。 2. PV PV作为存储资源，主要包括存储能力、访问模式、存储类型、回收策略、后端存储类型等关键信息的设置。下面的例子声明的PV具有如下属性：5GiB存储空间，访问模式为ReadWriteOnce，存储类型为slow（要求在系统中已存在名为slow的StorageClass），回收策略为Recycle，并且后端存储类型为nfs（设置了NFS Server的IP地址和路 径）： apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual persistentVolumeReclaimPolicy: Recycle capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" Kubernetes支持的PV类型如下。 ◎ AWSElasticBlockStore：AWS公有云提供的ElasticBlockStore。 ◎ AzureFile：Azure公有云提供的File。 ◎ AzureDisk：Azure公有云提供的Disk。 ◎ CephFS：一种开源共享存储系统。 ◎ FC（Fibre Channel）：光纤存储设备。 ◎ FlexVolume：一种插件式的存储机制。 ◎ Flocker：一种开源共享存储系统。 ◎ GCEPersistentDisk：GCE公有云提供的PersistentDisk。 ◎ Glusterfs：一种开源共享存储系统。 ◎ HostPath：宿主机目录，仅用于单机测试。 ◎ iSCSI：iSCSI存储设备。 ◎ Local：本地存储设备，从Kubernetes 1.7版本引入，到1.14版本时更新为稳定版，目前可以通过指定块（Block）设备提供Local PV，或通过社区开发的sig-storage-local-static-provisioner插件（https://github.com/kubernetes-sigs/sigstorage-local-static-provisioner）来管理Local PV的生命周期。 ◎ NFS：网络文件系统。 ◎ Portworx Volumes：Portworx提供的存储服务。 ◎ Quobyte Volumes：Quobyte提供的存储服务。 ◎ RBD（Ceph Block Device）：Ceph块存储。 ◎ ScaleIO Volumes：DellEMC的存储设备。 ◎ StorageOS：StorageOS提供的存储服务。 ◎ VsphereVolume：VMWare提供的存储系统。 每种存储类型都有各自的特点，在使用时需要根据它们各自的参数 进行设置 。 2.1 PV的关键配置参数 2.1.1 存储能力（Capacity） 描述存储设备具备的能力，目前仅支持对存储空间的设置（storage=xx），未来可能加入IOPS、吞吐率等指标的设置。 2.1.2 存储卷模式（Volume Mode） Kubernetes从1.13版本开始引入存储卷类型的设置（volumeMode=xxx），可选项包括Filesystem（文件系统）和Block（块设备），默认值为Filesystem。 目前有以下PV类型支持块设备类型： ◎ AWSElasticBlockStore ◎ AzureDisk ◎ FC ◎ GCEPersistentDisk ◎ iSCSI ◎ Local volume ◎ RBD（Ceph Block Device） ◎ VsphereVolume（alpha） 下面的例子为使用块设备的PV定义： 2.1.3 访问模式（Access Modes） 对PV进行访问模式的设置，用于描述用户的应用对存储资源的访问权限。访问模式如下。 ◎ ReadWriteOnce（RWO）：读写权限，并且只能被单个Node挂 载。 ◎ ReadOnlyMany（ROX）：只读权限，允许被多个Node挂载。 ◎ ReadWriteMany（RWX）：读写权限，允许被多个Node挂载。 某些PV可能支持多种访问模式，但PV在挂载时只能使用一种访问模式，多种访问模式不能同时生效。表8.1描述了不同的存储提供者支持的访问模式。 2.1.4 存储类别（Class） PV可以设定其存储的类别，通过storageClassName参数指定一个StorageClass资源对象的名称。具有特定类别的PV只能与请求了该类别的PVC进行绑定。未设定类别的PV则只能与不请求任何类别的PVC进行绑定。 2.1.5 回收策略（Reclaim Policy） 通过PV定义中的persistentVolumeReclaimPolicy字段进行设置，可选项如下。 保留（Retain） 保留数据，需要手工处理。回收策略 Retain 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象 被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\"已释放（released）\"。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 管理员可以通过下面的步骤来手动回收该卷： 删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产 （例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。 根据情况，手动清除所关联的存储资产上的数据。 手动删除所关联的存储资产；如果你希望重用该存储资产，可以基于存储资产的 定义创建新的 PersistentVolume 卷对象。删除（Delete) 对于支持 Delete 回收策略的卷插件，删除动作会将 PersistentVolume 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 动态供应的卷会继承其 StorageClass 中设置的回收策略，该策略默认 为 Delete。 管理员需要根据用户的期望来配置 StorageClass；否则 PV 卷被创建之后必须要被 编辑或者修补。参阅更改 PV 卷的回收策略 回收（Recycle） 警告： 回收策略 Recycle 已被废弃。取而代之的建议方案是使用动态供应。 简单清除文件的操作（例如执行rm -rf /thevolume/*命令）。 apiVersion: v1 kind: Pod metadata: name: pv-recycler namespace: default spec: restartPolicy: Never volumes: - name: vol hostPath: path: /any/path/it/will/be/replaced containers: - name: pv-recycler image: \"k8s.gcr.io/busybox\" command: [\"/bin/sh\", \"-c\", \"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* && test -z \\\"$(ls -A /scrub)\\\" || exit 1\"] volumeMounts: - name: vol mountPath: /scrub 定制回收器 Pod 模板中在 volumes 部分所指定的特定路径要替换为 正被回收的卷的路径。 目前，只有NFS和HostPath两种类型的存储支持Recycle策略；AWSEBS、GCE PD、Azure Disk和Cinder volumes支持Delete策略。 2.1.6 挂载参数（Mount Options） 在将PV挂载到一个Node上时，根据后端存储的特点，可能需要设置额外的挂载参数，可以根据PV定义中的mountOptions字段进行设置。下面的例子为对一个类型为gcePersistentDisk的PV设置挂载参数： 目前，以下PV类型支持设置挂载参数： ◎ AWSElasticBlockStore ◎ AzureDisk ◎ AzureFile ◎ CephFS ◎ Cinder (OpenStack block storage) ◎ GCEPersistentDisk ◎ Glusterfs ◎ NFS ◎ Quobyte Volumes ◎ RBD (Ceph Block Device) ◎ StorageOS ◎ VsphereVolume ◎ iSCSI 2.1.7 节点亲和性（Node Affinity） PV可以设置节点亲和性来限制只能通过某些Node访问Volume，可以在PV定义中的nodeAffinity字段进行设置。使用这些Volume的Pod将被调度到满足条件的Node上。 这个参数仅用于Local存储卷，例如： 公有云提供的存储卷（如AWS EBS、GCE PD、Azure Disk等）都由公有云自动完成节点亲和性设置，无须用户手工设置. 2.1.8 预留 PersistentVolume 通过在 PersistentVolumeClaim 中指定 PersistentVolume，你可以声明该特定 PV 与 PVC 之间的绑定关系。如果该 PersistentVolume 存在且未被通过其 claimRef 字段预留给 PersistentVolumeClaim，则该 PersistentVolume 会和该 PersistentVolumeClaim 绑定到一起。 绑定操作不会考虑某些卷匹配条件是否满足，包括节点亲和性等等。 控制面仍然会检查 存储类、访问模式和所请求的 存储尺寸都是合法的。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: foo-pvc namespace: foo spec: storageClassName: \"\" # 此处须显式设置空字符串，否则会被设置为默认的 StorageClass volumeName: foo-pv ... 此方法无法对 PersistentVolume 的绑定特权做出任何形式的保证。 如果有其他 PersistentVolumeClaim 可以使用你所指定的 PV，则你应该首先预留 该存储卷。你可以将 PV 的 claimRef 字段设置为相关的 PersistentVolumeClaim 以确保其他 PVC 不会绑定到该 PV 卷。 apiVersion: v1 kind: PersistentVolume metadata: name: foo-pv spec: storageClassName: \"\" claimRef: name: foo-pvc namespace: foo ... 如果你想要使用 claimPolicy 属性设置为 Retain 的 PersistentVolume 卷 时，包括你希望复用现有的 PV 卷时，这点是很有用的。 2.2 PV生命周期的各个阶段 某个PV在生命周期中可能处于以下4个阶段（Phaes）之一。 ◎ Available：可用状态，还未与某个PVC绑定。 ◎ Bound：已与某个PVC绑定。 ◎ Released：绑定的PVC已经删除，资源已释放，但没有被集群 回收。 ◎ Failed：自动资源回收失败。 PVC 对象通常由开发人员创建；或者以 PVC 模板的方式成为 StatefulSet 的一部分，然后由 StatefulSet控制器负责创建带编号的 PVC。 那么，这个 PV 对象，又是如何变成容器里的一个持久化存储的呢？ 所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。 而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。 显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。 而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。 当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： /var/lib/kubelet/pods//volumes/kubernetes.io~/ 接下来，kubelet 要做的操作就取决于你的 Volume 类型了。如果你的 Volume 类型是远程块存储，比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。 备注：你如果不太了解块存储的话，可以直接把它理解为：一块磁盘。 这相当于执行： $ gcloud compute instances attach-disk --disk 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。 Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行： # 通过lsblk命令获取磁盘设备ID $ sudo lsblk # 格式化成ext4格式 $ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/ # 挂载到挂载点 $ sudo mkdir -p /var/lib/kubelet/pods//volumes/kubernetes.io~/ 这个将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。 Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。而如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet 的处理过程就会更简单一些。因为在这种情况下，kubelet 可以跳过“第一阶段”（Attach）的操作，这是因为一般来说，远程文件存储并没有一个“存储设备”需要挂载在宿主机上。所以，kubelet 会直接从“第二阶段”（Mount）开始准备宿主机上的 Volume 目录。 在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令： $ mount -t nfs :/ /var/lib/kubelet/pods//volumes/kubernetes.io~/ 通过这个挂载操作，Volume 的宿主机目录就成为了一个远程 NFS 目录的挂载点，后面你在这个目录里写入的所有文件，都会被保存在远程 NFS 服务器上。所以，我们也就完成了对这个 Volume 宿主机目录的“持久化”。 到这里，你可能会有疑问，Kubernetes 又是如何定义和区分这两个阶段的呢？ 其实很简单，在具体的 Volume 插件的实现接口上，Kubernetes 分别给这两个阶段提供了两种不同的参数列表： 对于“第一阶段”（Attach），Kubernetes 提供的可用参数是 nodeName，即宿主机的名字。 而对于“第二阶段”（Mount），Kubernetes 提供的可用参数是 dir，即 Volume 的宿主机目录。 经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令： $ docker run -v /var/lib/kubelet/pods//volumes/kubernetes.io~/:/ 我的镜像 ... 以上，就是 Kubernetes 处理 PV 的具体原理了。 对应地，在删除一个 PV 的时候，Kubernetes 也需要 Unmount 和 Dettach 两个阶段来处理。这个过程我就不再详细介绍了，执行“反向操作”即可。 实际上，你可能已经发现，这个 PV 的处理流程似乎跟 Pod 以及容器的启动流程没有太多的耦合，只要 kubelet 在向 Docker 发起 CRI 请求之前，确保“持久化”的宿主机目录已经处理完毕即可。 所以，在 Kubernetes 中，上述关于 PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的。 其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作：AttachDetachController。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。 需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。 而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作：VolumeManagerReconciler，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。 通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。 2.3 如何删除Terminating的pv finalizers不为空，无法删除pv $ k get pv prometheus-pv -oyaml apiVersion: v1 kind: PersistentVolume metadata: annotations: pv.kubernetes.io/bound-by-controller: \"yes\" creationTimestamp: \"2022-04-25T06:24:04Z\" deletionGracePeriodSeconds: 0 deletionTimestamp: \"2022-04-25T10:23:47Z\" finalizers: - kubernetes.io/pv-protection name: prometheus-pv ............ 执行： $ kubectl patch pv prometheus-pv -p '{\"metadata\":{\"finalizers\":null}}' 3. PVC PVC作为用户对存储资源的需求申请，主要包括存储空间请求、访问模式、PV选择条件和存储类别等信息的设置。下例声明的PVC具有如下属性：申请8GiB存储空间，访问模式为ReadWriteOnce，PV 选择条件为包含标签“release=stable”并且包含条件为“environment In [dev]”的标签，存储类别为“slow”（要求在系统中已存在名为slow的StorageClass）： PVC的关键配置参数说明如下。 ◎ 资源请求（Resources）：描述对存储资源的请求，目前仅支持request.storage的设置，即存储空间大小。 ◎ 访问模式（Access Modes）：PVC也可以设置访问模式，用于描述用户应用对存储资源的访问权限。其三种访问模式的设置与PV的设置相同。 ◎ 存储卷模式（Volume Modes）：PVC也可以设置存储卷模式，用于描述希望使用的PV存储卷模式，包括文件系统和块设备。 ◎ PV选择条件（Selector）：通过对Label Selector的设置，可使PVC对于系统中已存在的各种PV进行筛选。系统将根据标签选出合适的PV与该PVC进行绑定。选择条件可以使用matchLabels和matchExpressions进行设置，如果两个字段都设置了，则Selector的逻辑将是两组条件同时满足才能完成匹配。 ◎ 存储类别（Class）：PVC 在定义时可以设定需要的后端存储的类别（通过storageClassName字段指定），以减少对后端存储特性的详细信息的依赖。只有设置了该Class的PV才能被系统选出，并与该PVC进行绑定。PVC也可以不设置Class需求。如果storageClassName字段的值被设置为空（storageClassName=\"\"），则表示该PVC不要求特定的Class，系统将只选择未设定Class的PV与之匹配和绑定。PVC也可以完全不设置storageClassName字段，此时将根据系统是否启用了名为DefaultStorageClass的admission controller进行相应的操作。 ◎ 未启用DefaultStorageClass：等效于PVC设置storageClassName的值为空（storageClassName=\"\"），即只能选择未设定Class的PV与之匹配和绑定。 ◎ 启用DefaultStorageClass：要求集群管理员已定义默认的StorageClass。如果在系统中不存在默认的StorageClass，则等效于不启用DefaultStorageClass的情况。如果存在默认的StorageClass，则系统将自动为PVC创建一个PV（使用默认StorageClass的后端存储），并将它们进行绑定。集群管理员设置默认StorageClass的方法为，在StorageClass的定义中加上一个annotation“storageclass.kubernetes.io/isdefault-class= true”。如果管理员将多个StorageClass都定义为default，则由于不唯一，系统将无法为PVC创建相应的PV。 注意，PVC和PV都受限于Namespace，PVC在选择PV时受到Namespace的限制，只有相同Namespace中的PV才可能与PVC绑定。Pod在引用PVC时同样受Namespace的限制，只有相同Namespace中的PVC才能挂载到Pod内。 当Selector和Class都进行了设置时，系统将选择两个条件同时满足的PV与之匹配另外，如果资源供应使用的是动态模式，即管理员没有预先定义PV，仅通过StorageClass交给系统自动完成PV的动态创建，那么PVC再设定Selector时，系统将无法为其供应任何存储资源。 在启用动态供应模式的情况下，一旦用户删除了PVC，与之绑定的PV也将根据其默认的回收策略“Delete”被删除。如果需要保留PV（用户数据），则在动态绑定成功后，用户需要将系统自动生成PV的回收策略从“Delete”改成“Retain”。 开发人员可以声明一个 1 GiB 大小的 PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi 而用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分： 第一个条件，当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。 而第二个条件，则是 PV 和 PVC 的 storageClassName 字段必须一样。 在成功地将 PVC 和 PV 进行绑定之后，Pod 就能够像使用 hostPath 等常规类型的 Volume 一样，在自己的 YAML 文件里声明使用这个 PVC 了，如下所示： apiVersion: v1 kind: Pod metadata: labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs PVC 和 PV 的设计，其实跟“面向对象”的思想完全一致。 PVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则由 PV 负责完成。这样做的好处是，作为应用开发者，我们只需要跟 PVC 这个“接口”打交道，而不必关心具体的实现是 NFS 还是 Ceph。 4. PV和PVC的生命周期 我们可以将PV看作可用的存储资源，PVC则是对存储资源的需求，PV和PVC的相互关系遵循如图8.1所示的生命周期。 4.1 资源供应 Kubernetes支持两种资源的供应模式：静态模式（Static）和动态模式（Dynamic）。资源供应的结果就是创建好的PV。 ◎ 静态模式：集群管理员手工创建许多PV，在定义PV时需要将后端存储的特性进行设置。 ◎ 动态模式：集群管理员无须手工创建PV，而是通过StorageClass的设置对后端存储进行描述，标记为某种类型。此时要求PVC对存储的类型进行声明，系统将自动完成PV的创建及与PVC的绑定。PVC可以声明Class为\"\"，说明该PVC禁止使用动态模式。 图8.3描述了在动态资源供应模式下，通过StorageClass和PVC完成资源动态绑定（系统自动生成PV），并供Pod使用的存储管理机制。 PV和PVC的生命周期 Available（可用）——一块空闲资源还没有被任何声明绑定 Bound（已绑定）——卷已经被声明绑定 Released（已释放）——声明被删除，但是资源还未被集群重新声明 Failed（失败）——该卷的自动回收失败 在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器，叫作 Volume Controller。这个 Volume Controller 维护着多个控制循环，其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。 5. StorageClass 在了解了 Kubernetes 的 Volume 处理机制之后，我再来为你介绍这个体系里最后一个重要概念：StorageClass。 我在前面介绍 PV 和 PVC 的时候，曾经提到过，PV 这个对象的创建，是由运维人员完成的。但是，在大规模的生产环境里，这其实是一个非常麻烦的工作。这是因为，一个大规模的 Kubernetes 集群里很可能有成千上万个 PVC，这就意味着运维人员必须得事先创建出成千上万个 PV。更麻烦的是，随着新的 PVC 不断被提交，运维人员就不得不继续添加新的、能满足条件的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而失败。在实际操作中，这几乎没办法靠人工做到。 所以，Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。相比之下，前面人工管理 PV 的方式就叫作 Static Provisioning。Dynamic Provisioning 机制工作的核心，在于一个名叫 StorageClass 的 API 对象。 而 StorageClass 对象的作用，其实就是创建 PV 的模板。 具体地说，StorageClass 对象会定义如下两个部分内容： 第一，PV 的属性。比如，存储类型、Volume 的大小等等。 第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。 有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到StorageClass作为对存储资源的抽象定义，对用户设置的PVC申请屏蔽后端存储的细节，一方面减少了用户对于存储资源细节的关注，另一方面减轻了管理员手工管理PV的工作，由系统自动完成PV的创建和绑定，实现了动态的资源供应。基于StorageClass的动态资源供应模式将逐步成为云平台的标准存储配置模式。 StorageClass的定义主要包括名称、后端存储的提供者（provisioner）和后端存储的相关参数配置。StorageClass一旦被创建出来，则将无法修改。如需更改，则只能删除原StorageClass的定义重建。下例定义了一个名为standard的StorageClass，提供者为aws-ebs，其 参数设置了一个type，值为gp2： apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain allowVolumeExpansion: true mountOptions: - debug volumeBindingMode: Immediate 5.1 StorageClass的关键配置参数 提供者（Provisioner） 描述存储资源的提供者，也可以看作后端存储驱动。目前Kubernetes支持的Provisioner都以“kubernetes.io/”为开头，用户也可以使用自定义的后端存储提供者。为了符合StorageClass的用法，自定义Provisioner需要符合存储卷的开发规范。 参数（Parameters） 后端存储资源提供者的参数设置，不同的Provisioner包括不同的参数设置。某些参数可以不显示设定，Provisioner将使用其默认值。接下来通过几种常见的Provisioner对StorageClass的定义进行详细说明。 5.2 厂商与类型 GCE apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 在这个 YAML 文件里，我们定义了一个名叫 block-service 的 StorageClass。这个 StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。 而这个 StorageClass 的 parameters 字段，就是 PV 的参数。比如：上面例子里的 type=pd-ssd，指的是这个 PV 的类型是“SSD 格式的 GCE 远程磁盘”。 ceph 如果你想使用我们之前部署在本地的 Kubernetes 集群以及 Rook 存储服务的话，你的 StorageClass 需要使用如下所示的 YAML 文件来定义： apiVersion: ceph.rook.io/v1beta1 kind: Pool metadata: name: replicapool namespace: rook-ceph spec: replicated: size: 3 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: ceph.rook.io/block parameters: pool: replicapool #The value of \"clusterNamespace\" MUST be the same as the one in which your rook cluster exist clusterNamespace: rook-ceph 在这个 YAML 文件中，我们定义的还是一个名叫 block-service 的 StorageClass，只不过它声明使的存储插件是由 Rook 项目。有了 StorageClass 的 YAML 文件之后，运维人员就可以在 Kubernetes 里创建这个 StorageClass 了： $ kubectl create -f sc.yaml 这时候，作为应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: block-service resources: requests: storage: 30Gi 可以看到，我们在这个 PVC 里添加了一个叫作 storageClassName 的字段，用于指定该 PVC 所要使用的 StorageClass 的名字是：block-service。 $ kubectl create -f pvc.yaml $ kubectl describe pvc claim1 Name: claim1 Namespace: default StorageClass: block-service Status: Bound Volume: pvc-e5578707-c626-11e6-baf6-08002729a32b Labels: Capacity: 30Gi Access Modes: RWO No Events. $ kubectl describe pv pvc-e5578707-c626-11e6-baf6-08002729a32b Name: pvc-e5578707-c626-11e6-baf6-08002729a32b Labels: StorageClass: block-service Status: Bound Claim: default/claim1 Reclaim Policy: Delete Access Modes: RWO Capacity: 30Gi ... No events. 此外，你还可以看到，这个自动创建出来的 PV 的 StorageClass 字段的值，也是 block-service。这是因为，Kubernetes 只会将 StorageClass 相同的 PVC 和 PV 绑定起来。 AWS EBS存储卷 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/aws-ebs parameters: type: io1 iopsPerGB: \"10\" fsType: ext4 type：io1，gp2，sc1，st1。详细信息参见 AWS 文档。默认值：gp2。 zone(弃用)：AWS 区域。如果没有指定 zone 和 zones， 通常卷会在 Kubernetes集群节点所在的活动区域中轮询调度分配。 zone 和 zones 参数不能同时使用。 zones(弃用)：以逗号分隔的 AWS 区域列表。 如果没有指定 zone 和 zones，通常卷会在 Kubernetes集群节点所在的 活动区域中轮询调度分配。zone和zones参数不能同时使用。 iopsPerGB：只适用于 io1 卷。每 GiB 每秒 I/O 操作。 AWS 卷插件将其与请求卷的大小相乘以计算 IOPS 的容量，并将其限制在 20000 IOPS（AWS 支持的最高值，请参阅 AWS 文档。 这里需要输入一个字符串，即 \"10\"，而不是 10。 fsType：受 Kubernetes 支持的文件类型。默认值：\"ext4\"。 encrypted：指定 EBS 卷是否应该被加密。合法值为 \"true\" 或者 \"false\"。 这里需要输入字符串，即 \"true\",而非 true。 kmsKeyId：可选。加密卷时使用密钥的完整 Amazon 资源名称。 如果没有提供，但 encrypted 值为 true，AWS生成一个密钥。关于有效的 ARN 值，请参阅 AWS 文档。 Glusterfs apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://127.0.0.1:8081\" clusterid: \"630372ccdc720a92c681fb928f27b53f\" restauthenabled: \"true\" restuser: \"admin\" secretNamespace: \"default\" secretName: \"heketi-secret\" gidMin: \"40000\" gidMax: \"50000\" volumetype: \"replicate:3\" resturl：制备 gluster 卷的需求的 Gluster REST 服务/Heketi 服务 url。 通用格式应该是 IPaddress:Port，这是 GlusterFS 动态制备器的必需参数。 如果 Heketi 服务在 OpenShift/kubernetes 中安装并暴露为可路由服务，则可以使用类似于http://heketi-storage-project.cloudapps.mystorage.com 的格式，其中 fqdn是可解析的 heketi 服务网址。 restauthenabled：Gluster REST 服务身份验证布尔值，用于启用对 REST 服务器的身份验证。 如果此值为'true'，则必须填写 restuser 和 restuserkey 或 secretNamespace + secretName。此选项已弃用，当在指定 restuser、restuserkey、secretName 或 secretNamespace时，身份验证被启用。 restuser：在 Gluster 可信池中有权创建卷的 Gluster REST服务/Heketi 用户。 restuserkey：Gluster REST 服务/Heketi 用户的密码将被用于对 REST 服务器进行身份验证。此参数已弃用，取而代之的是 secretNamespace + secretName。 secretNamespace，secretName：Secret 实例的标识，包含与 Gluster REST服务交互时使用的用户密码。 这些参数是可选的，secretNamespace 和 secretName 都省略时使用空密码。 所提供的Secret 必须将类型设置为 \"kubernetes.io/glusterfs\"，例如以这种方式创建： kubectl create secret generic heketi-secret \\ --type=\"kubernetes.io/glusterfs\" --from-literal=key='opensesame' \\ --namespace=default Secret 的例子可以在 glusterfs-provisioning-secret.yaml 中找到。 clusterid：630372ccdc720a92c681fb928f27b53f 是集群的 ID，当制备卷时， Heketi 将会使用这个文件。它也可以是一个 clusterid 列表，例如： \"8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397\"。这个是可选参数。 gidMin，gidMax：storage class GID 范围的最小值和最大值。在此范围（gidMin-gidMax）内的唯一值（GID）将用于动态制备卷。这些是可选的值。 如果不指定，所制备的卷为一个2000-2147483647 之间的值，这是 gidMin 和 gidMax 的默认值。 volumetype：卷的类型及其参数可以用这个可选值进行配置。如果未声明卷类型，则 由制备器决定卷的类型。 例如： 'Replica volume': volumetype: replicate:3 其中 '3' 是 replica 数量. 'Disperse/EC volume': volumetype: disperse:4:2 其中 '4' 是数据，'2' 是冗余数量. 'Distribute volume': volumetype: none 有关可用的卷类型和管理选项，请参阅 管理指南。 更多相关的参考信息，请参阅 如何配置 Heketi。 当动态制备持久卷时，Gluster 插件自动创建名为 gluster-dynamic- 的端点和无头服务。在 PVC 被删除时动态端点和无头服务会自动被删除。 本地 FEATURE STATE: Kubernetes v1.14 [stable] kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 本地卷还不支持动态制备，然而还是需要创建 StorageClass 以延迟卷绑定， 直到完成 Pod 的调度。这是由 WaitForFirstConsumer 卷绑定模式指定的。 延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的 PersistentVolume 时能考虑到所有 Pod 的调度限制。 5.3 设置默认的StorageClass a. 修改kube-apiserver参数 要在系统中设置一个默认的StorageClass，则首先需要启用名为 DefaultStorageClass的admission controller，即在kube-apiserver的命令行 参数--admission-control中增加： --admission-control=...,DefaultStorageClass b. 标记默认 StorageClass 非默认 $ kubectl get storageclass NAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d kubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' c. 标记一个 StorageClass 为默认 在StorageClass的定义中设置一个annotation： 或者 kubectl patch storageclass -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl get storageclass 输出类似这样： NAME PROVISIONER AGE standard kubernetes.io/gce-pd 1d gold (default) kubernetes.io/gce-pd 1d 有了 Dynamic Provisioning 机制，运维人员只需要在 Kubernetes 集群里创建出数量有限的 StorageClass 对象就可以了。这就好比，运维人员在 Kubernetes 集群里创建出了各种各样的 PV 模板。这时候，当开发人员提交了包含 StorageClass 字段的 PVC 之后，Kubernetes 就会根据这个 StorageClass 创建出对应的 PV。 需要注意的是，StorageClass 并不是专门为了 Dynamic Provisioning 而设计的。 比如，在本篇一开始的例子里，我在 PV 和 PVC 里都声明了 storageClassName=manual。而我的集群里，实际上并没有一个名叫 manual 的 StorageClass 对象。这完全没有问题，这个时候 Kubernetes 进行的是 Static Provisioning，但在做绑定决策的时候，它依然会考虑 PV 和 PVC 的 StorageClass 定义。 而这么做的好处也很明显：这个 PVC 和 PV 的绑定关系，就完全在我自己的掌控之中。 这里，你可能会有疑问，我在之前讲解 StatefulSet 存储状态的例子时，好像并没有声明 StorageClass 啊？ 实际上，如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的 StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟 storageClassName 也是“”的 PV 进行绑定。 总结 StorageClass 到底是干什么用的。这些概念之间的关系，可以用如下所示的一幅示意图描述： 从图中我们可以看到，在这个体系中： PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。 PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。 而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。 当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。 参考： k8s配置动态存储管理 glusterFS kubernetes 持久存储 pv StorageClass [storage.k8s.io/v1]管理 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 10:11:15 "},"对象/Kubernetes-StatefulSet.html":{"url":"对象/Kubernetes-StatefulSet.html","title":"StatefulSet","keywords":"","body":"Kubernetes StatefulSet0. 简介1. 创建 StatefulSet2. 扩容/缩容 StatefulSet3. 更新 StatefulSet3.1 Rolling Update 策略3.2 分段更新3.3 灰度发布4. 删除 StatefulSet4.1 非级联删除4.2 级联删除5. Pod 管理策略5.1 Parallel Pod 管理策略6. 示例Kubernetes StatefulSet tagsstart StatefulSet 对象 tagsstop 《异形》 0. 简介 StatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理 Deployment 和扩展一组 Pod，并且能为这些 Pod 提供序号和唯一性保证。 和 Deployment 相同的是，StatefulSet 管理了基于相同容器定义的一组 Pod。但和 Deployment 不同的是，StatefulSet 为它们的每个 Pod 维护了一个固定的 ID。这些 Pod 是基于相同的声明来创建的，但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 StatefulSet 和其他控制器使用相同的工作模式。你在 StatefulSet 对象 中定义你期望的状态，然后 StatefulSet 的 控制器 就会通过各种更新来达到那种你想要的状态。 使用 StatefulSets 1. 创建 StatefulSet 它创建了一个 Headless Service nginx 用来发布 StatefulSet web 中的 Pod 的 IP 地址。 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi 你需要使用两个终端窗口。在第一个终端中，使用 kubectl get 来查看 StatefulSet 的 Pods 的创建情况。 kubectl get pods -w -l app=nginx 在另一个终端中，使用 kubectl apply来创建定义在 web.yaml 中的 Headless Service 和 StatefulSet。 $ kubectl apply -f web.yaml service/nginx created statefulset.apps/web created $ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None 80/TCP 12s $ kubectl get statefulset web NAME DESIRED CURRENT AGE web 2 1 20s $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 19s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。Pod 的名称的形式为-。webStatefulSet 拥有两个副本，所以它创建了两个 Pod：web-0和web-1 $ for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done web-0 web-1 $ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.6 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.6 headless service 的 CNAME 指向 SRV 记录（记录每个 Running 和 Ready 状态的 Pod）。SRV 记录指向一个包含 Pod IP 地址的记录表项。 kubectl get pod -w -l app=nginx kubectl delete pod -l app=nginx pod \"web-0\" deleted pod \"web-1\" deleted 等待 StatefulSet 重启它们，并且两个 Pod 都变成 Running 和 Ready 状态。 kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 34s 使用 kubectl exec 和 kubectl run 查看 Pod 的主机名和集群内部的 DNS 表项。 for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done web-0 web-1 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.7 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变。在本教程中使用的集群中它们就改变了。这就是为什么不要在其他应用中使用 StatefulSet 中的 Pod 的 IP 地址进行连接，这点很重要。 如果你需要查找并连接一个 StatefulSet 的活动成员，你应该查询 Headless Service 的 CNAME。和 CNAME 相关联的 SRV 记录只会包含 StatefulSet 中处于 Running 和 Ready 状态的 Pod。 如果你的应用已经实现了用于测试 liveness 和 readiness 的连接逻辑，你可以使用 Pod 的 SRV 记录（web-0.nginx.default.svc.cluster.local， web-1.nginx.default.svc.cluster.local）。因为他们是稳定的，并且当你的 Pod 的状态变为 Running 和 Ready 时，你的应用就能够发现它们的地址。 2. 扩容/缩容 StatefulSet 2.1 扩容 $ kubectl scale sts web --replicas=5 #扩展副本数为 5 $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2h web-1 1/1 Running 0 2h NAME READY STATUS RESTARTS AGE web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 ContainerCreating 0 0s web-3 1/1 Running 0 18s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 0s web-4 1/1 Running 0 19s 2.2 缩容 $ kubectl patch sts web -p '{\"spec\":{\"replicas\":3}}' $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3h web-1 1/1 Running 0 3h web-2 1/1 Running 0 55s web-3 1/1 Running 0 36s web-4 0/1 ContainerCreating 0 18s NAME READY STATUS RESTARTS AGE web-4 1/1 Running 0 19s web-4 1/1 Terminating 0 24s web-4 1/1 Terminating 0 24s web-3 1/1 Terminating 0 42s web-3 1/1 Terminating 0 42s 3. 更新 StatefulSet 更新策略由 StatefulSet API Object 的spec.updateStrategy 字段决定。这个特性能够用来更新一个 StatefulSet 中的 Pod 的 container images，resource requests，以及 limits，labels 和 annotations。RollingUpdate滚动更新是 StatefulSets 默认策略。 3.1 Rolling Update 策略 $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\"}}}' statefulset.apps/web patched $ kubectl patch statefulset web --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"gcr.io/google_containers/nginx-slim:0.8\"}]' statefulset.apps/web patched $ kubectl get po -l app=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 7m web-1 1/1 Running 0 7m web-2 1/1 Running 0 8m web-2 1/1 Terminating 0 8m web-2 1/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-1 1/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 6s web-0 1/1 Terminating 0 7m web-0 1/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 10s 获取 Pod 来查看他们的容器镜像。 $ for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 kubectl rollout status sts/ 也可以查看 rolling update 的状态。 3.2 分段更新 你可以使用 RollingUpdate 更新策略的 partition 参数来分段更新一个 StatefulSet。分段的更新将会使 StatefulSet 中的其余所有 Pod 保持当前版本的同时仅允许改变 StatefulSet 的 .spec.template。 Patch web StatefulSet 来对 updateStrategy 字段添加一个分区。 $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":3}}}}' statefulset.apps/web patched 再次 Patch StatefulSet 来改变容器镜像。 $ kubectl patch statefulset web --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"k8s.gcr.io/nginx-slim:0.7\"}]' statefulset.apps/web patched 删除 StatefulSet 中的 Pod。 $ kubectl delete po web-2 pod \"web-2\" deleted 3.3 灰度发布 通过 patch 命令修改 StatefulSet 来减少分区。 $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":2}}}}' statefulset.apps/web patched $ kubectl get po -lapp=nginx -w #等待 web-2 变成 Running 和 Ready。 4. 删除 StatefulSet 4.1 非级联删除 --cascade=false 参数给命令。这个参数告诉 Kubernetes 只删除 StatefulSet 而不要删除它的任何 Pod。 $ kubectl delete statefulset web --cascade=false statefulset.apps \"web\" deleted $ kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 1/1 Running 0 7m web-2 1/1 Running 0 5m $ kubectl delete pod web-0 pod \"web-0\" deleted $ kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 10m web-2 1/1 Running 0 7m $ kubectl apply -f web.yaml statefulset.apps/web created service/nginx unchanged $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 16m web-2 1/1 Running 0 2m NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 18s web-2 1/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m 当重新创建 web StatefulSet 时，web-0被第一个重新启动。由于 web-1 已经处于 Running 和 Ready 状态，当 web-0 变成 Running 和 Ready 时，StatefulSet 会直接接收这个 Pod。由于你重新创建的 StatefulSet 的 replicas 等于 2，一旦 web-0 被重新创建并且 web-1 被认为已经处于 Running 和 Ready 状态时，web-2将会被终止。 4.2 级联删除 $ kubectl delete statefulset web $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 11m web-1 1/1 Running 0 27m NAME READY STATUS RESTARTS AGE web-0 1/1 Terminating 0 12m web-1 1/1 Terminating 0 29m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m 虽然级联删除会删除 StatefulSet 和它的 Pod，但它并不会删除和 StatefulSet 关联的 Headless Service。你必须手动删除nginx Service。 $ kubectl delete service nginx service \"nginx\" deleted 5. Pod 管理策略 5.1 Parallel Pod 管理策略 Parallel pod 管理策略告诉 StatefulSet 控制器并行的终止所有 Pod，在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态. apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" podManagementPolicy: \"Parallel\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi 执行：效果并行执行 kubectl get po -lapp=nginx -w kubectl apply -f web-parallel.yaml kubectl apply -f web-parallel.yaml kubectl delete sts web kubectl delete svc nginx 6. 示例 创建service apiVersion: v1 kind: Service metadata: labels: app: cassandra name: cassandra spec: clusterIP: None ports: - port: 9042 selector: app: cassandra kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml 创建stateful apiVersion: apps/v1 kind: StatefulSet metadata: name: cassandra labels: app: cassandra spec: serviceName: cassandra replicas: 3 selector: matchLabels: app: cassandra template: metadata: labels: app: cassandra spec: terminationGracePeriodSeconds: 1800 containers: - name: cassandra image: gcr.io/google-samples/cassandra:v13 imagePullPolicy: Always ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql resources: limits: cpu: \"500m\" memory: 1Gi requests: cpu: \"500m\" memory: 1Gi securityContext: capabilities: add: - IPC_LOCK lifecycle: preStop: exec: command: - /bin/sh - -c - nodetool drain env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEEDS value: \"cassandra-0.cassandra.default.svc.cluster.local\" - name: CASSANDRA_CLUSTER_NAME value: \"K8Demo\" - name: CASSANDRA_DC value: \"DC1-K8Demo\" - name: CASSANDRA_RACK value: \"Rack1-K8Demo\" - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP readinessProbe: exec: command: - /bin/bash - -c - /ready-probe.sh initialDelaySeconds: 15 timeoutSeconds: 5 # These volume mounts are persistent. They are like inline claims, # but not exactly because the names need to match exactly one of # the stateful pod volumes. volumeMounts: - name: cassandra-data mountPath: /cassandra_data # These are converted to volume claims by the controller # and mounted at the paths mentioned above. # do not use these in production until ssd GCEPersistentDisk or other ssd pd volumeClaimTemplates: - metadata: name: cassandra-data spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: fast resources: requests: storage: 1Gi --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: k8s.io/minikube-hostpath parameters: type: pd-ssd # 创建 statefulset kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml 参考资料： StatefulSets 基础 使用 StatefulSets 部署 Cassandra Kubernetes StatefulSet - Examples & Best Practices 宋净超 kubernetes StatefulSet google cloud StatefulSet 详解 Kubernetes StatefulSet 实现原理 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 05:31:05 "},"对象/Kubernetes-Job.html":{"url":"对象/Kubernetes-Job.html","title":"Job","keywords":"","body":"kubernetes job1. 简介2. 参数3. job创建过程细讲4. Job Controller 对并行作业的控制方法5. Job Controller 的工作原理6. 三种job方法6.1 外部管理器 +Job 模板6.2 拥有固定任务数目的并行 Job6.3 指定并行度（parallelism），但不设置固定的 completions 的值7. CronJob定时8. 实战8.1 非并发Job8.2 粗并发Job8.3 CronJob8.4. Job的自动清理kubernetes job tagsstart job cronjob 对象 tagsstop 《相助》一部关于黑人种族主题电影 @[toc] 1. 简介 Job对象通常用于运行那些仅需要执行一次的任务（例如数据库迁移，批处理脚本等等）。通过Job对象创建运行的Pod具有高可靠性，因为Job Controller会自动重启运行失败的Pod（例如Pod所在Node重启或宕机）。 Job的本质是确保一个或多个Pod健康地运行直至运行完毕。 2. 参数 .spec.completions #需要成功执行的次数 .spec.parallelism #并发的数量 .spec.template.spec.restartPolicy.spec.template.spec.restartPolicy属性拥有三个候选值：OnFailure，Never和Always。默认值为Always。它主要用于描述Pod内容器的重启策略。在Job中只能将此属性设置为OnFailure或Never。如果.spec.template.spec.restartPolicy = OnFailure，如果Pod内某个容器的exit code不为0，那么Pod就会在其内部重启这个容器。.spec.template.spec.restartPolicy = Never，那么Pod内某个容器exit code不为0时，就不会触发容器的重启 .spec.backoffLimit .spec.backoffLimit用于设置Job的容错次数，默认值为6。当Job运行的Pod失败次数到达.spec.backoffLimit次时，Job Controller不再新建Pod，直接停止运行这个Job，将其运行结果标记为Failure。另外，Pod运行失败后再次运行的时间间隔呈递增状态，例如10s，20s，40s。。。 .spec.activeDeadlineSeconds .spec.activeDeadlineSeconds属性用于设置Job运行的超时时间。如果Job运行的时间超过了设定的秒数，那么此Job就自动停止运行所有的Pod，并将Job退出状态标记为reason:DeadlineExceeded。 ttlSecondsAfterFinished 1.12版本之后，k8s提出了通过TTL自动删除Job的特性，当前仅对job生效，对 Complete 和 Failed 状态的Job都会自动删除，以后会逐步对所有的其他资源对象生效。 Job pi-with-ttl 的 ttlSecondsAfterFinished 值为 100，则，在其结束 100 秒之后，将可以被自动删除 如果 ttlSecondsAfterFinished 被设置为 0，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod 如果 ttlSecondsAfterFinished 值未设置，则 TTL 控制器不会清理该 Job 截止日期 该.spec.startingDeadlineSeconds字段是可选的。如果它由于任何原因错过了计划的时间，则表示开始工作的最后期限（以秒为单位）。截止日期之后，cron作业不会开始作业。未能按时完成任务的作业将计为失败的作业。如果未指定此字段，则作业没有截止日期。 CronJob控制器计算出cron作业错过了多少时间表。如果错过了100个以上的计划，则不再计划cron作业。如果.spec.startingDeadlineSeconds未设置，则CronJob控制器将从status.lastScheduleTime现在开始计数错过的日程表。 并发策略 该.spec.concurrencyPolicy字段也是可选的。它指定如何处理由该cron作业创建的作业的并发执行。该规范可能仅指定以下并发策略之一： Allow （默认）：cron作业允许同时运行的作业 Forbid：cron作业不允许并发运行；如果是时候开始新的作业并且之前的作业尚未完成，则cron作业会跳过新的作业 Replace：如果是时候开始新的作业并且之前的作业尚未完成，则cron作业将用新的作业替换当前正在运行的作业 3. job创建过程细讲 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 #容错4次 $ kubectl create -f job.yaml bc 命令是 Linux 里的“计算器”；-l 表示，我现在要使用标准数学库；而 a(1)，则是调用数学库中的 arctangent 函数，计算 atan(1)。这是什么意思呢？中学知识告诉我们：tan(π/4) = 1。所以，4*atan(1)正好就是π，也就是 3.1415926…。这其实就是一个计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000。在我的计算机上，这个计算大概用时 1 分 54 秒。 但是，跟其他控制器不同的是，Job 对象并不要求你定义一个 spec.selector 来描述要控制哪些 Pod $ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl 可以看到，这个 Job 对象在创建后，它的 Pod 模板，被自动加上了一个 controller-uid= 这样的 Label。而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。 而 Job Controller 之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。需要注意的是，这种自动生成的 Label 对用户来说并不友好，所以不太适合推广到 Deployment 等长作业编排对象上。 Pod 进入了 Running 状态 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 1/1 Running 0 10s 而几分钟后计算结束，这个 Pod 就会进入 Completed 状态： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 0/1 Completed 0 4m 这也是我们需要在 Pod 模板中定义 restartPolicy=Never 的原因：离线计算的 Pod 永远都不应该被重启，否则它们会再重新计算一遍。事实上，restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure；而在 Deployment 对象里，restartPolicy 则只允许被设置为 Always。 此时，我们通过 kubectl logs 查看一下这个 Pod 的日志，就可以看到计算得到的 Pi 值已经被打印了出来： $ kubectl logs pi-rq5rl 3.141592653589793238462643383279... 这时候，你一定会想到这样一个问题，如果这个离线作业失败了要怎么办？比如，我们在这个例子中定义了 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod，如下所示： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-55h89 0/1 ContainerCreating 0 2s pi-tqbcz 0/1 Error 0 5s Job 对象的 spec.backoffLimit 字段里定义了重试次数为 4（即，backoffLimit=4），而这个字段的默认值是 6。 需要注意的是，Job Controller 重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10 s、20 s、40 s …后，而如果你定义的 restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。这也正好对应了 restartPolicy 的含义。 如前所述，当一个 Job 的 Pod 运行结束后，它会进入 Completed 状态。但是，如果这个 Pod 因为某种原因一直不肯结束呢？在 Job 的 API 对象里，有一个 spec.activeDeadlineSeconds 字段可以设置最长运行时间，比如： spec: backoffLimit: 5 activeDeadlineSeconds: 100 一旦运行超过了 100 s，这个 Job 的所有 Pod 都会被终止。并且，你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。以上，就是一个 Job API 对象最主要的概念和用法了。不过，离线业务之所以被称为 Batch Job，当然是因为它们可以以“Batch”，也就是并行的方式去运行。 4. Job Controller 对并行作业的控制方法 在 Job 对象中，负责并行控制的参数有两个： spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行； spec.completions，它定义的是 Job 至少要完成的 Pod 数目，即 Job 的最小完成数。 apiVersion: batch/v1 kind: Job metadata: name: pi spec: #这个 Job 最大的并行数是 2，而最小的完成数是 4 parallelism: 2 completions: 4 template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=5000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 $ kubectl create -f job.yaml #两个状态字段，即 DESIRED 和 SUCCESSFUL #DESIRED 的值，正是 completions 定义的最小完成数。 $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 0 3s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 1/1 Running 0 6s pi-gmcq5 1/1 Running 0 6s 40 s 后，这两个 Pod 相继完成计算。这时我们可以看到，每当有一个 Pod 完成计算进入 Completed 状态时，就会有一个新的 Pod 被自动创建出来，并且快速地从 Pending 状态进入到 ContainerCreating 状态： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 Pending 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 Pending 0 0s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 ContainerCreating 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 ContainerCreating 0 0s 紧接着，Job Controller 第二次创建出来的两个并行的 Pod 也进入了 Running 状态： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 54s pi-62rbt 1/1 Running 0 13s pi-84ww8 1/1 Running 0 14s pi-gmcq5 0/1 Completed 0 54s 终，后面创建的这两个 Pod 也完成了计算，进入了 Completed 状态。这时，由于所有的 Pod 均已经成功退出，这个 Job 也就执行完了，所以你会看到它的 SUCCESSFUL 字段的值变成了 4： $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 5m pi-62rbt 0/1 Completed 0 4m pi-84ww8 0/1 Completed 0 4m pi-gmcq5 0/1 Completed 0 5m $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 4 5m 5. Job Controller 的工作原理 首先，Job Controller 控制的对象，直接就是 Pod。 其次，Job Controller 在控制循环中进行的调谐（Reconcile）操作，是根据实际在 Running 状态 Pod 的数目、已经成功退出的 Pod 的数目，以及 parallelism、completions参数的值共同计算出在这个周期里，应该创建或者删除的 Pod 数目，然后调用 Kubernetes API 来执行这个操作。 以创建 Pod 为例。在上面计算 Pi 值的这个例子中，当 Job 一开始创建出来时，实际处于 Running 状态的 Pod 数目 =0，已经成功退出的 Pod 数目 =0，而用户定义的 completions，也就是最终用户需要的 Pod 数目 =4。 所以，在这个时刻，需要创建的 Pod 数目 = 最终需要的 Pod 数目 - 实际在 Running 状态 Pod 数目 - 已经成功退出的 Pod 数目 = 4 - 0 - 0= 4。也就是说，Job Controller 需要创建 4 个 Pod 来纠正这个不一致状态。 可是，我们又定义了这个 Job 的 parallelism=2。也就是说，我们规定了每次并发创建的 Pod 个数不能超过 2 个。所以，Job Controller 会对前面的计算结果做一个修正，修正后的期望创建的 Pod 数目应该是：2 个。 这时候，Job Controller 就会并发地向 kube-apiserver 发起两个创建 Pod 的请求。类似地，如果在这次调谐周期里，Job Controller 发现实际在 Running 状态的 Pod 数目，比 parallelism 还大，那么它就会删除一些 Pod，使两者相等。综上所述，Job Controller 实际上控制了，作业执行的并行度，以及总共需要完成的任务数这两个重要参数。而在实际使用时，你需要根据作业的特性，来决定并行度（parallelism）和任务数（completions）的合理取值。 6. 三种job方法 6.1 外部管理器 +Job 模板 也是最简单粗暴的用法，这种模式的特定用法是：把 Job 的 YAML 文件定义为一个“模板”，然后用一个外部工具控制这些“模板”来生成 Job。这时，Job 的定义方式如下所示： apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"] restartPolicy: Never 以看到，我们在这个 Job 的 YAML 里，定义了 $ITEM 这样的“变量”。所以，在控制这种 Job 时，我们只要注意如下两个方面即可： 创建 Job 时，替换掉 $ITEM 这样的变量； 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。 而做到第一点非常简单。比如，你可以通过这样一句 shell 把 $ITEM 替换掉： $ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" > ./jobs/job-$i.yaml done 这样，一组来自于同一个模板的不同 Job 的 yaml 就生成了。接下来，你就可以通过一句 kubectl create 指令创建这些 Job 了： $ kubectl create -f ./jobs $ kubectl get pods -l jobgroup=jobexample NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m 6.2 拥有固定任务数目的并行 Job 这种模式下，我只关心最后是否有指定数目（spec.completions）个任务成功退出。至于执行时的并行度是多少，我并不关心。比如，我们这个计算 Pi 值的例子，就是这样一个典型的、拥有固定任务数目（completions=4）的应用场景。 它的 parallelism 值是 2；或者，你可以干脆不指定 parallelism，直接使用默认的并行度（即：1）。此外，你还可以使用一个工作队列（Work Queue）进行任务分发。这时，Job 的 YAML 文件定义如下所示： apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure 我们可以看到，它的 completions 的值是：8，这意味着我们总共要处理的任务数目是 8 个。也就是说，总共会有 8 个任务会被逐一放入工作队列里（你可以运行一个外部小程序作为生产者，来提交任务）。所以，一旦你用 kubectl create 创建了这个 Job，它就会以并发度为 2 的方式，每两个 Pod 一组，创建出 8 个 Pod。每个 Pod 都会去连接 BROKER_URL，从 RabbitMQ 里读取任务，然后各自进行处理。这个 Pod 里的执行逻辑，我们可以用这样一段伪代码来表示： /* job-wq-1的伪代码 */ queue := newQueue($BROKER_URL, $QUEUE) task := queue.Pop() process(task) exit 6.3 指定并行度（parallelism），但不设置固定的 completions 的值 此时，你就必须自己想办法，来决定什么时候启动新 Pod，什么时候 Job 才算执行完成。在这种情况下，任务的总数是未知的，所以你不仅需要一个工作队列来负责任务分发，还需要能够判断工作队列已经为空（即：所有的工作已经结束了）。这时候，Job 的定义基本上没变化，只不过是不再需要定义 completions 的值了而已 apiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: - name: c image: gcr.io/myproject/job-wq-2 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job2 restartPolicy: OnFailure 而对应的 Pod 的逻辑会稍微复杂一些，我可以用这样一段伪代码来描述： /* job-wq-2的伪代码 */ for !queue.IsEmpty($BROKER_URL, $QUEUE) { task := queue.Pop() process(task) } print(\"Queue empty, exiting\") exit 7. CronJob定时 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure CronJob 是一个 Job 对象的控制器（Controller） CronJob 与 Job 的关系，正如同 Deployment 与 ReplicaSet 的关系一样。CronJob 是一个专门用来管理 Job 对象的控制器。只不过，它创建和删除 Job 的依据，是 schedule 字段定义的、一个标准的Unix Cron格式的表达式。比如，\"*/1 * * * *\"。这个 Cron 表达式里 /1 中的 表示从 0 开始，/ 表示“每”，1 表示偏移量。所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。 Cron 表达式中的五个部分分别代表：分钟、小时、日、月、星期 $ kubectl create -f ./cronjob.yaml cronjob \"hello\" created # 一分钟后 $ kubectl get jobs NAME DESIRED SUCCESSFUL AGE hello-4111706356 1 1 2s $ kubectl get cronjob hello NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 Thu, 6 Sep 2018 14:34:00 -070 需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如： concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在； concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过； concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。 而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。 8. 实战 8.1 非并发Job 非并发Job的含义是，Job启动后，只运行一个pod，pod运行结束后整个Job也就立刻结束。 以下是简单的Job配置文件，只包含一个pod，输出圆周率小数点后2000位，运行时间大概为10s： apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 以上示例无需设置选择器、pod标签。无需设置.spec.completions、.spec.parallelism，这两个字段的默认值都是1。backoffLimit=4，表示允许pod失败的次数。将以上内容保存成文件并创建Job： $ kubectl create -f https://k8s.io/examples/controllers/job.yaml job \"pi\" created 8.2 粗并发Job 本例创建一个Job，但Job要创建多个pod。了解完示例后就明白为什么叫“粗并发”。 本示例需要一个消息队列服务的配合，不详细描述如何部署、填充消息队列服务。假设我们有一个RabbitMQ服务，集群内访问地址为：amqp://guest:guest@rabbitmq-service:5672。其有一个名为job1的队列，队列内有apple banana cherry date fig grape lemon melon共8个成员。 另外假设我们有一个名为gcr.io//job-wq-1的image，其功能是从队列中读取出一个元素并打印到标准输出，然后结束。注意，它只处理一个元素就结束了。接下来创建如下Job： apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: gcr.io//job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure 上例中，completions的值为8，等于job1队列中元素的个数。因为每个成功的pod处理一个元素，所以需要成功8次，job1中的所有成员就会被处理完成。在粗并发模式下，completions的值必需指定，否则其默认值为1，整个Job只处理一个成员就结束了。 上例中，parallelism的值是2。虽然需要pod成功8次，但在同一时间，只允许有两个pod并发。一个成功结束后，再启动另一个。这个参数的主要目的是控制并发pod的个数，可根据实际情况调整。当然可以不指定，那么默认的并发个数就是1。 env中的内容告诉image如何访问队列。 8.3 CronJob apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure kubectl create -f https://k8s.io/examples/application/job/cronjob.yaml kubectl get cronjob hello kubectl logs $pods kubectl delete cronjob hello 命令行执行cronjob kubectl run hello --schedule=\"*/1 * * * *\" --restart=OnFailure --image=busybox -- /bin/sh -c \"date; echo Hello from the Kubernetes cluster\" 8.4. Job的自动清理 apiVersion: batch/v1 kind: Job metadata: name: pi-with-ttl spec: ttlSecondsAfterFinished: 100 template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never 参考： kubernetes job Understanding Jobs in Kubernetes google cloud running Job Kubernetes - Jobs Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 11:15:29 "},"对象/Runtimeclass/":{"url":"对象/Runtimeclass/","title":"Runtimeclass","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 09:44:30 "},"对象/Runtimeclass/kubernetes-object-RuntimeClass-principle.html":{"url":"对象/Runtimeclass/kubernetes-object-RuntimeClass-principle.html","title":"原理","keywords":"","body":"kubernetes RuntimeClass 原理1. 简介2. RuntimeClass 的工作流程3. RuntimeClass 功能介绍3.1 RuntimeClass结构体定义3.2 Scheduling 结构体的定义3.3 为什么引入 Pod Overhead？3.4 Pod Overhead 的使用场景与限制4. 多容器运行时示例5. 总结kubernetes RuntimeClass 原理 tagsstart 对象 RuntimeClass tagsstop 《龙猫》 1. 简介 RuntimeClass是用于选择容器运行时配置的功能。容器运行时配置用于运行Pod的容器。 可以在不同的Pod之间设置不同的RuntimeClass，以实现性能与安全性之间的平衡。例如，如果您的部分工作负荷应得到较高级别的信息安全保证，则可以选择安排这些Pod，以便它们在使用硬件虚拟化的容器运行时中运行。然后，您将受益于备用运行时的额外隔离，但要付出一些额外的开销。 您还可以使用RuntimeClass在相同的容器运行时但使用不同的设置来运行不同的Pod。 此同时，越来越多的容器运行时也想接入到 Kubernetes 中。如果还是按 rkt 和 Docker 一样内置支持的话，会给 Kubernetes 的代码维护和质量保障带来严重挑战。 社区也意识到了这一点，所以在 1.5 版本时推出了 CRI，它的全称是 Container Runtime Interface。这样做的好处是：实现了运行时和 Kubernetes 的解耦，社区不必再为各种运行时做适配工作，也不用担心运行时和 Kubernetes 迭代周期不一致所带来的版本维护问题。比较典型的，比如 containerd 中的 cri-plugin 就实现了 CRI、kata-containers、gVisor 这样的容器运行时只需要对接 containerd 就可以了。 随着越来越多的容器运行时的出现，不同的容器运行时也有不同的需求场景，于是就有了多容器运行时的需求。但是，如何来运行多容器运行时还需要解决以下几个问题： 集群里有哪些可用的容器运行时？ 如何为 Pod 选择合适的容器运行时？ 如何让 Pod 调度到装有指定容器运行时的节点上？ 容器运行时在运行容器时会产生有一些业务运行以外的额外开销，这种「额外开销」需要怎么统计？ 2. RuntimeClass 的工作流程 了解决上述提到的问题，社区推出了 RuntimeClass。它其实在 Kubernetes v1.12 中就已被引入，不过最初是以 CRD 的形式引入的。v1.14 之后，它又作为一种内置集群资源对象 RuntimeClass 被引入进来。v1.16 又在 v1.14 的基础上扩充了 Scheduling 和 Overhead 的能力。 下面以 v1.16 版本为例，讲解一下 RuntimeClass 的工作流程。如上图所示，左侧是它的工作流程图，右侧是一个 YAML 文件。 YAML 文件包含两个部分：上部分负责创建一个名字叫 runv 的 RuntimeClass 对象，下部分负责创建一个 Pod，该 Pod 通过 spec.runtimeClassName 引用了 runv 这个 RuntimeClass。 RuntimeClass 对象中比较核心的是 handler，它表示一个接收创建容器请求的程序，同时也对应一个容器运行时。比如示例中的 Pod 最终会被 runv 容器运行时创建容器；scheduling 决定 Pod 最终会被调度到哪些节点上。 结合左图来说明一下 RuntimeClass 的工作流程： K8s-master 接收到创建 Pod 的请求； 方格部分表示三种类型的节点。每个节点上都有 Label 标识当前节点支持的容器运行时，节点内会有一个或多个 handler，每handler 对应一种容器运行时。比如第二个方格表示节点内有支持 runc 和 runv 两种容器运行时的handler；第三个方格表示节点内有支持 runhcs 容器运行时的 handler； 根据 scheduling.nodeSelector, Pod 最终会调度到中间方格节点上，并最终由 runv handler 来创建 Pod。 3. RuntimeClass 功能介绍 3.1 RuntimeClass结构体定义 我们还是以 Kubernetes v1.16 版本中的 RuntimeClass 为例。首先介绍一下 RuntimeClass 的结构体定义。 一个 RuntimeClass 对象代表了一个容器运行时，它的结构体中主要包含 Handler、Overhead、Scheduling 三个字段。 在之前的例子中我们也提到过 Handler，它表示一个接收创建容器请求的程序，同时也对应一个容器运行时； Overhead 是 v1.16 中才引入的一个新的字段，它表示 Pod 中的业务运行所需资源以外的额外开销； 第三个字段Scheduling 也是在 v1.16 中被引入的，该 Scheduling 配置会被自动注入到 Pod 的 nodeSelector 中。 在 Pod 中引用 RuntimeClass 的用法非常简单，只要在 runtimeClassName 字段中配置好 RuntimeClass 的名字，就可以把这个 RuntimeClass 引入进来。 3.2 Scheduling 结构体的定义 Scheduling 表示调度，但这里的调度不是说 RuntimeClass 对象本身的调度，而是会影响到引用了 RuntimeClass 的 Pod 的调度。 Scheduling 中包含了两个字段，NodeSelector 和 Tolerations。这两个和 Pod 本身所包含的 NodeSelector 和 Tolerations 是极为相似的。 NodeSelector 代表的是支持该 RuntimeClass 的节点上应该有的 label 列表。一个 Pod 引用了该 RuntimeClass 后，RuntimeClass admission 会把该 label 列表与 Pod 中的 label 列表做一次合并。如果这两个 label 中有冲突的，会被 admission 拒绝。这里的冲突是指它们的 key 相同，但是 value 不相同，这种情况就会被 admission 拒绝。另外需要注意的是，RuntimeClass 并不会自动为 Node 设置 label，需要用户在使用前提前设置好。 Tolerations 表示 RuntimeClass 的容忍列表。一个 Pod 引用该 RuntimeClass 之后，admission 也会把 toleration 列表与 Pod 中的 toleration 列表做一个合并。如果这两处的 Toleration 有相同的容忍配置，就会将其合并成一个。 3.3 为什么引入 Pod Overhead？ 上图左边是一个 Docker Pod，右边是一个 Kata Pod。我们知道，Docker Pod 除了传统的 container 容器之外，还有一个 pause 容器，但我们在计算它的容器开销的时候会忽略 pause 容器。对于 Kata Pod，除了 container 容器之外，kata-agent, pause, guest-kernel 这些开销都是没有被统计进来的。像这些开销，多的时候甚至能超过 100MB，这些开销我们是没法忽略的。 这就是我们引入 Pod Overhead 的初衷。它的结构体定义如下： 它的定义非常简单，只有一个字段 PodFixed。它这里面也是一个映射，它的 key 是一个 ResourceName，value 是一个 Quantity。每一个 Quantity 代表的是一个资源的使用量。因此 PodFixed 就代表了各种资源的占用量，比如 CPU、内存的占用量，都可以通过 PodFixed 进行设置。 3.4 Pod Overhead 的使用场景与限制 Pod Overhead 的使用场景主要有三处： Pod 调度 在没有引入 Overhead 之前，只要一个节点的资源可用量大于等于 Pod 的 requests 时，这个 Pod 就可以被调度到这个节点上。引入 Overhead 之后，只有节点的资源可用量大于等于 Overhead 加上 requests 的值时才能被调度上来。 ResourceQuota 它是一个 namespace 级别的资源配额。假设我们有这样一个 namespace，它的内存使用量是 1G，我们有一个 requests 等于 500 的 Pod，那么这个 namespace 之下，最多可以调度两个这样的 Pod。而如果我们为这两个 Pod 增添了 200MB 的 Overhead 之后，这个 namespace 下就最多只可调度一个这样的 Pod。 Kubelet Pod 驱逐 引入 Overhead 之后，Overhead 就会被统计到节点的已使用资源中，从而增加已使用资源的占比，最终会影响到 Kubelet Pod 的驱逐。 以上是 Pod Overhead 的使用场景。除此之外，Pod Overhead 还有一些使用限制和注意事项： Pod Overhead 最终会永久注入到 Pod 内并且不可手动更改。即便是将 RuntimeClass 删除或者更新，Pod Overhead 依然存在并且有效； Pod Overhead 只能由 RuntimeClass admission 自动注入（至少目前是这样的），不可手动添加或更改。如果这么做，会被拒绝； HPA 和 VPA 是基于容器级别指标数据做聚合，Pod Overhead 不会对它们造成影响。 4. 多容器运行时示例 目前阿里云 ACK 安全沙箱容器已经支持了多容器运行时，我们以上图所示环境为例来说明一下多容器运行时是怎么工作的。 如上图所示有两个 Pod，左侧是一个 runc 的 Pod，对应的 RuntimeClass 是 runc，右侧是一个 runv 的Pod，引用的 RuntimeClass 是 runv。对应的请求已用不同的颜色标识了出来，蓝色的代表是 runc 的，红色的代表是 runv 的。图中下半部分，其中比较核心的部分是 containerd，在 containerd 中可以配置多个容器运行时，最终上面的请求也会到达这里进行请求的转发。 我们先来看一下 runc 的请求，它先到达 kube-apiserver，然后 kube-apiserver 请求转发给 kubelet，最终 kubelet 将请求发至 cri-plugin（它是一个实现了 CRI 的插件），cri-plugin 在 containerd 的配置文件中查询 runc 对应的 Handler，最终查到是通过 Shim API runtime v1 请求 containerd-shim，然后由它创建对应的容器。这是 runc 的流程。 runv 的流程与 runc 的流程类似。也是先将请求到达 kube-apiserver，然后再到达 kubelet，再把请求到达 cri-plugin，cri-plugin 最终还回去匹配 containerd 的配置文件，最终会找到通过 Shim API runtime v2 去创建 containerd-shim-kata-v2，然后由它创建一个 Kata Pod。 下面我们再看一下 containerd 的具体配置。 containerd 默认放在 [file:///etc/containerd/config.toml]() 这个位置下。比较核心的配置是在 plugins.cri.containerd 目录下。其中 runtimes 的配置都有相同的前缀 plugins.cri.containerd.runtimes，后面有 runc, runv 两种 RuntimeClass。这里面的 runc 和 runv 和前面 RuntimeClass 对象中 Handler 的名字是相对应的。除此之外，还有一个比较特殊的配置 plugins.cri.containerd.runtimes.default_runtime，它的意思是说，如果一个 Pod 没有指定 RuntimeClass，但是被调度到当前节点的话，那么就默认使用 runc 容器运行时。 下面的例子是创建 runc 和 runv 这两个 RuntimeClass 对象，我们可以通过 kubectl get runtimeclass 看到当前所有可用的容器运行时 下图从左至右分别是一个 runc 和 runv 的 Pod，比较核心的地方就是在 runtimeClassName 字段中分别引用了 runc 和 runv 的容器运行时。 最终将 Pod 创建起来之后，我们可以通过 kubectl 命令来查看各个 Pod 容器的运行状态以及 Pod 所使用的容器运行时。我们可以看到现在集群中有两个 Pod：一个是 runc-pod，另一个是 runv-pod，分别引用的是 runc 和 runv 的 RuntimeClass，并且它们的状态都是 Running。 5. 总结 本文的主要内容就到此为止了，这里为大家简单总结一下： RuntimeClass 是 Kubernetes 一种内置的集群资源，主要用来解决多个容器运行时混用的问题； RuntimeClass 中配置 Scheduling 可以让 Pod 自动调度到运行了指定容器运行时的节点上。但前提是需要用户提前为这些Node 设置好 label； RuntimeClass 中配置 Overhead，可以把 Pod中业务运行所需以外的开销统计进来，让调度、ResourceQuota、Kubelet Pod 驱逐等行为更准确 参考： 容器运行时类（Runtime Class） 理解 RuntimeClass 与使用多容器运行时 Getting Started with Kubernetes | Understanding Kubernetes RuntimeClass and Using Multiple Container Runtimes KEP-585: Runtime Class Oracle create a runtime class Openshift RuntimeClass Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 09:59:11 "},"对象/Kubernetes-PodSecurityPolicy.html":{"url":"对象/Kubernetes-PodSecurityPolicy.html","title":"PodSecurityPolicy","keywords":"","body":"kubernetes pod PodSecurityPolicy1. 简介2. API 版本对照表3. 支持的控制项4. 实例4.1 控制是否允许超出父进程特权4.2 限制端口4.3 限制只允许使用 lvm 和 cifs 等 flexVolume 插件kubernetes pod PodSecurityPolicy tagsstart 对象 pod PodSecurityPolicy tagsstop 美剧《开发者》（Devs）颠覆感藏在最后。 1. 简介 Pod Security Policy（PSP）是集群级的 Pod 安全策略，自动为集群内的 Pod 和 Volume 设置 Security Context。 使用 PSP 需要 API Server 开启 extensions/v1beta1/podsecuritypolicy，并且配置 PodSecurityPolicy admission 控制器。 注意： PodSecurityPolicy 自 Kubernetes v1.21 起已弃用，并将在 v1.25 中删除。我们建议迁移到Pod Security Admission或 3rd party admission 插件。有关迁移指南，请参阅从 PodSecurityPolicy 迁移到内置 PodSecurity 准入控制器。有关弃用的更多信息，请参阅PodSecurityPolicy 弃用：过去、现在和未来。 2. API 版本对照表 Kubernetes 版本 Extension 版本 v1.5-v1.15 extensions/v1beta1 v1.10+ policy/v1beta1 3. 支持的控制项 控制项 说明 privileged 运行特权容器 defaultAddCapabilities 可添加到容器的 Capabilities requiredDropCapabilities 会从容器中删除的 Capabilities allowedCapabilities 允许使用的 Capabilities 列表 volumes 控制容器可以使用哪些 volume hostNetwork 允许使用 host 网络 hostPorts 允许的 host 端口列表 hostPID 使用 host PID namespace hostIPC 使用 host IPC namespace seLinux SELinux Context runAsUser user ID supplementalGroups 允许的补充用户组 fsGroup volume FSGroup readOnlyRootFilesystem 只读根文件系统 allowedHostPaths 允许 hostPath 插件使用的路径列表 allowedFlexVolumes 允许使用的 flexVolume 插件列表 allowPrivilegeEscalation 允许容器进程设置 no_new_privs defaultAllowPrivilegeEscalation 默认是否允许特权升级 4. 实例 4.1 控制是否允许超出父进程特权 allowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。 此布尔值直接控制是否为容器进程设置 no_new_privs标志。 当容器满足一下条件之一时，allowPrivilegeEscalation 总是为 true： 以特权模式运行，或者 具有 CAP_SYS_ADMIN 权能 readOnlyRootFilesystem：以只读方式加载容器的根文件系统。 root@master:~/cks/securitytext# vim /etc/kubernetes/manifests/kube-apiserver.yaml --- - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy --- root@master:~/cks/securitytext# cat psp.yaml apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: default spec: allowPrivilegeEscalation: false privileged: false # Don't allow privileged pods! # The rest fills in some required fields. seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny volumes: - '*' root@master:~/cks/securitytext# k create -f psp.yaml podsecuritypolicy.policy/default created root@master:~/cks/securitytext# k create deploy nginx --image=nginx deployment.apps/nginx created root@master:~/cks/securitytext# k get deploy nginx -w NAME READY UP-TO-DATE AVAILABLE AGE nginx 0/1 0 0 22s ^Croot@master:~/cks/securitytext# k run nginx --image=nginx pod/nginx created root@master:~/cks/securitytext# k get pod nginx NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 44s root@master:~/cks/securitytext# k create role psp-access --verb=use --resource=podsecuritypolicies role.rbac.authorization.k8s.io/psp-access created root@master:~/cks/securitytext# k create rolebinding psp-access --role=psp-access --serviceaccount=default:default rolebinding.rbac.authorization.k8s.io/psp-access created root@master:~/cks/securitytext# k get deploy nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 0/1 0 0 3m26s root@master:~/cks/securitytext# k delete deploy nginx deployment.apps \"nginx\" deleted root@master:~/cks/securitytext# k create deploy nginx --image=nginx deployment.apps/nginx created ^Croot@master:~/cks/securitytext# k get deploy nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 20s allowPrivilegeEscalation设置为rue root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: # securityContext: # runAsUser: 1000 # runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} securityContext: allowPrivilegeEscalation: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# k -f pod.yaml create Error from server (Forbidden): error when creating \"pod.yaml\": pods \"pod\" is forbidden: PodSecurityPolicy: unable to admit pod: [spec.containers[0].securityContext.allowPrivilegeEscalation: Invalid value: true: Allowing privilege escalation for containers is not allowed] 4.2 限制端口 限制容器的 host 端口范围为 8000-8080 apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: permissive spec: seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny hostPorts: - min: 8000 max: 8080 volumes: - '*' 4.3 限制只允许使用 lvm 和 cifs 等 flexVolume 插件 apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: allow-flex-volumes spec: fsGroup: rule: RunAsAny runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - flexVolume allowedFlexVolumes: - driver: example/lvm - driver: example/cifs 参考： Pod Security Policies Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 11:16:13 "},"对象/Kubernetes-ConfigMaps.html":{"url":"对象/Kubernetes-ConfigMaps.html","title":"ConfigMap","keywords":"","body":"kubernetes ConfigMap1. 简介2. 创建configmap2.1 --from-file2.2 --from-env-file2.3 --from-literal3. 基于生成器创建 ConfigMap3.1 定义从文件生成 ConfigMap 时要使用的键3.2 从字面值生成 ConfigMap4. configmap与pod4.1 使用 ConfigMap 数据定义容器环境变量4.2 使用来自多个 ConfigMap 的数据定义容器环境变量4.3 将 ConfigMap 中的所有键值对配置为容器环境变量4.4 将 ConfigMap 数据添加到一个卷中4.6 pod引用configmap示例5. 使用ConfigMap的限制条件kubernetes ConfigMap tagsstart ConfigMap 对象 tagsstop “早安！公主！” ——《美丽人生》 1. 简介 ConfigMap 是一种 API 对象，用来将非机密性的数据保存到健值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将您的环境配置信息和 容器镜像 解耦，便于应用配置的修改。当您需要储存机密信息时可以使用 Secret 对象。 注意： ConfigMap 并不提供保密或者加密功能。如果你想存储的数据是机密的，请使用 Secret ，或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。 四种方式来使用 ConfigMap 配置 Pod 中的容器： 容器 entrypoint 的命令行参数 容器的环境变量 在只读卷里面添加一个文件，让应用来读取 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap 2. 创建configmap kubectl create configmap 或者在 kustomization.yaml 中的 ConfigMap 生成器 来创建 ConfigMap。注意，kubectl 从 1.14 版本开始支持 kustomization.yaml。 格式： kubectl create configmap 是要设置的 ConfigMap 名称， 是要从中提取数据的目录、 文件或者字面值。 ConfigMap 对象的名称必须是合法的 DNS 子域名. 2.1 --from-file 第一种方法 # 创建本地目录 mkdir -p configure-pod-container/configmap/ # 将实例文件下载到 `configure-pod-container/configmap/` 目录 wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties # 创建 configmap kubectl create configmap game-config --from-file=configure-pod-container/configmap/ 或者 kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties 第二种方法 $ kubectl create configmap game-config-3 --from-file== 是你要在 ConfigMap 中使用的键名， 是你想要键表示数据源文件的位置 $ kubectl create configmap game-config-3 --from-file=game-special-key=configure-pod-container/configmap/game.properties $ kubectl get configmaps game-config-3 -o yaml apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T18:54:22Z name: game-config-3 namespace: default resourceVersion: \"530\" selfLink: /api/v1/namespaces/default/configmaps/game-config-3 uid: 05f8da22-d671-11e5-8cd0-68f728db1985 data: game-special-key: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 2.2 --from-env-file $ cat configure-pod-container/configmap/game-env-file.properties enemies=aliens lives=3 allowed=\"true\" $ kubectl create configmap game-config-env-file \\ --from-env-file=configure-pod-container/configmap/game-env-file.properties $ kubectl get configmap game-config-env-file -o yaml apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2017-12-27T18:36:28Z name: game-config-env-file namespace: default resourceVersion: \"809965\" selfLink: /api/v1/namespaces/default/configmaps/game-config-env-file uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8 data: allowed: '\"true\"' enemies: aliens lives: \"3\" 2.3 --from-literal kubectl create configmap 与 --from-literal 参数一起使用，从命令行定义文字值: $ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm $ kubectl get configmaps special-config -o yaml apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T19:14:38Z name: special-config namespace: default resourceVersion: \"651\" selfLink: /api/v1/namespaces/default/configmaps/special-config uid: dadce046-d673-11e5-8cd0-68f728db1985 data: special.how: very special.type: charm 3. 基于生成器创建 ConfigMap 自 1.14 开始，kubectl 开始支持 kustomization.yaml。 你还可以基于生成器创建 ConfigMap，然后将其应用于 API 服务器上创建对象。 生成器应在目录内的 kustomization.yaml 中指定。 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件 cat ./kustomization.yaml configMapGenerator: - name: game-config-4 files: - configure-pod-container/configmap/kubectl/game.properties EOF 使用 kustomization 目录创建 ConfigMap 对象： $ kubectl apply -k . 3.1 定义从文件生成 ConfigMap 时要使用的键 在 ConfigMap 生成器，你可以定义一个非文件名的键名。 例如，从 configure-pod-container/configmap/game.properties 文件生成 ConfigMap， 但使用 game-special-key 作为键名： 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件 cat ./kustomization.yaml configMapGenerator: - name: game-config-5 files: - game-special-key=configure-pod-container/configmap/kubectl/game.properties EOF 3.2 从字面值生成 ConfigMap 要基于字符串 special.type=charm 和 special.how=very 生成 ConfigMap， 可以在 kusotmization.yaml 中配置 ConfigMap 生成器： 创建带有 ConfigMapGenerator 的 kustomization.yaml 文件 cat ./kustomization.yaml configMapGenerator: - name: special-config-2 literals: - special.how=very - special.type=charm EOF 4. configmap与pod 4.1 使用 ConfigMap 数据定义容器环境变量 在 ConfigMap 中将环境变量定义为键值对: kubectl create configmap special-config --from-literal=special.how=very 将 ConfigMap 中定义的 special.how 值分配给 Pod 规范中的 SPECIAL_LEVEL_KEY 环境变量 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never 创建 Pod: kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml 现在，Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very。 4.2 使用来自多个 ConfigMap 的数据定义容器环境变量 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very --- apiVersion: v1 kind: ConfigMap metadata: name: env-config namespace: default data: log_level: INFO kubectl create -f https://kubernetes.io/examples/configmap/configmaps.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never kubectl create -f https://kubernetes.io/examples/pods/pod-multiple-configmap-env-variable.yaml 现在，Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very 和 LOG_LEVEL=INFO 4.3 将 ConfigMap 中的所有键值对配置为容器环境变量 pods/pod-configmap-envFrom.yaml apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml 使用 envFrom 将所有 ConfigMap 的数据定义为容器环境变量，ConfigMap 中的键成为 Pod 中的环境变量名称。 pods/pod-configmap-envFrom.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] envFrom: - configMapRef: name: special-config restartPolicy: Never kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-envFrom.yaml 4.4 将 ConfigMap 数据添加到一个卷中 如基于文件创建 ConfigMap](#create-configmaps-from-files) 中所述，当你使用 --from-file 创建 ConfigMap 时，文件名成为存储在 ConfigMap 的 data 部分中的键， 文件内容成为键对应的值。 configmap/configmap-multikeys.yaml apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml 4.4.1 使用存储在 ConfigMap 中的数据填充数据卷 在 Pod 规约的 volumes 部分下添加 ConfigMap 名称。 这会将 ConfigMap 数据添加到指定为 volumeMounts.mountPath 的目录（在本例中为 /etc/config）。 command 部分引用存储在 ConfigMap 中的 special.level。 pods/pod-configmap-volume.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: # Provide the name of the ConfigMap containing the files you want # to add to the container name: special-config restartPolicy: Never kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume.yaml Pod 运行时，命令 ls /etc/config/ 产生下面的输出： SPECIAL_LEVEL SPECIAL_TYPE 注意： 如果在 /etc/config/ 目录中有一些文件，它们将被删除。 4.4.2 将 ConfigMap 数据添加到数据卷中的特定路径 使用 path 字段为特定的 ConfigMap 项目指定预期的文件路径。 在这里，SPECIAL_LEVEL 将挂载在 config-volume 数据卷中 /etc/config/keys 目录下。 pods/pod-configmap-volume-specific-key.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\",\"-c\",\"cat /etc/config/keys\" ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config items: - key: SPECIAL_LEVEL path: keys restartPolicy: Never 创建Pod： kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume-specific-key.yaml 当 pod 运行时，命令 cat /etc/config/keys 产生以下输出： very 4.4.3 将 ConfigMap 数据添加到数据卷中的特定文件 4.6 pod引用configmap示例 apiVersion: v1 kind: ConfigMap metadata: name: game-demo data: # 类属性键；每一个键都映射到一个简单的值 # 使用 --from-literal 定义的简单属性 player_initial_lives: \"3\" ui_properties_file_name: \"user-interface.properties\" # # 类文件键 # 使用 --from-file 定义复杂属性的例子 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true apiVersion: v1 kind: Pod metadata: name: configmap-demo-pod spec: containers: - name: demo image: game.example/demo-game env: # 定义环境变量 - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的 valueFrom: configMapKeyRef: name: game-demo # 这个值来自 ConfigMap key: player_initial_lives # 需要取值的键 - name: UI_PROPERTIES_FILE_NAME valueFrom: configMapKeyRef: name: game-demo key: ui_properties_file_name volumeMounts: - name: config mountPath: \"/config\" readOnly: true volumes: # 您可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中 - name: config configMap: # 提供你想要挂载的 ConfigMap 的名字 name: game-demo 5. 使用ConfigMap的限制条件 使用ConfigMap的限制条件如下。 ◎ ConfigMap必须在Pod之前创建。 ◎ ConfigMap受Namespace限制，只有处于相同Namespace中的Pod才可以引用它。 ◎ ConfigMap中的配额管理还未能实现。 ◎ kubelet只支持可以被API Server管理的Pod使用ConfigMap。kubelet在本Node上通过 --manifest-url或--config自动创建的静态Pod将无法引用ConfigMap。 ◎ 在Pod对ConfigMap进行挂载（volumeMount）操作时，在容器内部只能挂载为“目录”，无法挂载为“文件”。在挂载到容器内部后，在目录下将包含ConfigMap定义的每个item，如果在该目录下原来还有其他文件，则容器内的该目录将被挂载的ConfigMap覆盖。如果应用程序需要保留原来的其他文件，则需要进行额外的处理。可以将ConfigMap挂载到容器内部的临时目录，再通过启动脚本将配置文件复制或者链接到（cp或link命令）应用所用的实际配置目录下。 参考： kubernetes pod configmap google cloud 创建 ConfigMap ConfigMaps in Kubernetes (K8s) Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 08:11:11 "},"对象/Pod/":{"url":"对象/Pod/","title":"Pod","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"对象/Pod/Kubernetes-Pod-Probes.html":{"url":"对象/Pod/Kubernetes-Pod-Probes.html","title":"Probes","keywords":"","body":"kuberenetes pod Liveness, Readiness and Startup Probes1. Pod 状态2. Pod 生命周期条件3. 容器探针4. 什么时候应该使用活动或就绪探针？5. 配置技巧5.1 使用命名端口5.2 使用启动探测器保护慢启动容器6. 探测方法6.1 http 探测6.2 cmd 探测6.3 TCP 探测kuberenetes pod Liveness, Readiness and Startup Probes tagsstart Pod 探针 健康检测 tagsstop 1. Pod 状态 Pod的status字段是 PodStatus 对象，其中包含一个phase字段。 | 值 | 描述 | |-----------|-----------------------------------------------------------------------------------| | Pending | 该Pod已被Kubernetes系统接受，但是尚未创建一个或多个Container映像。这包括计划之前的时间以及通过网络下载图像所花费的时间，这可能需要一段时间。 | | Running | Pod已绑定到节点，并且所有容器都已创建。至少一个容器仍在运行，或者正在启动或重新启动。 | | Succeeded | Pod中的所有容器已成功终止，并且不会重新启动。 | | Failed | Pod中的所有容器均已终止，并且至少一个容器因故障而终止。也就是说，容器要么以非零状态退出，要么被系统终止。 | | Unknown | 由于某些原因，通常由于与Pod主机通信时出错而无法获得Pod的状态。 | 2. Pod 生命周期条件 Pod具有PodStatus，该状态具有PodConditions数组 ，该Pod已通过或未通过。PodCondition数组的每个元素都有六个可能的字段： 该lastProbeTime字段提供上次探测Pod条件的时间戳。 该lastTransitionTime字段提供Pod上一次从一种状态转换为另一种状态的时间戳。 该message字段是人类可读的消息，指示有关过渡的详细信息。 该reason字段是条件最后一次转换的唯一单词CamelCase原因。 该status字段是一个字符串，可能的值为“ True”，“ False”和“ Unknown”。 该type字段是具有以下可能值的字符串： PodScheduled：Pod已调度到一个节点； Ready：Pod能够处理请求，应将其添加到所有匹配服务的负载平衡池中； Initialized：所有初始化容器 已成功启动； Unschedulable：例如，由于缺乏资源或其他限制，调度程序无法立即调度Pod； ContainersReady：Pod中的所有容器已准备就绪。 3. 容器探针 有三种类型的处理程序： ExecAction：在Container中执行指定的命令。如果命令以状态代码0退出，则认为诊断成功。 TCPSocketAction：对指定端口上的容器的IP地址执行TCP检查。如果端口打开，则认为诊断成功。 HTTPGetAction：对指定端口和路径上的容器的IP地址执行HTTP Get请求。如果响应的状态码大于或等于200且小于400，则认为诊断成功。 每个探针具有以下三个结果之一： 成功：容器通过了诊断。 失败：容器无法通过诊断。 未知：诊断失败，因此不应采取任何措施。 kubelet可以选择对正在运行的Container进行两种探测并对其做出反应： livenessProbe：指示容器是否正在运行。如果活动探针失败，则kubelet将杀死Container，并且Container将接受其重新启动策略。如果容器未提供活动性探针，则默认状态为Success。 readinessProbe：指示容器是否准备好服务请求。如果就绪探针失败，则端点控制器将从与Pod匹配的所有服务的端点中删除Pod的IP地址。初始延迟之前的默认就绪状态为Failure。如果容器未提供就绪探测器，则默认状态为Success。4. 什么时候应该使用活动或就绪探针？ 如果您的Container中的进程在遇到问题或变得不正常时能够自行崩溃，则不一定需要进行活动调查；kubelet将根据Pod的自动执行正确的操作restartPolicy。 如果您希望在探测失败时杀死并重启容器，请指定活动探测，并指定restartPolicyAlways或OnFailure。 如果您仅想在探测成功后才开始向Pod发送流量，请指定就绪探测器。在这种情况下，就绪探针可能与活动探针相同，但是规范中存在就绪探针意味着Pod将在不接收任何流量的情况下启动，并且仅在探针开始成功之后才开始接收流量。如果您的容器需要在启动过程中加载大型数据，配置文件或迁移，请指定准备情况探针。 如果希望您的Container能够自行进行维护，则可以指定一个就绪探针，以检查特定于与活跃探针不同的就绪端点。 请注意，如果您只想在删除Pod时便能够清空请求，则不一定需要准备就绪探测器；删除后，无论是否准备就绪探针，Pod都会自动将自己置于未就绪状态。等待Pod中的Container停止时，Pod仍处于未就绪状态。 5. 配置技巧 5.1 使用命名端口 ports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port 5.2 使用启动探测器保护慢启动容器 ports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 10 startupProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 30 periodSeconds: 10 幸亏有启动探测，应用程序将会有最多 5 分钟(30 * 10 = 300s) 的时间来完成它的启动。 一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。 如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 restartPolicy 来设置 Pod 状态。 6. 探测方法 6.1 http 探测 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - args: - /server image: k8s.gcr.io/liveness livenessProbe: httpGet: # when \"host\" is not defined, \"PodIP\" will be used # host: my-host # when \"scheme\" is not defined, \"HTTP\" scheme will be used. Only \"HTTP\" and \"HTTPS\" are allowed # scheme: HTTPS path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 periodSeconds: 3 timeoutSeconds: 1 name: liveness periodSeconds 字段指定了 kubelet 每隔 3 秒执行一次存活探测。 initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒。 kubelet 会向容器内运行的服务（服务会监听 8080 端口）发送一个 HTTP GET 请求来执行探测。 如果服务器上 /healthz 路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。 如果处理程序返回失败代码，则 kubelet 会杀死这个容器并且重新启动它。 任何大于或等于 200 并且小于 400 的返回代码标示成功，其它返回代码都标示失败。 6.2 cmd 探测 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。 initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。 kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。 如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。 6.3 TCP 探测 apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 下面这个例子同时使用就绪和存活探测器。kubelet 会在容器启动 5 秒后发送第一个就绪探测。 这会尝试连接 goproxy 容器的 8080 端口。 如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。 除了就绪探测，这个配置包括了一个存活探测。 kubelet 会在容器启动 15 秒后进行第一次存活探测。 就像就绪探测一样，会尝试连接 goproxy 容器的 8080 端口。 如果存活探测失败，这个容器会被重新启动。 参考： 配置存活、就绪和启动探测器 Kubernetes Startup Probes - Examples & Common Pitfalls Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:49:16 "},"对象/Pod/Kubernetes-Pod-SecurityContext.html":{"url":"对象/Pod/Kubernetes-Pod-SecurityContext.html","title":"SecurityContext","keywords":"","body":"kuberenetes pod SecurityContext1. 简介2. 功能2.1 配置pod的安全上下文2.2 为 Pod 配置卷访问权限和属主变更策略2.3 对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。2.4 对启动的容器赋予SELinux标签3. 实例3.1 Set Container User and Group3.2 Force Container Non-Root3.3 Privileged Containers3.4 Create Privileged Containers3.5 PrivilegeEscalation3.6 Practice - Disable PriviledgeEscalationkuberenetes pod SecurityContext tagsstart Pod 对象 tagsstop 1. 简介 安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。安全上下文包括但不限于： 自主访问控制（Discretionary Access Control）：基于 用户 ID（UID）和组 ID（GID）.来判定对对象（例如文件）的访问权限 安全性增强的 Linux（SELinux）：为对象赋予安全性标签。 以特权模式或者非特权模式运行。 Linux 权能: 为进程赋予 root 用户的部分特权而非全部特权。 AppArmor：使用程序文件来限制单个程序的权限。 Seccomp：限制一个进程访问文件描述符的权限。 AllowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。此布尔值直接控制是否为容器进程设置no_new_privs 标志。当容器以特权模式运行或者具有 CAP_SYS_ADMIN权能时，AllowPrivilegeEscalation 总是为 true。 readOnlyRootFilesystem：以只读方式加载容器的根文件系统。 2. 功能 2.1 配置pod的安全上下文 apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 volumes: - name: sec-ctx-vol emptyDir: {} containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] volumeMounts: - name: sec-ctx-vol mountPath: /data/demo securityContext: allowPrivilegeEscalation: false 在配置文件中，该runAsUser字段指定对于Pod中的任何容器，所有进程都以用户ID 1000运行。 该runAsGroup字段为Pod中的任何容器中的所有进程指定主组ID3000。如果省略此字段，则容器的主要组ID将为root（0）。runAsGroup指定时，用户1000和组3000也将拥有所有创建的文件。 由于fsGroup指定了字段，因此容器的所有进程也是补充组ID2000的一部分。卷的所有者/data/demo和在该卷中创建的任何文件都将是组ID 2000。 $ kubectl apply -f https://k8s.io/examples/pods/security/security-context.yaml $ kubectl get pod security-context-demo $ kubectl exec -it security-context-demo -- sh $ ps PID USER TIME COMMAND 1 1000 0:00 sleep 1h 6 1000 0:00 sh ... $ cd /data $ ls -l drwxrwsrwx 2 root 2000 4096 Jun 6 20:08 demo $ cd demo $ echo hello > testfile $ ls -l -rw-r--r-- 1 1000 2000 6 Jun 6 20:08 testfile $ id uid=1000 gid=3000 groups=2000 2.2 为 Pod 配置卷访问权限和属主变更策略 FEATURE STATE: Kubernetes v1.18 [alpha] 默认情况下，Kubernetes 在挂载一个卷时，会递归地更改每个卷中的内容的属主和访问权限，使之与 Pod 的 securityContext 中指定的 fsGroup 匹配。对于较大的数据卷，检查和变更属主与访问权限可能会花费很长时间，降低 Pod 启动速度。你可以在 securityContext 中使用 fsGroupChangePolicy 字段来控制 Kubernetes 检查和管理卷属主和访问权限的方式。 fsGroupChangePolicy - fsGroupChangePolicy 定义在卷被暴露给 Pod 内部之前对其 内容的属主和访问许可进行变更的行为。此字段仅适用于那些支持使用 fsGroup 来 控制属主与访问权限的卷类型。此字段的取值可以是： OnRootMismatch：只有根目录的属主与访问权限与卷所期望的权限不一致时，才改变其中内容的属主和访问权限。这一设置有助于缩短更改卷的属主与访问权限所需要的时间。 Always：在挂载卷时总是更改卷中内容的属主和访问权限。 securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 fsGroupChangePolicy: \"OnRootMismatch\" 这是一个Alpha功能。要使用它，请为kube-api-server，kube-controller-manager和kubelet 启用功能门 ConfigurableFSGroupPolicy。 2.3 对启动的容器使用sysctl修改内核参数，比如es这类容器镜像。 apiVersion: apps/v1 kind: StatefulSet metadata: name: sc-demo spec: template: containers: - name: sc-demo image: xxxxxx command: [ \"sh\", \"-c\", \"sleep 1h\" ] securityContext: privileged: true 给予 privileged: true 是一个比较粗的权限，一般不建议如此，可以为权限定义更细粒度的权限，类似需要在容器中使用 perf 命令，则可以进行如下定义： ... securityContext: capabilities: add: [\"SYS_ADMIN\"] 具体 capabilities 的使用规则可参考：在 Kubernetes 中配置 Container Capabilities 2.4 对启动的容器赋予SELinux标签 ... securityContext: seLinuxOptions: level: \"s0:c123,c456\" 要指定 SELinux，需要在宿主操作系统中装载 SELinux 安全性模块。seLinuxOptions 字段的取值是一个 SELinuxOptions 对象 3. 实例 参考链接： https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#before-you-begin 3.1 Set Container User and Group root@master:~/cks/securitytext# k run pod --image=busybox --command -oyaml --dry-run=client > pod.yaml -- sh -c 'sleep 1d' root@master:~/cks/securitytext# cat pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: securityContext: runAsUser: 1000 runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# k create -f pod.yaml root@master:~/cks/security# k get pods NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 83s root@master:~/cks/securitytext# k exec -ti pod -- sh / $ id uid=1000 gid=3000 / $ touch test touch: test: Permission denied / $ cd /tmp /tmp $ touch test /tmp $ ls -lh total 0 -rw-r--r-- 1 1000 3000 0 May 15 15:00 test 3.2 Force Container Non-Root root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: securityContext: runAsUser: 1000 runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} securityContext: runAsNonRoot: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted root@master:~/cks/securitytext# k create -f pod.yaml pod/pod created root@master:~/cks/securitytext# k get pod NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 68s #未指定非root用户 root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: # securityContext: # runAsUser: 1000 # runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} securityContext: runAsNonRoot: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted root@master:~/cks/securitytext# k create -f pod.yaml pod/pod created oot@master:~/cks/securitytext# k get pod NAME READY STATUS RESTARTS AGE pod 0/1 CreateContainerConfigError 0 22s #报错 root@master:~/cks/securitytext# k describe pod pod Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 50s default-scheduler Successfully assigned default/pod to node2 Normal Pulled 33s kubelet Successfully pulled image \"busybox\" in 16.355541429s Warning Failed 29s (x2 over 33s) kubelet Error: container has runAsNonRoot and image will run as root (pod: \"pod_default(99f9dcab-cebe-4432-ad70-08c86d3ed9be)\", container: pod) Normal Pulled 29s kubelet Successfully pulled image \"busybox\" in 2.345508306s Normal Pulling 16s (x3 over 49s) kubelet Pulling image \"busybox\" 3.3 Privileged Containers 3.4 Create Privileged Containers #非root用户测试执行sysctl命令 root@master:~/cks/securitytext# k get pods -w NAME READY STATUS RESTARTS AGE pod 0/1 ContainerCreating 0 3s pod 1/1 Running 0 23s root@master:~/cks/securitytext# k exec -ti pod -- sh / $ sysctl kernel.hostname=attacker sysctl: error setting key 'kernel.hostname': Read-only file system #无权限 / $ exit command terminated with exit code 1 #默认用户测试执行sysctl命令 root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: # securityContext: # runAsUser: 1000 # runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} # securityContext: # runAsNonRoot: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted root@master:~/cks/securitytext# k create -f pod.yaml pod/pod created root@master:~/cks/securitytext# k get pods NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 32s root@master:~/cks/securitytext# k exec -ti pod -- sh / # sysctl kernel.hostname=attacker sysctl: error setting key 'kernel.hostname': Read-only file system #无权限 / # id uid=0(root) gid=0(root) groups=10(wheel) #添加privileged: true特权 root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: # securityContext: # runAsUser: 1000 # runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} securityContext: privileged: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted root@master:~/cks/securitytext# k create -f pod.yaml pod/pod created root@master:~/cks/securitytext# k get pod NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 36s root@master:~/cks/securitytext# k exec -ti pod -- sh / # id uid=0(root) gid=0(root) groups=10(wheel) / # sysctl kernel.hostname=attacker #命令生效 kernel.hostname = attacker 3.5 PrivilegeEscalation 3.6 Practice - Disable PriviledgeEscalation root@master:~/cks/securitytext# vim pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod name: pod spec: # securityContext: # runAsUser: 1000 # runAsGroup: 3000 containers: - command: - sh - -c - sleep 1d image: busybox name: pod resources: {} securityContext: allowPrivilegeEscalation: true dnsPolicy: ClusterFirst restartPolicy: Always status: {} root@master:~/cks/securitytext# kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted root@master:~/cks/securitytext# k create -f pod.yaml pod/pod created root@master:~/cks/securitytext# k get pod NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 29s root@master:~/cks/securitytext# k exec -ti pod -- sh / # cat /proc/1/status Name: sleep State: S (sleeping) Tgid: 1 Ngid: 0 Pid: 1 PPid: 0 TracerPid: 0 Uid: 0 0 0 0 Gid: 0 0 0 0 FDSize: 64 Groups: 10 NStgid: 1 NSpid: 1 NSpgid: 1 NSsid: 1 VmPeak: 1308 kB VmSize: 1308 kB VmLck: 0 kB VmPin: 0 kB VmHWM: 4 kB VmRSS: 4 kB VmData: 48 kB VmStk: 132 kB VmExe: 892 kB VmLib: 18446744073709551612 kB VmPTE: 12 kB VmPMD: 8 kB VmSwap: 0 kB HugetlbPages: 0 kB Threads: 1 SigQ: 0/7778 SigPnd: 0000000000000000 ShdPnd: 0000000000000000 SigBlk: 0000000000000000 SigIgn: 0000000000000004 SigCgt: 0000000000000000 CapInh: 00000000a80425fb CapPrm: 00000000a80425fb CapEff: 00000000a80425fb CapBnd: 00000000a80425fb CapAmb: 0000000000000000 Seccomp: 0 Speculation_Store_Bypass: thread vulnerable Cpus_allowed: 00000000,00000000,00000000,00000003 Cpus_allowed_list: 0-1 Mems_allowed: 00000000,00000001 Mems_allowed_list: 0 voluntary_ctxt_switches: 47 nonvoluntary_ctxt_switches: 10 参考： Configure a Security Context for a Pod or Container linux selinux策略管理与标签 您应该了解10 个 Kubernetes安全上下文设置（待翻译） Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:49:29 "},"对象/kubernetes-Secrets.html":{"url":"对象/kubernetes-Secrets.html","title":"secret","keywords":"","body":"kubernetes secret1. 概览1.1 Pod 使用 Secret三种方式1.2 secret 类型2. kubectl 创建 Secret3. 手动创建 Secret3.1 data3.2 stringData3.3 data and stringdata4 从生成器创建 Secret4.1 从文件生成 Secret4.2 基于字符串值来创建 Secret5. 解码 Secret6. 编辑 Secret7. 使用 Secret7.1 Pod Access Secrets Loaded in a Volume7.2 将 Secret 键名映射到特定路径7.3 Secret 文件权限8. 挂载的 Secret 会被自动更新9. 以环境变量的形式使用 Secrets10. 案例10.1 以环境变量的形式使用 Secret10.2 包含 SSH 密钥的 Pod10.3 包含生产/测试凭据的 Pod10.4 Secret 卷中以句点号开头的文件kubernetes secret tagsstart 对象 secret tagsstop 1. 概览 Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 用户可以创建 Secret，同时系统也创建了一些 Secret。 要使用 Secret，Pod 需要引用 Secret。 1.1 Pod 使用 Secret三种方式 作为挂载到一个或多个容器上的 卷 中的文件。 作为容器的环境变量 由 kubelet 在为 Pod 拉取镜像时使用 1.2 secret 类型 创建 Secret 时，您可以使用typeSecret 资源的字段或某些等效的kubectl命令行标志（如果可用）指定其类型。在type一个秘密被用于促进各种机密数据的程序处理。 | 内置类型 | 用法 | |-------------------------------------|----------------------------| | Opaque | 任意用户定义数据 | | kubernetes.io/service-account-token | 服务帐号令牌 | | kubernetes.io/dockercfg | 序列化~/.dockercfg文件 | | kubernetes.io/dockerconfigjson | 序列化~/.docker/config.json文件 | | kubernetes.io/basic-auth | 基本身份验证凭据 | | kubernetes.io/ssh-auth | 用于 SSH 身份验证的凭据 | | kubernetes.io/tls | TLS 客户端或服务器的数据 | | bootstrap.kubernetes.io/token | 引导令牌数据 | 2. kubectl 创建 Secret 使用generic 子命令来指示OpaqueSecret 类型 # 创建本例中要使用的文件 echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt #第一种 # kubectl create secret 命令将这些文件打包到一个 Secret 中并在 API server 中创建了一个对象。 Secret 对象的名称必须是合法的 DNS 子域名。 kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt # 默认的键名是文件名。你也可以使用 [--from-file=[key=]source] 参数来设置键名。 #第二种 kubectl create secret generic db-user-pass \\ --from-file=username=./username.txt \\ --from-file=password=./password.txt #第三种 kubectl create secret generic dev-db-secret \\ --from-literal=username=devuser \\ --from-literal=password='S!B\\*d$zDsb=' kubectl get secrets kubectl describe secrets/db-user-pass 说明： 特殊字符（例如 $、\\、*、= 和 !）可能会被你的 Shell 解析，因此需要转义。 在大多数 Shell 中，对密码进行转义的最简单方式是使用单引号（'）将其扩起来。 您无需对文件中保存（--from-file）的密码中的特殊字符执行转义操作。 3. 手动创建 Secret 3.1 data $ echo -n 'admin' | base64 YWRtaW4= $ echo -n '1f2d1e2e67df' | base64 MWYyZDFlMmU2N2Rm 编写一个yaml apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm kubectl apply -f ./secret.yaml 在某些情况下，你可能希望改用 stringData 字段。 此字段允许您将非 base64 编码的字符串直接放入 Secret 中， 并且在创建或更新 Secret 时将为您编码该字符串。 3.2 stringData apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque stringData: config.yaml: |- apiUrl: \"https://my.api.com/api/v1\" username: {{username}} password: {{password}} $ kubectl get secret mysecret -o yaml apiVersion: v1 kind: Secret metadata: creationTimestamp: 2018-11-15T20:40:59Z name: mysecret namespace: default resourceVersion: \"7225\" uid: c280ad2e-e916-11e8-98f2-025000000001 type: Opaque data: config.yaml: YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19 3.3 data and stringdata 如果在 data 和 stringData 中都指定了某一字段，则使用 stringData 中的值: apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= stringData: username: administrator secret 中的生成结果： apiVersion: v1 kind: Secret metadata: creationTimestamp: 2018-11-15T20:46:46Z name: mysecret namespace: default resourceVersion: \"7579\" uid: 91460ecb-e917-11e8-98f2-025000000001 type: Opaque data: username: YWRtaW5pc3RyYXRvcg== 其中的 YWRtaW5pc3RyYXRvcg== 解码后即是 administrator。 data 和 stringData 的键必须由字母数字字符 '-', '_' 或者 '.' 组成. 4 从生成器创建 Secret Kubectl 从 1.14 版本开始支持使用 Kustomize 管理对象。 Kustomize 提供资源生成器创建 Secret 和 ConfigMaps。 Kustomize 生成器要在当前目录内的 kustomization.yaml 中指定。 生成 Secret 之后，使用 kubectl apply 在 API 服务器上创建对象。 4.1 从文件生成 Secret cat ./kustomization.yaml secretGenerator: - name: db-user-pass files: - username.txt - password.txt EOF 应用包含 kustomization.yaml 目录以创建 Secret 对象。 kubectl apply -k . kubectl get secrets 输出类似于： NAME TYPE DATA AGE db-user-pass-96mffmfh4k Opaque 2 51s $ kubectl describe secrets/db-user-pass-96mffmfh4k Name: db-user-pass Namespace: default Labels: Annotations: Type: Opaque Data ==== password.txt: 12 bytes username.txt: 5 bytes 4.2 基于字符串值来创建 Secret cat ./kustomization.yaml secretGenerator: - name: db-user-pass literals: - username=admin - password=secret EOF 应用包含 kustomization.yaml 目录以创建 Secret 对象。 kubectl apply -k . kubectl get secret mysecret -o yaml 5. 解码 Secret 输出类似于： apiVersion: v1 kind: Secret metadata: creationTimestamp: 2016-01-22T18:41:56Z name: mysecret namespace: default resourceVersion: \"164619\" uid: cfee02d6-c137-11e5-8d73-42010af00002 type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 解码 password 字段： echo 'MWYyZDFlMmU2N2Rm' | base64 --decode 输出类似于： 1f2d1e2e67df 6. 编辑 Secret kubectl edit secrets mysecret 7. 使用 Secret 7.1 Pod Access Secrets Loaded in a Volume 创建一个 Secret 或者使用已有的 Secret。多个 Pod 可以引用同一个 Secret。 修改你的 Pod 定义，在 spec.volumes[] 下增加一个卷。可以给这个卷随意命名， 它的spec.volumes[].secret.secretName 必须是 Secret 对象的名字。 将 spec.containers[].volumeMounts[] 加到需要用到该 Secret 的容器中。 指定 spec.containers[].volumeMounts[].readOnly = true 和spec.containers[].volumeMounts[].mountPath 为你想要该 Secret 出现的尚未使用的目录。 修改你的镜像并且／或者命令行，让程序从该目录下寻找文件。 Secret 的 data 映射中的每一个键都对应 mountPath 下的一个文件名。 apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret 7.2 将 Secret 键名映射到特定路径 你可以使用 spec.volumes[].secret.items 字段修改每个键对应的目标路径： apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username 将会发生什么呢： username Secret 存储在 /etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。 password Secret 没有被映射7.3 Secret 文件权限 你还可以指定 Secret 将拥有的权限模式位。如果不指定，默认使用 0644 示例： apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" volumes: - name: foo secret: secretName: mysecret defaultMode: 256 之后，Secret 将被挂载到 /etc/foo 目录，而所有通过该 Secret 卷挂载 所创建的文件的权限都是 0400。 你还可以使用映射，如上一个示例，并为不同的文件指定不同的权限，如下所示： apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username mode: 511 在这里，位于 /etc/foo/my-group/my-username 的文件的权限值为 0777。 由于 JSON 限制，必须以十进制格式指定模式，即 511。 8. 挂载的 Secret 会被自动更新 当已经存储于卷中被使用的 Secret 被更新时，被映射的键也将终将被更新。 组件 kubelet 在周期性同步时检查被挂载的 Secret 是不是最新的。 但是，它会使用其本地缓存的数值作为 Secret 的当前值。 FEATURE STATE: Kubernetes v1.18 [alpha] Kubernetes 的 alpha 特性 不可变的 Secret 和 ConfigMap 提供了一种可选配置， 可以设置各个 Secret 和 ConfigMap 为不可变的。 对于大量使用 Secret 的集群（至少有成千上万各不相同的 Secret 供 Pod 挂载）， 禁止变更它们的数据有下列好处： 防止意外（或非预期的）更新导致应用程序中断 通过将 Secret 标记为不可变来关闭 kube-apiserver 对其的监视，从而显著降低 kube-apiserver 的负载，提升集群性能。 使用这个特性需要启用 ImmutableEmphemeralVolumes 特性开关 并将 Secret 或 ConfigMap 的 immutable 字段设置为 true. 例如： apiVersion: v1 kind: Secret metadata: ... data: ... immutable: true 说明： 一旦一个 Secret 或 ConfigMap 被标记为不可变，撤销此操作或者更改 data 字段的内容都是 不 可能的。 只能删除并重新创建这个 Secret。现有的 Pod 将维持对已删除 Secret 的挂载点 - 建议重新创建这些 Pod。 9. 以环境变量的形式使用 Secrets 将 Secret 作为 Pod 中的环境变量使用： 创建一个 Secret 或者使用一个已存在的 Secret。多个 Pod 可以引用同一个 Secret。 修改 Pod 定义，为每个要使用 Secret 的容器添加对应 Secret 键的环境变量。 使用 Secret 键的环境变量应在 env[x].valueFrom.secretKeyRef 中指定 要包含的 Secret 名称和键名。 更改镜像并／或者命令行，以便程序在指定的环境变量中查找值。 apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never echo $SECRET_USERNAME 输出类似于： admin echo $SECRET_PASSWORD 输出类似于： 1f2d1e2e67df 10. 案例 10.1 以环境变量的形式使用 Secret 创建一个 Secret 定义： apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: USER_NAME: YWRtaW4= PASSWORD: MWYyZDFlMmU2N2Rm 生成 Secret 对象： kubectl apply -f mysecret.yaml 使用 envFrom 将 Secret 的所有数据定义为容器的环境变量。 Secret 中的键名称为 Pod 中的环境变量名称： apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"env\" ] envFrom: - secretRef: name: mysecret restartPolicy: Never 10.2 包含 SSH 密钥的 Pod 创建一个包含 SSH 密钥的 Secret： kubectl create secret generic ssh-key-secret \\ --from-file=ssh-privatekey=/path/to/.ssh/id_rsa \\ --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub 注意： 发送自己的 SSH 密钥之前要仔细思考：集群的其他用户可能有权访问该密钥。 你可以使用一个服务帐户，分享给 Kubernetes 集群中合适的用户，这些用户是你要分享的。 如果服务账号遭到侵犯，可以将其收回。 现在我们可以创建一个 Pod，令其引用包含 SSH 密钥的 Secret，并通过存储卷来使用它： apiVersion: v1 kind: Pod metadata: name: secret-test-pod labels: name: secret-test spec: volumes: - name: secret-volume secret: secretName: ssh-key-secret containers: - name: ssh-test-container image: mySshImage volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\" 容器中的命令运行时，密钥的片段可以在以下目录找到： /etc/secret-volume/ssh-publickey /etc/secret-volume/ssh-privatekey 然后容器可以自由使用 Secret 数据建立一个 SSH 连接。 10.3 包含生产/测试凭据的 Pod 下面的例子展示的是两个 Pod。 一个 Pod 使用包含生产环境凭据的 Secret，另一个 Pod 使用包含测试环境凭据的 Secret。 你可以创建一个带有 secretGenerator 字段的 kustomization.yaml 文件，或者执行 kubectl create secret： kubectl create secret generic prod-db-secret \\ --from-literal=username=produser \\ --from-literal=password=Y4nys7f11 kubectl create secret generic test-db-secret \\ --from-literal=username=testuser \\ --from-literal=password=iluvtests 说明： 特殊字符（例如 $、\\、*、= 和 !）会被你的 Shell解释，因此需要转义。 在大多数 Shell 中，对密码进行转义的最简单方式是用单引号（'）将其括起来。 例如，如果您的实际密码是 S!B\\*d$zDsb，则应通过以下方式执行命令： kubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\\*d$zDsb=' 注意：您无需对文件中的密码（--from-file）中的特殊字符进行转义。 创建 pod ： $ cat pod.yaml apiVersion: v1 kind: List items: - kind: Pod apiVersion: v1 metadata: name: prod-db-client-pod labels: name: prod-db-client spec: volumes: - name: secret-volume secret: secretName: prod-db-secret containers: - name: db-client-container image: myClientImage volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\" - kind: Pod apiVersion: v1 metadata: name: test-db-client-pod labels: name: test-db-client spec: volumes: - name: secret-volume secret: secretName: test-db-secret containers: - name: db-client-container image: myClientImage volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\" EOF 将 Pod 添加到同一个 kustomization.yaml 文件 $ cat > kustomization.yaml resources: - pod.yaml EOF 通过下面的命令应用所有对象 kubectl apply -k . 两个容器都会在其文件系统上存在以下文件，其中包含容器对应的环境的值： /etc/secret-volume/username /etc/secret-volume/password 10.4 Secret 卷中以句点号开头的文件 你可以通过定义以句点开头的键名，将数据“隐藏”起来。 例如，当如下 Secret 被挂载到 secret-volume 卷中： apiVersion: v1 kind: Secret metadata: name: dotfile-secret data: .secret-file: dmFsdWUtMg0KDQo= --- apiVersion: v1 kind: Pod metadata: name: secret-dotfiles-pod spec: volumes: - name: secret-volume secret: secretName: dotfile-secret containers: - name: dotfile-test-container image: k8s.gcr.io/busybox command: - ls - \"-l\" - \"/etc/secret-volume\" volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\" 卷中将包含唯一的叫做 .secret-file 的文件。 容器 dotfile-test-container 中，该文件处于 /etc/secret-volume/.secret-file 路径下。 说明： 以点号开头的文件在 ls -l 的输出中会被隐藏起来； 列出目录内容时，必须使用 ls -la 才能看到它们 参考资料： kubernetes secret/ Google Kubernetes Engine (GKE) secret Kubernetes Secrets – How to Create, Use and Access Secrets Kubernetes Secrets | How To Create, Use, and Access https://www.containiq.com/post/kubernetes-secrets Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 05:30:48 "},"对象/Kubernetes-Ingress.html":{"url":"对象/Kubernetes-Ingress.html","title":"Ingress","keywords":"","body":"kubernetes ingress 如何通过域名访问您的应用0. 背景1. 简介2. Ingress对象定义3. 安装ingress-controller4. 测试1: 代理一个服务5. 测试2: 代理两个服务6. 总结7. 思考题kubernetes ingress 如何通过域名访问您的应用 tagsstart 对象 Ingress tagsstop 0. 背景 由于每个 Service 都要有一个负载均衡服务，所以这个做法实际上既浪费成本又高。作为用户，我其实更希望看到 Kubernetes 为我内置一个全局的负载均衡器。然后，通过我访问的 URL，把请求转发给不同的后端 Service。 这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。所以，Ingress 的功能其实很容易理解：所谓 Ingress，就是 Service 的“Service”。 1. 简介 Ingress 是 Kubernetes 的一种 API 对象，将集群内部的 Service 通过 HTTP/HTTPS 方式暴露到集群外部，并通过规则定义 HTTP/HTTPS 的路由。Ingress 具备如下特性：集群外部可访问的 URL、负载均衡、SSL Termination、按域名路由（name-based virtual hosting）。增加了7层的识别能力，可以根据 http header, path 等进行路由转发。 举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中，https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。而，https://cafe.example.com/tea，对应的则是“茶水点餐系统”。这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务。 那么现在，我如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？ Ingress: 配置转发规则，类似于 nginx 的配置文件 Ingress Controller: 转发，类似于 nginx，它会读取 Ingress 的规则并转化为 nginx 的配置文件 而 Ingress Controller 除了 nginx 外还有 haproxy，ingress 等等，我们选用 nginx 作为 Ingress Controller 2. Ingress对象定义 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: cafe-ingress spec: tls: - hosts: - cafe.example.com secretName: cafe-secret rules: - host: cafe.example.com http: paths: - path: /tea backend: serviceName: tea-svc servicePort: 80 - path: /coffee backend: serviceName: coffee-svc servicePort: 80 在上面这个名叫 cafe-ingress.yaml 文件中，最值得我们关注的，是 rules 字段。在 Kubernetes 里，这个字段叫作：IngressRule。 IngressRule 的 Key，就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。 而 host 字段定义的值，就是这个 Ingress 的入口。这也就意味着，当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。 而接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc）。 通过上面的讲解，不难看到，所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。 一个 Ingress 对象的主要内容，实际上就是一个“反向代理”服务（比如：Nginx）的配置文件的描述。而这个代理服务对应的转发规则，就是 IngressRule。 这就是为什么在每条 IngressRule 里，需要有一个 host 字段来作为这条 IngressRule 的入口，然后还需要有一系列 path 字段来声明具体的转发策略。这其实跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。而有了 Ingress 这样一个统一的抽象，Kubernetes 的用户就无需关心 Ingress 的具体细节了。在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。 然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。 接下来，我就以最常用的 Nginx Ingress Controller 为例，在我们前面用 kubeadm 部署的 Bare-metal 环境中，和你实践一下 Ingress 机制的使用过程。 3. 安装ingress-controller 安装指导 下载 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.43.0/deploy/static/provider/baremetal/deploy.yaml 由于需要爬梯子下载镜像k8s.gcr.io/ingress-nginx/controller:v0.43.0，所以可以优先从阿里云下载 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.43.0/deploy/static/provider/baremetal/deploy.yaml docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v0.43.0 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v0.43.0 k8s.gcr.io/ingress-nginx/controller:v0.43.0 kubectl apply -f deploy.yaml 内容类型包含： kind: Namespace kind: ServiceAccount kind: ConfigMap kind: ClusterRole kind: ClusterRoleBinding kind: Role kind: RoleBinding kind: Service kind: Service kind: Deployment kind: ValidatingWebhookConfiguration kind: ServiceAccount kind: ClusterRole kind: ClusterRoleBinding kind: Role kind: RoleBinding kind: Job kind: Job 查看部分内容： kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: ... spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -> 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE - name: http valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 可以看到，在上述 YAML 文件中，我们定义了一个使用 nginx-ingress-controller 镜像的 Pod。需要注意的是，这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath）。 而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。 而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置。 此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。 可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。 查看pod ingress-nginx-controller是否正常running $ kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-create-tqtmk 0/1 Completed 0 31m ingress-nginx ingress-nginx-admission-patch-cnwtg 0/1 Completed 0 31m ingress-nginx ingress-nginx-controller-548df9766d-kp5qq 1/1 Running 0 31m 查看svc ,30304和31794是我们从外网访问的统一端口 $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.97.119.123 80:30304/TCP,443:31794/TCP 32m ingress-nginx-controller-admission ClusterIP 10.111.32.235 443/TCP 32m 你一定要记录下这个 Service 的访问入口，即：宿主机的地址和 NodePort 的端口 为了后面方便使用，我会把上述访问入口设置为环境变量： $ IC_IP=10.168.0.2 # 任意一台宿主机的地址 $ IC_HTTPS_PORT=30304 # NodePort端口 可以扩容 kubectl scale --replicas=2 deploy/nginx-ingress-controller -n ingress-nginx 4. 测试1: 代理一个服务 deploy-demo.yaml apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: selector: app: myapp release: stable ports: - name: myapp port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp namespace: default spec: selector: matchLabels: app: myapp release: stable replicas: 3 template: metadata: labels: app: myapp release: stable spec: containers: - name: myapp image: nginx imagePullPolicy: IfNotPresent ports: - name: myapp 　　containerPort: 80 vim ingress-myapp.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-myapp namespace: default annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: httpd.hequan.com http: paths: - path: backend: serviceName: myapp servicePort: 80 kubectl create -f deploy-demo.yaml kubectl create -f ingress-myapp.yaml linuxhosts添加域名解析 vim /etc/hosts #任意node_ip 192.168.100.112 httpd.hequan.com windows添加C:\\Windows\\System32\\drivers\\etc\\hosts 192.168.100.112 httpd.hequan.com 访问 httpd.hequan.com:32080 5. 测试2: 代理两个服务 介质下载 #部署cafe的tea与coffee服务 $ kubectl create -f cafe.yaml #创建 Ingress 所需的 SSL 证书（tls.crt）和密钥（tls.key） $ kubectl create -f cafe-secret.yaml #创建ingress对象 $ kubectl create -f cafe-ingress.yaml 查看信息 $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE cafe-ingress cafe.example.com 80, 443 2h $ kubectl describe ingress cafe-ingress Name: cafe-ingress Namespace: default Address: Default backend: default-http-backend:80 () TLS: cafe-secret terminates cafe.example.com Rules: Host Path Backends ---- ---- -------- cafe.example.com /tea tea-svc:80 () /coffee coffee-svc:80 () Annotations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 4m nginx-ingress-controller Ingress default/cafe-ingress 可以看到，这个 Ingress 对象最核心的部分，正是 Rules 字段。其中，我们定义的 Host 是cafe.example.com，它有两条转发规则（Path），分别转发给 tea-svc 和 coffee-svc。 当然，在 Ingress 的 YAML 文件里，你还可以定义多个 Host，比如restaurant.example.com、movie.example.com等等，来为更多的域名提供负载均衡服务。 接下来，我们就可以通过访问这个 Ingress 的地址和端口，访问到我们前面部署的应用了，比如，当我们访问https://cafe.example.com:443/coffee时，应该是 coffee 这个 Deployment 负责响应我的请求。我们可以来尝试一下： $ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/coffee --insecureServer address: 10.244.1.56:80 Server name: coffee-7dbb5795f6-vglbv Date: 03/Nov/2018:03:55:32 +0000 URI: /coffee Request ID: e487e672673195c573147134167cf898 我们可以看到，访问这个 URL 得到的返回信息是：Server name: coffee-7dbb5795f6-vglbv。这正是 coffee 这个 Deployment 的名字。 而当我访问https://cafe.example.com:433/tea的时候，则应该是 tea 这个 Deployment 负责响应我的请求（Server name: tea-7d57856c44-lwbnp），如下所示： $ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/tea --insecure Server address: 10.244.1.58:80 Server name: tea-7d57856c44-lwbnp Date: 03/Nov/2018:03:55:52 +0000 URI: /tea Request ID: 32191f7ea07cb6bb44a1f43b8299415c 可以看到，Nginx Ingress Controller 为我们创建的 Nginx 负载均衡器，已经成功地将请求转发给了对应的后端 Service。以上，就是 Kubernetes 里 Ingress 的设计思想和使用方法了。 不过，你可能会有一个疑问，如果我的请求没有匹配到任何一条 IngressRule，那么会发生什么呢？首先，既然 Nginx Ingress Controller 是用 Nginx 实现的，那么它当然会为你返回一个 Nginx 的 404 页面。 不过，Ingress Controller 也允许你通过 Pod 启动命令里的–default-backend-service 参数，设置一条默认规则，比如：–default-backend-service=nginx-default-backend。这样，任何匹配失败的请求，就都会被转发到这个名叫 nginx-default-backend 的 Service。所以，你就可以通过部署一个专门的 Pod，来为用户返回自定义的 404 页面了。 6. 总结 Ingress 实际上就是 Kubernetes 对“反向代理”的抽象 目前，Ingress 只能工作在七层，而 Service 只能工作在四层。所以当你想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时，都必须通过 Ingress 来进行。 当然，正如同很多负载均衡项目可以同时提供七层和四层代理一样，将来 Ingress 的进化中，也会加入四层代理的能力。这样，一个比较完善的“反向代理”机制就比较成熟了。而 Kubernetes 提出 Ingress 概念的原因其实也非常容易理解，有了 Ingress 这个抽象，用户就可以根据自己的需求来自由选择 Ingress Controller。比如，如果你的应用对代理服务的中断非常敏感，那么你就应该考虑选择类似于 Traefik 这样支持“热加载”的 Ingress Controller 实现。 在实际的生产环境中，Ingress 带来的灵活度和自由度，对于使用容器的用户来说，其实是非常有意义的。要知道，当年在 Cloud Foundry 项目里，不知道有多少人为了给 Gorouter 组件配置一个 TLS 而伤透了脑筋。 7. 思考题 如果我的需求是，当访问www.mysite.com和 forums.mysite.com时，分别访问到不同的 Service（比如：site-svc 和 forums-svc）。那么，这个 Ingress 该如何定义呢？请你描述出 YAML 文件中的 rules 字段。 答案： apiVersion: extensions/v1beta1 kind: Ingress metadata: name: cafe-ingress spec: tls: - hosts: - www.mysite.com - forums.mysite.com secretName: mysite-secret rules: - host: www.mysite.com http: paths: - path: /mysite backend: serviceName: site-svc servicePort: 80 - host: forums.mysite.com http: paths: - path: /forums backend: serviceName: site-svc servicePort: 80 参考： Ingress Ingress 控制器 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 11:08:50 "},"对象/Kubernetes-Service.html":{"url":"对象/Kubernetes-Service.html","title":"Service","keywords":"","body":"Kubernetes Service1. 介绍2. 定义3. 没有 selector 的 Service4. 应用service5. 原理5.1 kube-proxy 和 iptables5.2 kube-proxy 和ipvs5.3 service与DNS关系6. VIP 和 Service 代理6.1 userspace 代理模式6.2 iptables 代理模式7. 对外访问的service类型7.1 NodePort 类型7.2 LoadBalancer 类型7.3 ExternalName类型8. nodePort、port、targetPort、containerPort 区分8.1 nodePort8.2 port8.3 targetPort8.4 containerPort9. 如何排查Service 相关的问题10. 总结Kubernetes Service tagsstart Service 对象 tagsstop 电影《降临》根据小说《你一生的故事》改编 1. 介绍 Kubernetes Service定义了这样一种抽象，Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。 Pod 能够被 Service 访问到，通常是通过 Label Selector实现的。 对 Kubernetes 集群中的应用，Kubernetes 提供了简单的 Endpoints API，只要 Service 中的一组 Pod 发生变更，应用程序就会被更新。 对非 Kubernetes 集群中的应用，Kubernetes 提供了基于 VIP 的网桥的方式访问 Service，再由 Service 重定向到 backend Pod。 所谓 Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则，以及 kube-dns 生成的 DNS 记录。 2. 定义 一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。 例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 \"app=MyApp\" 标签。 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 上述配置将创建一个名称为 “my-service” 的 Service 对象，这个 Service 的 80 端口，代理的是 Pod 的 9376 端口，并且具有标签 \"app=MyApp\" 的 Pod 上。 这个 Service 将被指派一个 IP 地址（通常称为 “Cluster IP”），它会被服务的代理使用（见下面）。 该 Service 的 selector 将会持续评估，处理结果将被 POST 到一个名称为 “my-service” 的 Endpoints 对象上。 Service 能够将一个接收端口映射到任意的 targetPort。 默认情况下，targetPort 将被设置为与 port 字段相同的值。 可能更有趣的是，targetPort 可以是一个字符串，引用了 backend Pod 的一个端口的名称。 但是，实际指派给该端口名称的端口号，在每个 backend Pod 中可能并不相同。 对于部署和设计 Service ，这种方式会提供更大的灵活性。 例如，可以在 backend 软件下一个版本中，修改 Pod 暴露的端口，并不会中断客户端的调用。 Kubernetes Service 能够支持 TCP 和 UDP 协议，默认 TCP 协议。 很多 Service 需要暴露多个端口。对于这种情况，Kubernetes 支持在 Service 对象中定义多个端口。 当使用多个端口时，必须给出所有的端口的名称，这样 Endpoint 就不会产生歧义，例如： kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377 3. 没有 selector 的 Service Servcie 抽象了该如何访问 Kubernetes Pod，但也能够抽象其它类型的 backend，例如： 希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。 希望服务指向另一个 Namespace 中或其它集群中的服务。 正在将工作负载转移到 Kubernetes 集群，和运行在 Kubernetes 集群之外的 backend。 在任何这些场景中，都能够定义没有 selector 的 Service ： kind: Service apiVersion: v1 metadata: name: my-service #名字匹配 spec: ports: - protocol: TCP port: 80 targetPort: 9376 由于这个 Service 没有 selector，就不会创建相关的 Endpoints 对象。可以手动将 Service 映射到指定的 Endpoints： kind: Endpoints apiVersion: v1 metadata: name: my-service #名字匹配 subsets: - addresses: - ip: 1.2.3.4 ports: - port: 9376 注意：Endpoint IP 地址不能是 loopback（127.0.0.0/8）、 link-local（169.254.0.0/16）、或者 link-local 多播（224.0.0.0/24）。 4. 应用service apiVersion: apps/v1 kind: Deployment metadata: name: hostnames spec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP 这个应用的作用，就是每次访问 9376 端口时，返回它自己的 hostname。而被 selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get ep 命令看到它们，如下所示： $ kubectl get endpoints hostnames NAME ENDPOINTS hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。 而此时，通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod 了： $ kubectl get svc hostnames NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hostnames ClusterIP 10.0.1.175 80/TCP 5s $ curl 10.0.1.175:80 hostnames-0uton $ curl 10.0.1.175:80 hostnames-yp2kp $ curl 10.0.1.175:80 hostnames-bvc05 这个 VIP 地址是 Kubernetes 自动为 Service 分配的。而像上面这样，通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。对于这种方式，我们称为：ClusterIP 模式的 Service。 5. 原理 Kubernetes 里的 Service 究竟是如何工作的呢？ 实际上，Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。 5.1 kube-proxy 和 iptables 举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示： -A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 可以看到，这条 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。 而我们前面已经看到，10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。 那么，我们即将跳转到的 KUBE-SVC-NWV5X2332I4OT4T3 规则，又有什么作用呢？实际上，它是一组规则的集合，如下所示： -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR 可以看到，这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。 而随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。 而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。 这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。 你可以想一下，如果把这三条规则的 probability 字段的值都设置成 1/3，最终每条规则被选中的概率会变成多少。通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示： -A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376 -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376 可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。 而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。 这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。 以上，就是 Service 最基本的工作原理。 5.2 kube-proxy 和ipvs Kubernetes 的 kube-proxy 还支持一种叫作 IPVS 的模式。这又是怎么一回事儿呢？ 其实，通过上面的讲解，你可以看到，kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。 难想到，当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以说，一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。 而 IPVS 模式的 Service，就是解决这个问题的一个行之有效的方法。 IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示： # ip addr ... 73：kube-ipvs0： mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff inet 10.0.1.175/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever 而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示： # ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.102.128.4:80 rr -> 10.244.3.6:9376 Masq 1 0 0 -> 10.244.1.7:9376 Masq 1 0 0 -> 10.244.2.3:9376 Masq 1 0 0 可以看到，这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。 这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。 所以，在大规模集群里，我非常建议你为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。 5.3 service与DNS关系 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。 而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。 此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：..pod.cluster.local。这条记录指向 Pod 的 IP 地址。 而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：...svc.cluster.local。这条记录也指向 Pod 的 IP 地址。 但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：...svc.cluster.local，比如： apiVersion: v1 kind: Service metadata: name: default-subdomain spec: selector: name: busybox clusterIP: None ports: - name: foo port: 1234 targetPort: 1234 --- apiVersion: v1 kind: Pod metadata: name: busybox1 labels: name: busybox spec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox 在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。 需要注意的是，在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。 一个可选（尽管强烈推荐）集群插件 是 DNS 服务器。 DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录。 如果整个集群的 DNS 一直被启用，那么所有的 Pod 应该能够自动对 Service 进行名称解析。 例如，有一个名称为 \"my-service\" 的 Service，它在 Kubernetes 集群中名为 \"my-ns\" 的 Namespace 中，为 \"my-service.my-ns\" 创建了一条 DNS 记录。 在名称为 \"my-ns\" 的 Namespace 中的 Pod 应该能够简单地通过名称查询找到 \"my-service\"。 在另一个 Namespace 中的 Pod 必须限定名称为 \"my-service.my-ns\"。 这些名称查询的结果是 Cluster IP。 Kubernetes 也支持对端口名称的 DNS SRV（Service）记录。 如果名称为 \"my-service.my-ns\" 的 Service 有一个名为 \"http\" 的 TCP 端口，可以对 \"_http._tcp.my-service.my-ns\" 执行 DNS SRV 查询，得到 \"http\" 的端口号。 Kubernetes DNS 服务器是唯一的一种能够访问 ExternalName 类型的 Service 的方式。 更多信息可以查看DNS Pod 和 Service。 总结： 一种是通过..svc.cluster.local访问。对应于clusterIP 另一种是通过...svc.cluster.local访问,对应于headless service. / # nslookup *.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: *.default.svc.cluster.local Address 1: 10.244.1.7 busybox-3.default-subdomain.default.svc.cluster.local Address 2: 10.96.0.1 kubernetes.default.svc.cluster.local Address 3: 10.97.103.223 hostnames.default.svc.cluster.local 6. VIP 和 Service 代理 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起，默认就是 iptables 代理。 在 Kubernetes v1.0 版本，Service 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务。 6.1 userspace 代理模式 这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的backend Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个 backend Pod，是基于 Service 的 SessionAffinity 来确定的。 最后，它安装 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP）和 Port 的请求，并重定向到代理端口，代理端口再代理请求到 backend Pod。 网络返回的结果是，任何到达 Service 的 IP:Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes、Service、或 Pod 的任何信息。 默认的策略是，通过 round-robin 算法来选择 backend Pod。 实现基于客户端 IP 的会话亲和性，可以通过设置 service.spec.sessionAffinity 的值为 \"ClientIP\" （默认值为 \"None\"）。 6.2 iptables 代理模式 这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会安装 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某个上面。 对于每个 Endpoints 对象，它也会安装 iptables 规则，这个规则会选择一个 backend Pod。 默认的策略是，随机选择一个 backend。 实现基于客户端 IP 的会话亲和性，可以将 service.spec.sessionAffinity 的值设置为 \"ClientIP\" （默认值为 \"None\"）。 和 userspace 代理类似，网络返回的结果是，任何到达 Service 的 IP:Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes、Service、或 Pod 的任何信息。 这应该比 userspace 代理更快、更可靠。然而，不像 userspace 代理，如果初始选择的 Pod 没有响应，iptables 代理能够自动地重试另一个 Pod，所以它需要依赖 readiness probes。 7. 对外访问的service类型 对一些应用（如 Frontend）的某些部分，可能希望通过外部（Kubernetes 集群外部）IP 地址暴露 Service。 Kubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。 Type 的取值以及行为如下： ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 :，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和ClusterIP 服务。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns才支持。 7.1 NodePort 类型 如果设置 type 的值为 \"NodePort\"，每个 Node 将从该端口（每个 Node 上的同一端口）代理到 Service。该端口将通过 Service 的 spec.ports[*].nodePort 字段被指定。 需要注意的是，Service 将能够通过 :spec.ports[*].nodePort 和 spec.clusterIp:spec.ports[*].port 而对外可见。 对外访问这里最常用的一种方式就是：NodePort apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: type: NodePort ports: - nodePort: 8080 targetPort: 80 protocol: TCP name: http - nodePort: 443 protocol: TCP name: https selector: run: my-nginx 在这个 Service 的定义里，我们声明它的类型是，type=NodePort。然后，我在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口。 当然，如果你不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，你可以通过 kube-apiserver 的–service-node-port-range 参数来修改它。 那么这时候，要访问这个 Service，你只需要访问： :8080 可以访问到某一个被代理的 Pod 的 80 端口了。而在理解了我在上一篇文章中讲解的 Service 的工作原理之后，NodePort 模式也就非常容易理解了。显然，kube-proxy 要做的，就是在每台宿主机上生成这样一条 iptables 规则： -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示： -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE 可以看到，这条规则设置在 POSTROUTING 检查点，也就是说，它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。 可是，为什么一定要对流出的包做 SNAT操作呢？ client \\ ^ \\ \\ v \\ node 1 v | endpoint 当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。 而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。 可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。所以此时，Pod 就会直接将回复发给client。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。 所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。 当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。 所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。 而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示： client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | v endpoint 当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。 7.2 LoadBalancer 类型 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service，将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段被发布出去。 配置文件样板： apiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: - port: 8765 targetPort: 9376 type: LoadBalancer 命令行创建： $ kubectl expose rc example --port=8765 --target-port=9376 \\ --name=example-service --type=LoadBalancer $ kubectl describe services example-service Name: example-service Namespace: default Labels: Annotations: Selector: app=example Type: LoadBalancer IP: 10.67.252.103 LoadBalancer Ingress: 192.0.2.89 Port: 80/TCP NodePort: 32445/TCP Endpoints: 10.64.0.4:80,10.64.1.5:80,10.64.2.4:80 Session Affinity: None Events: 如果你在 Minikube 上运行服务，你可以通过以下命令找到分配的 IP 地址和端口：minikube service example-service --url demo配置文件 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 nodePort: 30061 clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 type: LoadBalancer status: loadBalancer: ingress: - ip: 146.148.47.155 来自外部负载均衡器的流量将直接打到 backend Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。 在这些情况下，将根据用户设置的 loadBalancerIP 来创建负载均衡器。 某些云提供商允许设置 loadBalancerIP。如果没有设置 loadBalancerIP，将会给负载均衡器指派一个临时 IP。 如果设置了 loadBalancerIP，但云提供商并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。 7.3 ExternalName类型 第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName ExternalName Service 是 Service 的特例，它没有 selector，也没有定义任何的端口和 Endpoint。 相反地，对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式来提供服务。 kind: Service apiVersion: v1 metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com 当查询主机 my-service.prod.svc.CLUSTER时，集群的 DNS 服务将返回一个值为 my.database.example.com 的 CNAME 记录。 访问这个服务的工作方式与其它的相同，唯一不同的是重定向发生在 DNS 层，而且不会进行代理或转发。 如果后续决定要将数据库迁移到 Kubernetes 集群中，可以启动对应的 Pod，增加合适的 Selector 或 Endpoint，修改 Service 的 type。 如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。 通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 的端口上的流量，将会被路由到 Service 的 Endpoint 上。 externalIPs 不会被 Kubernetes 管理，它属于集群管理员的职责范畴。 根据 Service 的规定，externalIPs 可以同任意的 ServiceType 来一起指定。 在上面的例子中，my-service 可以在 80.11.12.10:80（外部 IP:端口）上被客户端访问。 kind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。 8. nodePort、port、targetPort、containerPort 区分 8.1 nodePort nodePort提供了集群外部客户端访问service的一种方式，:nodePort提供了集群外部客户端访问service的端口，即nodeIP:nodePort提供了外部流量访问k8s集群中service的入口。 比如外部用户要访问k8s集群中的一个Web应用，那么我们可以配置对应service的type=NodePort，nodePort=30001。其他用户就可以通过浏览器http://node:30001访问到该web服务。 而数据库等服务可能不需要被外界访问，只需被内部服务访问即可，那么我们就不必设置service的NodePort。 8.2 port port是暴露在cluster ip上的端口，:port提供了集群内部客户端访问service的入口，即clusterIP:port。 mysql容器暴露了3306端口（参考DockerFile），集群内其他容器通过33306端口访问mysql服务，但是外部流量不能访问mysql服务，因为mysql服务没有配置NodePort。对应的service.yaml如下： apiVersion: v1 kind: Service metadata: name: mysql-service spec: ports: - port: 33306 targetPort: 3306 selector: name: mysql-pod 8.3 targetPort targetPort是pod上的端口，从port/nodePort上来的数据，经过kube-proxy流入到后端pod的targetPort上，最后进入容器。 与制作容器时暴露的端口一致（使用DockerFile中的EXPOSE），例如官方的nginx（参考DockerFile）暴露80端口。 对应的service.yaml如下： apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort // 配置NodePort，外部流量可访问k8s中的服务 ports: - port: 30080 // 服务访问端口 targetPort: 80 // pod控制器中定义的端口 nodePort: 30001 // NodePort selector: name: nginx-pod 8.4 containerPort containerPort是在pod控制器中定义的、pod中的容器需要暴露的端口。 总的来说，port和nodePort都是service的端口，前者暴露给k8s集群内部服务访问，后者暴露给k8s集群外部流量访问。从这两个端口到来的数据都需要经过反向代理kube-proxy，流入后端pod的targetPort上，最后到达pod内容器的containerPort 9. 如何排查Service 相关的问题 其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。 比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常： # 在一个Pod里执行 $ nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints： $ kubectl get endpoints hostnames NAME ENDPOINTS hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示： I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \"/kube-proxy\" I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier. I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable. I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53] I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53] I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443] I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master I1027 22:14:54.040048 5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCP I1027 22:14:54.040154 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDP I1027 22:14:54.040223 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而一个 iptables 模式的 Service 对应的规则，它们包括： KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和Service 端口一一对应； KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。 通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。 当然，还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。 其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示： $ for d in /sys/devices/virtual/net/cni0/brif/veth*/hairpin_mode; do echo \"$d = $(cat $d)\"; done /sys/devices/virtual/net/cni0/brif/veth4bfbfe74/hairpin_mode = 1 /sys/devices/virtual/net/cni0/brif/vethfc2a18c5/hairpin_mode = 1 而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示： $ ifconfig cni0 |grep PROMISC UP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1 10. 总结 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。 并且，你可以看到，Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则（Service 只应该看到和代理同一个租户下的 Pod）。再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns（kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry）。 参考： kubernetes service 创建外部负载均衡器 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 11:16:41 "},"对象/Kubernetes-NetworkPolicy.html":{"url":"对象/Kubernetes-NetworkPolicy.html","title":"NetworkPolicy","keywords":"","body":"kubernetes NetworkPolicy1. 简介2. 语法3. 选择器 to 和 from 的行为4. 默认策略4.1 默认拒绝所有入口流量4.2 默认允许所有入口流量4.3 默认拒绝所有出口流量4.4 默认允许所有出口流量4.5 默认拒绝所有入口和所有出口流量5. 实战示例5.1 创建一个nginx Deployment 并且通过服务将其暴露5.2 限制 nginx 服务的访问6. networkpolicy原理7. 总结kubernetes NetworkPolicy tagsstart NetworkPolicy 对象 tagsstop 《美丽心灵》是关于20世纪伟大数学家小约翰•福布斯-纳什的人物传记片。 1. 简介 网络策略（NetworkPolicy）是一种关于 Pod 间及与其他网络端点间所允许的通信规则的规范。 NetworkPolicy 资源使用 标签 选择 Pod，并定义选定 Pod 所允许的通信规则。 2. 语法 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 说明： spec: NetworkPolicy 规约 中包含了在一个命名空间中定义特定网络策略所需的所有信息。 podSelector: 每个 NetworkPolicy 都包括一个 podSelector ，它对该策略所应用的一组 Pod进行选择。示例中的策略选择带有 \"role=db\" 标签的 Pod。空的 podSelector 选择命名空间下的所有 Pod。 policyTypes: 每个 NetworkPolicy 都包含一个 policyTypes 列表，其中包含 Ingress 或 Egress 或两者兼具。policyTypes 字段表示给定的策略是否应用于进入所选 Pod 的入口流量或者来自所选 Pod的出口流量，或两者兼有。如果 NetworkPolicy 未指定 policyTypes 则默认情况下始终设置 Ingress，如果NetworkPolicy 有任何出口规则的话则设置 Egress。 ingress: 每个 NetworkPolicy 可包含一个 ingress 规则的白名单列表。每个规则都允许同时匹配 from 和ports 部分的流量。示例策略中包含一条简单的规则： 它匹配一个单一的端口，来自三个来源中的一个， 第一个通过 ipBlock指定，第二个通过namespaceSelector 指定，第三个通过 podSelector 指定。 egress: 每个 NetworkPolicy 可包含一个 egress 规则的白名单列表。每个规则都允许匹配 to 和 port部分的流量。该示例策略包含一条规则，该规则将单个端口上的流量匹配到 10.0.0.0/24 中的任何目的地。 所以，该网络策略示例: 隔离 \"default\" 命名空间下 \"role=db\" 的 Pod (如果它们不是已经被隔离的话)。 （Ingress 规则）允许以下 Pod 连接到 \"default\" 命名空间下的带有 “role=db” 标签的所有 Pod 的6379 TCP 端口： \"default\" 命名空间下任意带有 \"role=frontend\" 标签的 Pod 带有 \"project=myproject\" 标签的任意命名空间中的 Pod IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255（即，除了 172.17.1.0/24 之外的所有 172.17.0.0/16） （Egress 规则）允许从带有 \"role=db\" 标签的命名空间下的任何 Pod 到 CIDR 10.0.0.0/24 下 5978TCP 端口的连接。 3. 选择器 to 和 from 的行为 可以在 ingress from 部分或 egress to 部分中指定四种选择器： 第一种 podSelector: 这将在与 NetworkPolicy 相同的命名空间中选择特定的 Pod，应将其允许作为入口源或出口目的地。 第二种 namespaceSelector: 这将选择特定的命名空间，应将所有 Pod 用作其输入源或输出目的地。 第三种 namespaceSelector 和 podSelector: 一个指定 namespaceSelector 和 podSelector 的 to/from 条目选择特定命名空间中的特定 Pod。注意使用正确的 YAML 语法；这项策略： ... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 在 from 数组中仅包含一个元素，只允许来自标有 role=client 的 Pod 且该 Pod 所在的命名空间中标有 user=alice 的连接。但是 这项 策略： ... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 在 from 数组中包含两个元素，允许来自本地命名空间中标有 role=client 的 Pod 的连接，或 来自任何命名空间中标有 user = alice 的任何 Pod 的连接。 这两种定义方式的区别，请你一定要分清楚。 第四种： ipBlock: 这将选择特定的 IP CIDR 范围以用作入口源或出口目的地。 这些应该是群集外部 IP，因为 Pod IP 存在时间短暂的且随机产生。 群集的入口和出口机制通常需要重写数据包的源 IP 或目标 IP。在发生这种情况的情况下，不确定在 NetworkPolicy 处理之前还是之后发生，并且对于网络插件，云提供商，Service 实现等的不同组合，其行为可能会有所不同。 在进入的情况下，这意味着在某些情况下，您可以根据实际的原始源 IP 过滤传入的数据包，而在其他情况下，NetworkPolicy 所作用的 源IP 则可能是 LoadBalancer 或 Pod 的节点等。 对于出口，这意味着从 Pod 到被重写为集群外部 IP 的 Service IP 的连接可能会或可能不会受到基于 ipBlock 的策略的约束 4. 默认策略 默认情况下，如果命名空间中不存在任何策略，则所有进出该命名空间中的 Pod 的流量都被允许。以下示例使您可以更改该命名空间中的默认行为。 4.1 默认拒绝所有入口流量 您可以通过创建选择所有容器但不允许任何进入这些容器的入口流量的 NetworkPolicy 来为命名空间创建 \"default\" 隔离策略。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress 这样可以确保即使容器没有选择其他任何 NetworkPolicy，也仍然可以被隔离。此策略不会更改默认的出口隔离行为。 4.2 默认允许所有入口流量 如果要允许所有流量进入某个命名空间中的所有 Pod（即使添加了导致某些 Pod 被视为“隔离”的策略），则可以创建一个策略来明确允许该命名空间中的所有流量。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress 4.3 默认拒绝所有出口流量 您可以通过创建选择所有容器但不允许来自这些容器的任何出口流量的 NetworkPolicy 来为命名空间创建 \"default\" egress 隔离策略。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress spec: podSelector: {} policyTypes: - Egress 4.4 默认允许所有出口流量 如果要允许来自命名空间中所有 Pod 的所有流量（即使添加了导致某些 Pod 被视为“隔离”的策略），则可以创建一个策略，该策略明确允许该命名空间中的所有出口流量。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-egress spec: podSelector: {} egress: - {} policyTypes: - Egress 4.5 默认拒绝所有入口和所有出口流量 您可以为命名空间创建 \"default\" 策略，以通过在该命名空间中创建以下 NetworkPolicy 来阻止所有入站和出站流量。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all spec: podSelector: {} policyTypes: - Ingress - Egress 5. 实战示例 5.1 创建一个nginx Deployment 并且通过服务将其暴露 $ kubectl create deployment nginx --image=nginx $ kubectl expose deployment nginx --port=80 #此 Deployment 以名为 nginx 的 Service 暴露出来 $ kubectl get svc,pod $ kubectl run busybox --rm -ti --image=busybox /bin/sh # 创建测试工具的pod $ wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) remote file exists 5.2 限制 nginx 服务的访问 如果想限制对 nginx 服务的访问，只让那些拥有标签 access: true 的 Pod 访问它， 那么可以创建一个如下所示的 NetworkPolicy 对象： apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-nginx spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" 注意： NetworkPolicy 中包含选择策略所适用的 Pods 集合的 podSelector。 你可以看到上面的策略选择的是带有标签 app=nginx 的 Pods。 此标签是被自动添加到 nginx Deployment 中的 Pod 上的。 如果 podSelector 为空，则意味着选择的是名字空间中的所有 Pods。 $ kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml $ kubectl run busybox --rm -ti --image=busybox -- /bin/sh #创建测试工具的pod $ wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) wget: download timed out 定义访问标签后再次测试 创建一个拥有正确标签的 Pod，你将看到请求是被允许的： $ kubectl run busybox --rm -ti --labels=\"access=true\" --image=busybox -- /bin/sh 在 Shell 中运行命令： $ wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) remote file exists 6. networkpolicy原理 在具体实现上，凡是支持 NetworkPolicy 的 CNI 网络插件，都维护着一个 NetworkPolicy Controller，通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应，然后在宿主机上完成 iptables 规则的配置工作。 在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。所以说，如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话，你就需要再额外安装一个网络插件，比如 Calico 项目，来负责执行 NetworkPolicy。 安装 Flannel + Calico 的流程非常简单，你直接参考这个文档一键安装即可。 那么，这些网络插件，又是如何根据 NetworkPolicy 对 Pod 进行隔离的呢？ 接下来，我就以三层网络插件为例（比如 Calico 和 kube-router），来为你分析一下这部分的原理。为了方便讲解，这一次我编写了一个比较简单的 NetworkPolicy 对象，如下所示： apiVersion: extensions/v1beta1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db ingress: - from: - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: tcp port: 6379 可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Namespace 里的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。 而被隔离的对象，是所有携带了 role=db 标签的 Pod。 那么这个时候，Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义，在宿主机上生成 iptables 规则。这个过程，我可以通过如下所示的一段 Go 语言风格的伪代码来为你描述： for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址 for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址 for port, protocol := range ingress.ports { iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT } } } 可以看到，这是一条最基本的、通过匹配条件决定下一步动作的 iptables 规则。这条规则的名字是 KUBE-NWPLCY-CHAIN，含义是：当 IP 包的源地址是 srcIP、目的地址是 dstIP、协议是 protocol、目的端口是 port 的时候，就允许它通过（ACCEPT）。 而正如这段伪代码所示，匹配这条规则所需的这四个参数，都是从 NetworkPolicy 对象里读取出来的。 可以看到，Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 此外，在设置好上述“隔离”规则之后，网络插件还需要想办法，将所有对被隔离 Pod 的访问请求，都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。 在 CNI 网络插件中，上述需求可以通过设置两组 iptables 规则来实现。 第一组规则，负责“拦截”对被隔离 Pod 的访问请求。生成这一组规则的伪代码，如下所示： for pod := range 该Node上的所有Pod { if pod是networkpolicy.spec.podSelector选中的 { iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN ... } } 可以看到，这里的的 iptables 规则使用到了内置链：FORWARD。它是什么意思呢？说到这里，我就得为你稍微普及一下 iptables 的知识了。 实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。它们的关系，可以用如下的示意图来表示： 可以看到，这幅示意图中，IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。在 iptables 中，这些“检查点”被称为：链（Chain）。这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的。这些“检查点”的具体工作原理，可以用如下所示的示意图进行描述： 可以看到，当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。 而在经过路由之后，IP 包的去向就分为了两种： 第一种，继续在本机处理； 第二种，被转发到其他目的地。 我们先说一下 IP 包的第一种去向。这时候，IP 包将继续向上层协议栈流动。在它进入传输层之前，Netfilter 会设置一个名叫 INPUT 的“检查点”。到这里，IP 包流入路径（Input Path）结束。接下来，这个 IP 包通过传输层进入用户空间，交给用户进程处理。而处理完成后，用户进程会通过本机发出返回的 IP 包。这时候，这个 IP 包就进入了流出路径（Output Path）。 此时，IP 包首先还是会经过主机的路由表进行路由。路由结束后，Netfilter 就会设置一个名叫 OUTPUT 的“检查点”。然后，在 OUTPUT 之后，再设置一个名叫 POSTROUTING“检查点”。你可能会觉得奇怪，为什么在流出路径结束后，Netfilter 会连着设置两个“检查点”呢？ 这就要说到在流入路径里，路由判断后的第二种去向了。在这种情况下，这个 IP 包不会进入传输层，而是会继续在网络层流动，从而进入到转发路径（Forward Path）。在转发路径中，Netfilter 会设置一个名叫 FORWARD 的“检查点”。而在 FORWARD“检查点”完成后，IP 包就会来到流出路径。而转发的 IP 包由于目的地已经确定，它就不会再经过路由，也自然不会经过 OUTPUT，而是会直接来到 POSTROUTING“检查点”。所以说，POSTROUTING 的作用，其实就是上述两条路径，最终汇聚在一起的“最终检查点”。 需要注意的是，在有网桥参与的情况下，上述 Netfilter 设置“检查点”的流程，实际上也会出现在链路层（二层），并且会跟我在上面讲述的网络层（三层）的流程有交互。这些链路层的“检查点”对应的操作界面叫作 ebtables。所以，准确地说，数据包在 Linux Netfilter 子系统里完整的流动过程，其实应该如下所示: 以看到，我前面为你讲述的，正是上图中绿色部分，也就是网络层的 iptables 链的工作流程。另外，你应该还能看到，每一个白色的“检查点”上，还有一个绿色的“标签”，比如：raw、nat、filter 等等。在 iptables 里，这些标签叫作：表。比如，同样是 OUTPUT 这个“检查点”，filter Output 和 nat Output 在 iptables 里的语法和参数，就完全不一样，实现的功能也完全不同。所以说，iptables 表的作用，就是在某个具体的“检查点”（比如 Output）上，按顺序执行几个不同的检查动作（比如，先执行 nat，再执行 filter）. 在理解了 iptables 的工作原理之后，我们再回到 NetworkPolicy 上来。这时候，前面由网络插件设置的、负责“拦截”进入 Pod 的请求的三条 iptables 规则，就很容易读懂了： iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN ... 其中，第一条 FORWARD 链“拦截”的是一种特殊情况：它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包。其中，--physdev-is-bridged 的意思就是，这个 FORWARD 链匹配的是，通过本机上的网桥设备，发往目的地址是 podIP 的 IP 包。 当然，如果是像 Calico 这样的非网桥模式的 CNI 插件，就不存在这个情况了。kube-router 其实是一个简化版的 Calico，它也使用 BGP 来维护路由信息，但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互。 而第二条 FORWARD 链“拦截”的则是最普遍的情况，即：容器跨主通信。这时候，流入容器的数据包都是经过路由转发（FORWARD 检查点）来的。 不难看到，这些规则最后都跳转（即：-j）到了名叫 KUBE-POD-SPECIFIC-FW-CHAIN 的规则上。它正是网络插件为 NetworkPolicy 设置的第二组规则。 而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用，就是做出“允许”或者“拒绝”的判断。这部分功能的实现，可以简单描述为下面这样的 iptables 规则： iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable 可以看到，首先在第一条规则里，我们会把 IP 包转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配。按照我们之前的讲述，如果匹配成功，那么 IP 包就会被“允许通过”。而如果匹配失败，IP 包就会来到第二条规则上。可以看到，它是一条 REJECT 规则。通过这条规则，不满足 NetworkPolicy 定义的请求就会被拒绝掉，从而实现了对该容器的“隔离”。 以上，就是 CNI 网络插件实现 NetworkPolicy 的基本方法了。 7. 总结 Kubernetes 对 Pod 进行“隔离”的手段，即：NetworkPolicy。 而基于上述讲述，你就会发现这样一个事实： Kubernetes 的网络模型以及大多数容器网络实现，其实既不会保证容器之间二层网络的互通，也不会实现容器之间的二层网络隔离。这跟 IaaS 项目管理虚拟机的方式，是完全不同的。所以说，Kubernetes 从底层的设计和实现上，更倾向于假设你已经有了一套完整的物理基础设施。然后，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力。 并且，基于上述思路，Kubernetes 将来也不大可能把 Namespace 变成一个具有实质意义的隔离机制，或者把它映射成为“子网”或者“租户”。毕竟你可以看到，NetworkPolicy 对象的描述能力，要比基于 Namespace 的划分丰富得多。这也是为什么，到目前为止，Kubernetes 项目在云计算生态里的定位，其实是基础设施与 PaaS 之间的中间层。这是非常符合“容器”这个本质上就是进程的抽象粒度的。当然，随着 Kubernetes 社区以及 CNCF 生态的不断发展，Kubernetes 项目也已经开始逐步下探，“吃”掉了基础设施领域的很多“蛋糕”。这也正是容器生态继续发展的一个必然方向。 参考： Kubernetes NetworkPolicy Get started with Kubernetes network policy Network Policy Deep Dive into Network Policy Kubernetes Network Policies: A Practitioner's Guide Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 11:17:00 "},"对象/RBAC/":{"url":"对象/RBAC/","title":"RBAC","keywords":"","body":"kubernetes RBAC 入门1. 简介2. API 对象2.1 角色（Role）2.2 集群角色（ClusterRole）2.3 角色绑定（RoleBinding）和集群角色绑定（ClusterRoleBinding）3. 对资源的引用4. 聚合的 ClusterRole5. Role 示例6. 对主体的引用7. RoleBinding 示例8. 默认 Roles 和 Role Bindings9. 面向用户的角色10. 核心组件角色11. 其他组件角色12. 内置控制器的角色13. 对角色绑定创建或更新的限制14. 命令行工具14.1 kubectl create role14.2 kubectl create clusterrole14.3 kubectl create rolebinding14.4 kubectl create clusterrolebinding14.5 kubectl auth reconcile15. 服务账号权限16. 宽松的 RBAC 权限17. 示例容器网络接口（CNI）weaveworkkubernetes RBAC 入门 tagsstart 对象 tagsstop 不可以吃太胖哦，会被杀掉的！ ——《千与千寻》 1. 简介 RBAC（Role-Based Access Control，基于角色的访问控制）在Kubernetes的1.5版本中引入，在1.6版本时升级为Beta版本，在1.8版本时升级为GA。作为kubeadm安装方式的默认选项，足见其重要程度。相对于其他访问控制方式，新的RBAC具有如下优势。 ◎ 对集群中的资源和非资源权限均有完整的覆盖。 ◎ 整个RBAC完全由几个API对象完成，同其他API对象一样，可以用kubectl或API进行操作。 ◎ 可以在运行时进行调整，无须重新启动API Server。要使用RBAC授权模式，需要在API Server的启动参数中加上--authorization-mode=RBAC。 kube-apiserver --authorization-mode=Example,RBAC -- -- 下面对RBAC的原理和用法进行说明。 2. API 对象 RBAC API 声明了四种 Kubernetes 对象：Role、ClusterRole、RoleBinding 和 ClusterRoleBinding。你可以像使用其他 Kubernetes 对象一样， 通过类似 kubectl 这类工具 描述对象, 或修补对象。 2.1 角色（Role） 一个角色就是一组权限的集合，这里的权限都是许可形式的，不存在拒绝的规则。在一个命名空间中，可以用角色来定义一个角色，如果是集群级别的，就需要使用ClusterRole了。角色只能对命名空间内的资源进行授权，在下面例子中定义的角色具备读取Pod的权限： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] # \"\" 标明 core API 组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] rules中的参数说明如下。 ◎ apiGroups：支持的API组列表，例如“apiVersion:batch/v1”“apiVersion: extensions:v1beta1”“apiVersion: apps/v1beta1”等， 详细的API组说明参见第9章的说明。 ◎ resources：支持的资源对象列表，例如pods、deployments、 jobs等。 ◎ verbs：对资源对象的操作方法列表，例如get、watch、list、delete、replace、patch等，详细的操作方法说明参见第9章的说明。 2.2 集群角色（ClusterRole） 集群角色除了具有和角色一致的命名空间内资源的管理能力，因其集群级别的范围，还可以用于以下特殊元素的授权。 ◎ 集群范围的资源，例如Node。 ◎ 非资源型的路径，例如“/healthz”。 ◎ 包含全部命名空间的资源，例如pods（用于kubectl get pods -- all-namespaces这样的操作授权）。 下面的集群角色可以让用户有权访问任意一个或所有命名空间的secrets（视其绑定方式而定）： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # \"namespace\" 被忽略，因为 ClusterRoles 不受名字空间限制 name: secret-reader rules: - apiGroups: [\"\"] # 在 HTTP 层面，用来访问 Secret 对象的资源的名称为 \"secrets\" resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] 2.3 角色绑定（RoleBinding）和集群角色绑定（ClusterRoleBinding） 角色绑定或集群角色绑定用来把一个角色绑定到一个目标上，绑定目标可以是User（用户）、Group（组）或者Service Account。使用RoleBinding为某个命名空间授权，使用ClusterRoleBinding为集群范围内授权。 RoleBinding可以引用Role进行授权。下面的例子中的RoleBinding将在default命名空间中把pod-reader角色授予用户jane，这一操作可以让jane读取default命名空间中的Pod： apiVersion: rbac.authorization.k8s.io/v1 # 此角色绑定允许 \"jane\" 读取 \"default\" 名字空间中的 Pods kind: RoleBinding metadata: name: read-pods namespace: default subjects: # 你可以指定不止一个“subject（主体）” - kind: User name: jane # \"name\" 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" 指定与某 Role 或 ClusterRole 的绑定关系 kind: Role # 此字段必须是 Role 或 ClusterRole name: pod-reader # 此字段必须与你要绑定的 Role 或 ClusterRole 的名称匹配 apiGroup: rbac.authorization.k8s.io RoleBinding也可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。一种常见的做法是集群管理员为集群范围预先定义好一组ClusterRole，然后在多个命名空间中重复使用这些ClusterRole。例如，在下面的例子中，虽然secret-reader是一个集群角色，但是因为使用了RoleBinding，所以dave只能读取development命名空间中的secret： apiVersion: rbac.authorization.k8s.io/v1 # 此角色绑定使得用户 \"dave\" 能够读取 \"default\" 名字空间中的 Secrets # 你需要一个名为 \"secret-reader\" 的 ClusterRole kind: RoleBinding metadata: name: read-secrets # RoleBinding 的名字空间决定了访问权限的授予范围。 # 这里仅授权在 \"development\" 名字空间内的访问权限。 namespace: development subjects: - kind: User name: dave # 'name' 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 集群角色绑定中的角色只能是集群角色，用于进行集群级别或者对所有命名空间都生效的授权。下面的例子允许manager组的用户读取任意Namespace中的secret： apiVersion: rbac.authorization.k8s.io/v1 # 此集群角色绑定允许 “manager” 组中的任何人访问任何名字空间中的 secrets kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: Group name: manager # 'name' 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 创建了绑定之后，你不能再修改绑定对象所引用的 Role 或 ClusterRole。 试图改变绑定对象的 roleRef 将导致合法性检查错误。 如果你想要改变现有绑定对象中 roleRef 字段的内容，必须删除重新创建绑定对象。 这种限制有两个主要原因： 针对不同角色的绑定是完全不一样的绑定。要求通过删除/重建绑定来更改 roleRef, 这样可以确保要赋予绑定的所有主体会被授予新的角色（而不是在允许修改 roleRef的情况下导致所有现有主体胃镜验证即被授予新角色对应的权限）。 将 roleRef 设置为不可以改变，这使得可以为用户授予对现有绑定对象的 update 权限，这样可以让他们管理主体列表，同时不能更改被授予这些主体的角色。 命令 kubectl auth reconcile 可以创建或者更新包含 RBAC 对象的清单文件， 并且在必要的情况下删除和重新创建绑定对象，以改变所引用的角色。 更多相关信息请参照命令用法和示例 3. 对资源的引用 多数资源可以用其名称的字符串来表达，也就是Endpoint中的URL相对路径，例如pods。然而，某些Kubernetes API包含下级资源，例如Pod的日志（logs）。Pod日志的Endpoint是GET/api/v1/namespaces/{namespace}/pods/{name}/log。 在这个例子中，Pod是一个命名空间内的资源，log就是一个下级资源。要在一个RBAC角色中体现，就需要用斜线“/”来分隔资源和下级资源。若想授权让某个主体同时能够读取Pod和Pod log，则可以配置resources为一个数组： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-and-pod-logs-reader rules: - apiGroups: [\"\"] resources: [\"pods\", \"pods/log\"] verbs: [\"get\", \"list\"] 对于某些请求，也可以通过 resourceNames 列表按名称引用资源。 在指定时，可以将请求限定为资源的单个实例。 下面的例子中限制可以 \"get\" 和 \"update\" 一个名为 my-configmap 的 ConfigMap： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: configmap-updater rules: - apiGroups: [\"\"] # 在 HTTP 层面，用来访问 ConfigMap 的资源的名称为 \"configmaps\" resources: [\"configmaps\"] resourceNames: [\"my-configmap\"] verbs: [\"update\", \"get\"] 可想而知，resourceName这种用法对list、watch、create或deletecollection操作是无效的，这是因为必须要通过URL进行鉴权，而资源名称在list、watch、create deletecollection请求中只是请求Body数据的一部分。 4. 聚合的 ClusterRole 你可以将若干 ClusterRole 聚合（Aggregate） 起来，形成一个复合的 ClusterRole。 某个控制器作为集群控制面的一部分会监视带有 aggregationRule 的 ClusterRole 对象集合。aggregationRule 为控制器定义一个标签 选择算符供后者匹配 应该组合到当前 ClusterRole 的 roles 字段中的 ClusterRole 对象。 下面是一个聚合 ClusterRole 的示例： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring aggregationRule: clusterRoleSelectors: - matchLabels: rbac.example.com/aggregate-to-monitoring: \"true\" rules: [] # 控制面自动填充这里的规则 如果你创建一个与某现有聚合 ClusterRole 的标签选择算符匹配的 ClusterRole， 这一变化会触发新的规则被添加到聚合 ClusterRole 的操作。 下面的例子中，通过创建一个标签同样为 rbac.example.com/aggregate-to-monitoring: true 的 ClusterRole，新的规则可被添加到 \"monitoring\" ClusterRole 中。 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-endpoints labels: rbac.example.com/aggregate-to-monitoring: \"true\" # 当你创建 \"monitoring-endpoints\" ClusterRole 时， # 下面的规则会被添加到 \"monitoring\" ClusterRole 中 rules: - apiGroups: [\"\"] resources: [\"services\", \"endpoints\", \"pods\"] verbs: [\"get\", \"list\", \"watch\"] 默认的面向用户的角色 使用 ClusterRole 聚合。 这使得作为集群管理员的你可以为扩展默认规则，包括为定制资源设置规则， 比如通过 CustomResourceDefinitions 或聚合 API 服务器提供的定制资源。 例如，下面的 ClusterRoles 让默认角色 \"admin\" 和 \"edit\" 拥有管理自定义资源 \"CronTabs\" 的权限， \"view\" 角色对 CronTab 资源拥有读操作权限。 你可以假定 CronTab 对象在 API 服务器所看到的 URL 中被命名为 \"crontabs\"。 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: aggregate-cron-tabs-edit labels: # 添加以下权限到默认角色 \"admin\" 和 \"edit\" 中 rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rules: - apiGroups: [\"stable.example.com\"] resources: [\"crontabs\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-view labels: # 添加以下权限到 \"view\" 默认角色中 rbac.authorization.k8s.io/aggregate-to-view: \"true\" rules: - apiGroups: [\"stable.example.com\"] resources: [\"crontabs\"] verbs: [\"get\", \"list\", \"watch\"] 5. Role 示例 以下示例均为从 Role 或 CLusterRole 对象中截取出来，我们仅展示其 rules 部分。 允许读取在核心 API 组下的 \"Pods\"： rules: - apiGroups: [\"\"] # 在 HTTP 层面，用来访问 Pod 的资源的名称为 \"pods\" resources: [\"pods\"] verbs: [\"get\", \"list\", \"watch\"] 允许读/写在 \"extensions\" 和 \"apps\" API 组中的 Deployment（在 HTTP 层面，对应 URL 中资源部分为 \"deployments\"）： rules: - apiGroups: [\"extensions\", \"apps\"] resources: [\"deployments\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 允许读取核心 API 组中的 \"pods\" 和读/写 \"batch\" 或 \"extensions\" API 组中的 \"jobs\"： rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"batch\", \"extensions\"] resources: [\"jobs\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 允许读取名称为 \"my-config\" 的 ConfigMap（需要通过 RoleBinding 绑定以 限制为某名字空间中特定的 ConfigMap）： rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] 允许读取在核心组中的 \"nodes\" 资源（因为 Node 是集群作用域的，所以需要 ClusterRole 绑定到 ClusterRoleBinding 才生效）： rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] 允许针对非资源端点 /healthz 和其子路径上发起 GET 和 POST 请求 （必须在 ClusterRole 绑定 ClusterRoleBinding 才生效）： rules: - nonResourceURLs: [\"/healthz\", \"/healthz/*\"] # nonResourceURL 中的 '*' 是一个全局通配符 verbs: [\"get\", \"post\"] 6. 对主体的引用 RoleBinding 或者 ClusterRoleBinding 可绑定角色到某 主体（Subject）上。 主体可以是组，用户或者 服务账号。 Kubernetes 用字符串来表示用户名。 用户名可以是普通的用户名，像 \"alice\"；或者是邮件风格的名称，如 \"bob@example.com\"， 或者是以字符串形式表达的数字 ID。 你作为 Kubernetes 管理员负责配置 身份认证模块 以便后者能够生成你所期望的格式的用户名。 注意： 前缀 system: 是 Kubernetes 系统保留的，所以你要确保 所配置的用户名或者组名不能出现上述 system: 前缀。 除了对前缀的限制之外，RBAC 鉴权系统不对用户名格式作任何要求。 7. RoleBinding 示例 下面示例是 RoleBinding 中的片段，仅展示其 subjects 的部分。 对于名称为 alice@example.com 的用户： subjects: - kind: User name: \"alice@example.com\" apiGroup: rbac.authorization.k8s.io 对于名称为 frontend-admins 的用户组： subjects: - kind: Group name: \"frontend-admins\" apiGroup: rbac.authorization.k8s.io 对于 kube-system 名字空间中的默认服务账号： subjects: - kind: ServiceAccount name: default namespace: kube-system 对于 \"qa\" 名字空间中所有的服务账号： subjects: - kind: Group name: system:serviceaccounts:qa apiGroup: rbac.authorization.k8s.io 对于在任何名字空间中的服务账号： subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 对于所有已经过认证的用户： subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io 对于所有未通过认证的用户： subjects: - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 对于所有用户： subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 8. 默认 Roles 和 Role Bindings API 服务器创建一组默认的 ClusterRole 和 ClusterRoleBinding 对象。 这其中许多是以 system: 为前缀的，用以标识对应资源是直接由集群控制面管理的。 所有的默认 ClusterRole 和 ClusterRoleBinding 都有 kubernetes.io/bootstrapping=rbac-defaults 标签。 注意： 在修改名称包含 system: 前缀的 ClusterRole 和 ClusterRoleBinding 时要格外小心。 对这些资源的更改可能导致集群无法继续工作。 默认 ClusterRole 默认 ClusterRoleBinding 描述 system:basic-user system:authenticated 组 允许用户以只读的方式去访问他们自己的基本信息。在 1.14 版本之前，这个角色在 默认情况下也绑定在 system:unauthenticated 上。 system:discovery system:authenticated 组 允许以只读方式访问 API 发现端点，这些端点用来发现和协商 API 级别。 在 1.14 版本之前，这个角色在默认情况下绑定在 system:unauthenticated 上。 system:public-info-viewer system:authenticated 和 system:unauthenticated 组 允许对集群的非敏感信息进行只读访问，它是在 1.14 版本中引入的。 9. 面向用户的角色 一些默认的 ClusterRole 不是以前缀 system: 开头的。这些是面向用户的角色。 它们包括超级用户（Super-User）角色（cluster-admin）、 使用 ClusterRoleBinding 在集群范围内完成授权的角色（cluster-status）、 以及使用 RoleBinding 在特定名字空间中授予的角色（admin、edit、view）。 面向用户的 ClusterRole 使用 ClusterRole 聚合以允许管理员在 这些 ClusterRole 上添加用于定制资源的规则。如果想要添加规则到 admin、edit 或者 view， 可以创建带有以下一个或多个标签的 ClusterRole： metadata: labels: rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-view: \"true\" 10. 核心组件角色 默认 ClusterRole 默认 ClusterRoleBinding 描述 cluster-admin system:masters 组 允许超级用户在平台上的任何资源上执行所有操作。 当在 ClusterRoleBinding 中使用时，可以授权对集群中以及所有名字空间中的全部资源进行完全控制。 当在 RoleBinding 中使用时，可以授权控制 RoleBinding 所在名字空间中的所有资源，包括名字空间本身。 admin 无 允许管理员访问权限，旨在使用 RoleBinding 在名字空间内执行授权。 如果在 RoleBinding 中使用，则可授予对名字空间中的大多数资源的读/写权限， 包括创建角色和角色绑定的能力。 但是它不允许对资源配额或者名字空间本身进行写操作。 edit 无 允许对名字空间的大多数对象进行读/写操作。 它不允许查看或者修改角色或者角色绑定。 不过，此角色可以访问 Secret，以名字空间中任何 ServiceAccount 的身份运行 Pods， 所以可以用来了解名字空间内所有服务账号的 API 访问级别。 view 无 system:kube-scheduler system:kube-scheduler user 允许访问 scheduler 组件所需要的资源。 system:volume-scheduler system:kube-scheduler user 允许访问 kube-scheduler 组件所需要的卷资源。 system:kube-controller-manager system:kube-controller-manager user 允许访问控制器管理器 组件所需要的资源。 各个控制回路所需要的权限在控制器角色 详述。 system:node 无 允许访问 kubelet 所需要的资源，包括对所有 Secret 的读操作和对所有 Pod 状态对象的写操作。 你应该使用 Node 鉴权组件 和 NodeRestriction 准入插件 而不是 system:node 角色。同时基于 kubelet 上调度执行的 Pod 来授权 kubelet 对 API 的访问。 system:node 角色的意义仅是为了与从 v1.8 之前版本升级而来的集群兼容。 system:node-proxier system:kube-proxy user 允许访问 kube-proxy 组件所需要的资源 11. 其他组件角色 默认 ClusterRole 默认 ClusterRoleBinding 描述 system:auth-delegator 无 允许将身份认证和鉴权检查操作外包出去。 这种角色通常用在插件式 API 服务器上，以实现统一的身份认证和鉴权。 system:heapster 无 为 Heapster 组件（已弃用）定义的角色。 system:kube-aggregator 无 为 kube-aggregator 组件定义的角色。 system:kube-dns 在 kube-system 名字空间中的 kube-dns 服务账号 为 kube-dns 组件定义的角色。 system:kubelet-api-admin 无 允许 kubelet API 的完全访问权限。 system:node-bootstrapper 无 允许访问执行 kubelet TLS 启动引导 所需要的资源。 system:node-problem-detector 无 为 node-problem-detector 组件定义的角色。 system:persistent-volume-provisioner 无 允许访问大部分 动态卷驱动 所需要的资源 12. 内置控制器的角色 Kubernetes 控制器管理器 运行内建于 Kubernetes 控制面的控制器。 当使用 --use-service-account-credentials 参数启动时, kube-controller-manager 使用单独的服务账号来启动每个控制器。 每个内置控制器都有相应的、前缀为 system:controller: 的角色。 如果控制管理器启动时未设置 --use-service-account-credentials， 它使用自己的身份凭据来运行所有的控制器，该身份必须被授予所有相关的角色。 这些角色包括: system:controller:attachdetach-controller system:controller:certificate-controller system:controller:clusterrole-aggregation-controller system:controller:cronjob-controller system:controller:daemon-set-controller system:controller:deployment-controller system:controller:disruption-controller system:controller:endpoint-controller system:controller:expand-controller system:controller:generic-garbage-collector system:controller:horizontal-pod-autoscaler system:controller:job-controller system:controller:namespace-controller system:controller:node-controller system:controller:persistent-volume-binder system:controller:pod-garbage-collector system:controller:pv-protection-controller system:controller:pvc-protection-controller system:controller:replicaset-controller system:controller:replication-controller system:controller:resourcequota-controller system:controller:root-ca-cert-publisher system:controller:route-controller system:controller:service-account-controller system:controller:service-controller system:controller:statefulset-controller system:controller:ttl-controller 13. 对角色绑定创建或更新的限制 例如，下面的 ClusterRole 和 RoleBinding 将允许用户 user-1 把名字空间 user-1-namespace 中的 admin、edit 和 view 角色赋予其他用户： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: role-grantor rules: - apiGroups: [\"rbac.authorization.k8s.io\"] resources: [\"rolebindings\"] verbs: [\"create\"] - apiGroups: [\"rbac.authorization.k8s.io\"] resources: [\"clusterroles\"] verbs: [\"bind\"] # 忽略 resourceNames 意味着允许绑定任何 ClusterRole resourceNames: [\"admin\",\"edit\",\"view\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: role-grantor-binding namespace: user-1-namespace roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: role-grantor subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: user-1 当启动引导第一个角色和角色绑定时，需要为初始用户授予他们尚未拥有的权限。 对初始角色和角色绑定进行初始化时需要： 使用用户组为 system:masters 的凭据，该用户组由默认绑定关联到 cluster-admin 这个超级用户角色。 如果你的 API 服务器启动时启用了不安全端口（使用 --insecure-port）, 你也可以通过 该端口调用 API ，这样的操作会绕过身份验证或鉴权 14. 命令行工具 14.1 kubectl create role 创建 Role 对象，定义在某一名字空间中的权限。例如: 创建名称为 \"pod-reader\" 的 Role 对象，允许用户对 Pods 执行 get、watch 和 list 操作： kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods 创建名称为 \"pod-reader\" 的 Role 对象并指定 resourceNames： kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod 创建名为 \"foo\" 的 Role 对象并指定 apiGroups： kubectl create role foo --verb=get,list,watch --resource=replicasets.apps 创建名为 \"foo\" 的 Role 对象并指定子资源权限: kubectl create role foo --verb=get,list,watch --resource=pods,pods/status 创建名为 \"my-component-lease-holder\" 的 Role 对象，使其具有对特定名称的 资源执行 get/update 的权限： kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component 14.2 kubectl create clusterrole 创建 ClusterRole 对象。例如： 创建名称为 \"pod-reader\" 的 ClusterRole对象，允许用户对 Pods 对象执行 get、watch和list` 操作： kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods 创建名为 \"pod-reader\" 的 ClusterRole 对象并指定 resourceNames： kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod 创建名为 \"foo\" 的 ClusterRole 对象并指定 apiGroups： kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps 创建名为 \"foo\" 的 ClusterRole 对象并指定子资源: kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status 创建名为 \"foo\" 的 ClusterRole 对象并指定 nonResourceURL： kubectl create clusterrole \"foo\" --verb=get --non-resource-url=/logs/* 创建名为 \"monitoring\" 的 ClusterRole 对象并指定 aggregationRule： kubectl create clusterrole monitoring --aggregation-rule=\"rbac.example.com/aggregate-to-monitoring=true\" 14.3 kubectl create rolebinding 在特定的名字空间中对 Role 或 ClusterRole 授权。例如： 在名字空间 \"acme\" 中，将名为 admin 的 ClusterRole 中的权限授予名称 \"bob\" 的用户: kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme 在名字空间 \"acme\" 中，将名为 view 的 ClusterRole 中的权限授予名字空间 \"acme\" 中名为 myapp 的服务账号： kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme 在名字空间 \"acme\" 中，将名为 view 的 ClusterRole 对象中的权限授予名字空间 \"myappnamespace\" 中名称为 myapp 的服务账号： kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme 14.4 kubectl create clusterrolebinding 在整个集群（所有名字空间）中用 ClusterRole 授权。例如： 在整个集群范围，将名为 cluster-admin 的 ClusterRole 中定义的权限授予名为 \"root\" 用户： kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root 在整个集群范围内，将名为 system:node-proxier 的 ClusterRole 的权限授予名为 \"system:kube-proxy\" 的用户： kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy 在整个集群范围内，将名为 view 的 ClusterRole 中定义的权限授予 \"acme\" 名字空间中 名为 \"myapp\" 的服务账号： kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp 14.5 kubectl auth reconcile 使用清单文件来创建或者更新 rbac.authorization.k8s.io/v1 API 对象。 尚不存在的对象会被创建，如果对应的名字空间也不存在，必要的话也会被创建。 已经存在的角色会被更新，使之包含输入对象中所给的权限。如果指定了 --remove-extra-permissions，可以删除额外的权限。 已经存在的绑定也会被更新，使之包含输入对象中所给的主体。如果指定了 --remove-extra-permissions，则可以删除多余的主体。 例如: 测试应用 RBAC 对象的清单文件，显示将要进行的更改： kubectl auth reconcile -f my-rbac-rules.yaml --dry-run 应用 RBAC 对象的清单文件，保留角色中的额外权限和绑定中的其他主体： kubectl auth reconcile -f my-rbac-rules.yaml 应用 RBAC 对象的清单文件, 删除角色中的额外权限和绑定中的其他主体： kubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions 15. 服务账号权限 为特定应用的服务账户授予角色（最佳实践） 这要求应用在其 Pod 规约中指定 serviceAccountName， 并额外创建服务账号（包括通过 API、应用程序清单、kubectl create serviceaccount 等）。 例如，在名字空间 \"my-namespace\" 中授予服务账号 \"my-sa\" 只读权限： kubectl create rolebinding my-sa-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:my-sa \\ --namespace=my-namespace 将角色授予某名字空间中的 \"default\" 服务账号 如果某应用没有指定 serviceAccountName，那么它将使用 \"default\" 服务账号。 说明： \"default\" 服务账号所具有的权限会被授予给名字空间中所有未指定 serviceAccountName 的 Pod。 例如，在名字空间 \"my-namespace\" 中授予服务账号 \"default\" 只读权限： kubectl create rolebinding default-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:default \\ --namespace=my-namespace 许多插件组件 在 kube-system 名字空间以 \"default\" 服务账号运行。 要允许这些插件组件以超级用户权限运行，需要将集群的 cluster-admin 权限授予 kube-system 名字空间中的 \"default\" 服务账号。 说明： 启用这一配置意味着在 kube-system 名字空间中包含以超级用户账号来访问 API 的 Secrets。 kubectl create clusterrolebinding add-on-cluster-admin \\ --clusterrole=cluster-admin \\ --serviceaccount=kube-system:default 将角色授予名字空间中所有服务账号 如果你想要名字空间中所有应用都具有某角色，无论它们使用的什么服务账号， 可以将角色授予该名字空间的服务账号组。 例如，在名字空间 \"my-namespace\" 中的只读权限授予该名字空间中的所有服务账号： kubectl create rolebinding serviceaccounts-view \\ --clusterrole=view \\ --group=system:serviceaccounts:my-namespace \\ --namespace=my-namespace 在集群范围内为所有服务账户授予一个受限角色（不鼓励） 如果你不想管理每一个名字空间的权限，你可以向所有的服务账号授予集群范围的角色。 例如，为集群范围的所有服务账号授予跨所有名字空间的只读权限： kubectl create clusterrolebinding serviceaccounts-view \\ --clusterrole=view \\ --group=system:serviceaccounts 授予超级用户访问权限给集群范围内的所有服务帐户（强烈不鼓励） 如果你不关心如何区分权限，你可以将超级用户访问权限授予所有服务账号。 警告： 这样做会允许所有应用都对你的集群拥有完全的访问权限，并将允许所有能够读取 Secret（或创建 Pod）的用户对你的集群有完全的访问权限。 kubectl create clusterrolebinding serviceaccounts-cluster-admin \\ --clusterrole=cluster-admin \\ --group=system:serviceaccounts 16. 宽松的 RBAC 权限 下面的策略允许 所有 服务帐户充当集群管理员。 容器中运行的所有应用程序都会自动收到服务帐户的凭据，可以对 API 执行任何操作， 包括查看 Secrets 和修改权限。这一策略是不被推荐的。 kubectl create clusterrolebinding permissive-binding \\ --clusterrole=cluster-admin \\ --user=admin \\ --user=kubelet \\ --group=system:serviceaccounts 17. 示例 容器网络接口（CNI）weavework controlplane $ cat /opt/weave-kube.yaml apiVersion: v1 kind: List items: - apiVersion: v1 kind: ServiceAccount metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net namespace: kube-system - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net rules: - apiGroups: - '' resources: - pods - namespaces - nodes verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - networkpolicies verbs: - get - list - watch - apiGroups: - '' resources: - nodes/status verbs: - patch - update - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net roleRef: kind: ClusterRole name: weave-net apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: weave-net namespace: kube-system - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net namespace: kube-system rules: - apiGroups: - '' resourceNames: - weave-net resources: - configmaps verbs: - get - update - apiGroups: - '' resources: - configmaps verbs: - create - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net namespace: kube-system roleRef: kind: Role name: weave-net apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: weave-net namespace: kube-system - apiVersion: apps/v1 kind: DaemonSet metadata: name: weave-net annotations: cloud.weave.works/launcher-info: |- { \"original-request\": { \"url\": \"/k8s/v1.10/net.yaml?k8s-version=v1.16.0\", \"date\": \"Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\" }, \"email-address\": \"support@weave.works\" } labels: name: weave-net namespace: kube-system spec: minReadySeconds: 5 selector: matchLabels: name: weave-net template: metadata: labels: name: weave-net spec: containers: - name: weave command: - /home/weave/launch.sh env: - name: IPALLOC_RANGE value: 10.32.0.0/24 - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: 'docker.io/weaveworks/weave-kube:2.6.0' readinessProbe: httpGet: host: 127.0.0.1 path: /status port: 6784 resources: requests: cpu: 10m securityContext: privileged: true volumeMounts: - name: weavedb mountPath: /weavedb - name: cni-bin mountPath: /host/opt - name: cni-bin2 mountPath: /host/home - name: cni-conf mountPath: /host/etc - name: dbus mountPath: /host/var/lib/dbus - name: lib-modules mountPath: /lib/modules - name: xtables-lock mountPath: /run/xtables.lock - name: weave-npc env: - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: 'docker.io/weaveworks/weave-npc:2.6.0' resources: requests: cpu: 10m securityContext: privileged: true volumeMounts: - name: xtables-lock mountPath: /run/xtables.lock hostNetwork: true hostPID: true restartPolicy: Always securityContext: seLinuxOptions: {} serviceAccountName: weave-net tolerations: - effect: NoSchedule operator: Exists volumes: - name: weavedb hostPath: path: /var/lib/weave - name: cni-bin hostPath: path: /opt - name: cni-bin2 hostPath: path: /home - name: cni-conf hostPath: path: /etc - name: dbus hostPath: path: /var/lib/dbus - name: lib-modules hostPath: path: /lib/modules - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate updateStrategy: type: RollingUpdate 参考： kubernetes Using RBAC Authorization Extend Kubernetes with Custom Resource Definitions and RBAC for ServiceAccounts Demystifying RBAC in Kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 08:31:53 "},"对象/RBAC/kubernetes-RBAC-principle.html":{"url":"对象/RBAC/kubernetes-RBAC-principle.html","title":"原理","keywords":"","body":"kubernetes RBAC 原理1. Role2. RoleBinding3. ClusterRole 和ClusterRoleBinding4. 细化RBAC与运用过程kubernetes RBAC 原理 tagsstart 对象 tagsstop 我只能送你到这里了，剩下的路你要自己走，不要回头。——《千与千寻》 上一篇是：kubernetes RBAC 入门 在 Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC，基于角色的访问控制（Role-Based Access Control） RBAC 体系的核心概念： Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。 Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。 RoleBinding：定义了“被作用者”和“角色”的绑定关系。 1. Role kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] role 对象指定了它能产生作用的 Namepace 是：mynamespace Role 对象的 rules 字段，就是它所定义的权限规则，这条规则的含义就是：允许“被作用者”，对 mynamespace 下面的 Pod 对象，进行 GET、WATCH 和 LIST 操作。 这个具体的“被作用者”又是如何指定的呢？这就需要通过 RoleBinding 来实现了。 2. RoleBinding kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。 可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。这个 User 到底是从哪里来的呢？ 实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户” roleRef 这个字段，RoleBinding 对象就可以直接通过名字，来引用我们前面定义的 Role 对象（example-role），从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系。 Role 和 RoleBinding 对象都是 Namespaced 对象（Namespaced Object），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。 那么，对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？ 3. ClusterRole 和ClusterRoleBinding kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io ClusterRole 和 ClusterRoleBinding 的组合，意味着名叫 example-user 的用户，拥有对所有 Namespace 里的 Pod 进行 GET、WATCH 和 LIST 操作的权限。 更进一步地，在 Role 或者 ClusterRole 里面，如果要赋予用户 example-user 所有权限，那你就可以给它指定一个 verbs 字段的全集，如下所示： verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 4. 细化RBAC与运用过程 rules 字段也可以进一步细化，比如，你可以只针对某一个具体的对象进行权限设置，如下所示： rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] 这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。而正如我前面介绍过的，在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。 这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。 接下来，我通过一个具体的实例来为你讲解一下为 ServiceAccount 分配权限的过程。首先，我们要定义一个 ServiceAccount。它的 API 对象非常简单，如下所示： apiVersion: v1 kind: ServiceAccount metadata: namespace: mynamespace name: example-sa 个最简单的 ServiceAccount 对象只需要 Name 和 Namespace 这两个最基本的字段。然后，我们通过编写 RoleBinding 的 YAML 文件，来为这个 ServiceAccount 分配权限： kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: ServiceAccount name: example-sa namespace: mynamespace roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 可以看到，在这个 RoleBinding 对象里，subjects 字段的类型（kind），不再是一个 User，而是一个名叫 example-sa 的 ServiceAccount。而 roleRef 引用的 Role 对象，依然名叫 example-role 接着，我们用 kubectl 命令创建这三个对象： $ kubectl create -f svc-account.yaml $ kubectl create -f role-binding.yaml $ kubectl create -f role.yaml $ kubectl get sa -n mynamespace -o yaml - apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-09-08T12:59:17Z name: example-sa namespace: mynamespace resourceVersion: \"409327\" ... secrets: - name: example-sa-token-vmfg6 可以看到，Kubernetes 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，即：上述 ServiceAcount 定义里最下面的 secrets 字段。这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，我们一般称它为：Token。Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式保存在 Etcd 当中。这时候，用户的 Pod，就可以声明使用这个 ServiceAccount 了，比如下面这个例子： apiVersion: v1 kind: Pod metadata: namespace: mynamespace name: sa-token-test spec: containers: - name: nginx image: nginx:1.7.9 serviceAccountName: example-sa 这个 Pod 运行起来之后，我们就可以看到，该 ServiceAccount 的 token，也就是一个 Secret 对象，被 Kubernetes 自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下，如下所示： 容器里的应用，就可以使用这个 ca.crt 来访问 APIServer 了。更重要的是，此时它只能够做 GET、WATCH 和 LIST 操作。因为 example-sa 这个 ServiceAccount 的权限，已经被我们绑定了 Role 做了限制。 如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。但在这种情况下，这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。当然，这个访问所需要的 Token，还是默认 ServiceAccount 对应的 Secret 对象为它提供的。 $kubectl describe sa default Name: default Namespace: default Labels: Annotations: Image pull secrets: Mountable secrets: default-token-s8rbq Tokens: default-token-s8rbq Events: $ kubectl get secret NAME TYPE DATA AGE kubernetes.io/service-account-token 3 82d $ kubectl describe secret default-token-s8rbq Name: default-token-s8rbq Namespace: default Labels: Annotations: kubernetes.io/service-account.name=default kubernetes.io/service-account.uid=ffcb12b2-917f-11e8-abde-42010aa80002 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 7 bytes token: 以看到，Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 Secret：它的类型是kubernetes.io/service-account-token；它的 Annotation 字段，声明了kubernetes.io/service-account.name=default，即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定。 除了前面使用的“用户”（User），Kubernetes 还拥有“用户组”（Group）的概念，也就是一组“用户”的意思。如果你为 Kubernetes 配置了外部认证服务的话，这个“用户组”的概念就会由外部认证服务提供。而对于 Kubernetes 的内置“用户”ServiceAccount 来说，上述“用户组”的概念也同样适用。实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是： system:serviceaccount:: 而它对应的内置“用户组”的名字，就是： system:serviceaccounts: 这两个对应关系，请你一定要牢记。比如，现在我们可以在 RoleBinding 里定义如下的 subjects： subjects: - kind: Group name: system:serviceaccounts:mynamespace apiGroup: rbac.authorization.k8s.io 这就意味着这个 Role 的权限规则，作用于 mynamespace 里的所有 ServiceAccount。这就用到了“用户组”的概念。而下面这个例子： subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount。最后，值得一提的是，在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。一般来说，这些系统 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。比如，其中一个名叫 system:kube-scheduler 的 ClusterRole，定义的权限规则是 kube-scheduler（Kubernetes 的调度器组件）运行所需要的必要权限。你可以通过如下指令查看这些权限的列表： $ kubectl describe clusterrole system:kube-scheduler Name: system:kube-scheduler ... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- ... services [] [] [get list watch] replicasets.apps [] [] [get list watch] statefulsets.apps [] [] [get list watch] replicasets.extensions [] [] [get list watch] poddisruptionbudgets.policy [] [] [get list watch] pods/status [] [] [patch update] 这个 system:kube-scheduler 的 ClusterRole，就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount，它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount。 除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用： cluster-admin； admin； edit； view。 通过它们的名字，你应该能大致猜出它们都定义了哪些权限。比如，这个名叫 view 的 ClusterRole，就规定了被作用者只有 Kubernetes API 的只读权限。而我还要提醒你的是，上面这个 cluster-admin 角色，对应的是整个 Kubernetes 项目中的最高权限（verbs=*），如下所示： $ kubectl describe clusterrole cluster-admin -n kube-system Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate=true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.* [] [] [*] [*] [] [*] 下一篇：kubernetes RBAC 实践 参考： Kubernetes Using RBAC Authorization Extend Kubernetes with Custom Resource Definitions and RBAC for ServiceAccounts Demystifying RBAC in Kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 08:45:34 "},"对象/Auditing/":{"url":"对象/Auditing/","title":"Auditing","keywords":"","body":"kuberentes Auditing 入门1. 前言2. Auditing 阶段3. Auditing级别4. Auditing 技巧5. Auditing 后端5.1 Log 后端5.2 Webhook 后端6. 事件批处理7. 参数调优7.1 日志条目截断8. Auditing 日志格式示例8.1 user 信息和 sourceIP8.2 有关已启动和执行的请求的信息8.3 已执行操作的响应状态9. Auditing policy 场景kuberentes Auditing 入门 tagsstart Auditing 对象 tagsstop 1. 前言 Kubernetes是一种容器编排工具，可降低部署和管理容器化应用程序的复杂性。它正迅速成为部署和管理大型应用程序和微服务的行业标准，并被各种组织广泛用于管理他们在云端和本地的应用程序。 作为Kubernetes 管理员，您必须记录集群中发生的事件。这些记录将作为调试问题和提高集群安全性的真实来源。Kubernetes Auditing记录在您的集群中执行的操作（或某人试图执行的操作）。 在本文中，您将了解 Kubernetes Auditing 是什么，它们为何重要，以及如何在 Kubernetes 集群中启用 Auditing日志。 日志自然是 Auditing日志的核心；在整个 Kubernetes 运行时，系统设置为跟踪性能指标、元数据和任何被认为重要的管理活动。从捕获点开始，这些日志被“流式传输”到它们的存储目的地，然后可用于回顾性分析。每个都带有时间戳以添加上下文。 在 Auditing日志记录过程中，有权访问其历史日志记录的人开始寻找任何突出的信息或任何外在表明可疑活动的信息。这些可能包括意外登录或系统崩溃。 Auditing日志是一组记录，其中包含对Kubernetes API发出的所有请求的按时间顺序排列的列表。Kubernetes 存储每个用户以及控制平面生成的操作。这些日志采用 JSON 格式，包含 HTTP 方法、发起请求的用户信息、发起的请求、处理请求的Kubernetes 组件等信息。包括： The control plane (built-in controllers, the scheduler) Node daemons (the kubelet, kube-proxy, and others) Cluster services (e.g., the cluster autoscaler, kube-state-metrics,CoreDNS, etc.) Users making kubectl requests Applications, controllers, and operators that send requests through a kube client Even the API server itself Auditing允许集群管理员回答以下问题： Kubernetes 发生了什么？ 什么时候发生了什么事？ 谁触发了事件？ 事件发生在什么级别？ 它发生在哪里，它是在哪里发起的？ 它去哪儿了？ 2. Auditing 阶段 当某个人或组件向 Kubernetes API 服务器发出请求时，该请求将经历一个或多个阶段： | 阶段 | 说明 | |------------------|--------------------| | RequestReceived | Auditing处理程序已收到请求。 | | ResponseStarted | 已发送响应标头，但尚未发送响应正文。 | | ResponseComplete | 响应正文已完成，不再发送任何字节。 | | Panic | 内部服务器出错，请求未完成。 | 请求的每个阶段都会生成一个事件，该事件根据政策进行处理。政策指定是否应将事件记录为日志条目，如果需要记录，应将哪些数据包含在日志条目中。 3. Auditing级别 Auditing策略定义了关于应该记录哪些事件以及它们应该包含哪些数据的规则。 Auditing策略对象结构在 audit.k8s.ioAPI 组中定义。处理事件时，会按顺序将其与规则列表进行比较。第一个匹配规则设置事件的 Auditing级别。定义的 Auditing级别是： None- 不记录符合此规则的事件。 Metadata- 记录请求元数据（请求用户、时间戳、资源、动词等），但不记录请求或响应正文。 Request- 记录事件元数据和请求正文，但不记录响应正文。这不适用于非资源请求。 RequestResponse- 记录事件元数据、请求和响应主体。这不适用于非资源请求。 您可以将带有策略的文件传递给kube-apiserver 使用--audit-policy-file标志。如果省略该标志，则不记录任何事件。请注意，必须在 Auditing策略文件中提供该rules字段。没有 (0) 规则的策略被视为非法。 下面是一个示例 Auditing策略文件audit/audit-policy.yaml： apiVersion: audit.k8s.io/v1 # This is required. kind: Policy # Don't generate audit events for all requests in RequestReceived stage. omitStages: - \"RequestReceived\" rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: \"\" # Resource \"pods\" doesn't match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: [\"pods\"] # Log \"pods/log\", \"pods/status\" at Metadata level - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] # Don't log requests to a configmap called \"controller-leader\" - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] # Don't log authenticated requests to certain non-resource URL paths. - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" # Log the request body of configmap changes in kube-system. - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the Metadata level. - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the Request level. - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # A catch-all rule to log all other requests at the Metadata level. - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - \"RequestReceived\" 您可以使用最小的 Auditing策略文件来记录Metadata级别的所有请求： # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: Metadata 如果您正在制作自己的 Auditing配置文件，则可以使用 Google Container-Optimized OS 的 Auditing配置文件作为起点。您可以检查 生成 Auditing策略文件的configure-helper.sh脚本。您可以通过直接查看脚本来查看大部分 Auditing策略文件。 您还可以参考Policy配置参考 以获取有关定义的字段的详细信息。 4. Auditing 技巧 通常，create、update 和 delete 等写入请求会在 RequestResponse 级别记录。 通常，get、list 和 watch 事件会在 Metadata 级别记录。 有些事件被视为特殊情况。如需查看被视为特殊情况的请求的确切列表，请参阅政策脚本。在撰写本文时，以下这些是特殊情况： 由 kubelet、system:node-problem-detector 或 system:serviceaccount:kube-system:node-problem-detector 发出的针对 nodes/status 资源或 pods/status 资源的 update 和 patch 请求会在 Request 级别记录。 由 system:nodes 组中的任何身份发出的针对 nodes/status 资源或 pods/status 资源的 update 和 patch 请求会在 Request 级别记录。 system:serviceaccount:kube-system:namespace-controller 发出的 deletecollection 请求会在 Request 级别记录。 针对 secrets 资源、configmaps 资源或 tokenreviews 资源的请求会在 Metadata 级别记录。 某些请求是不会被记录的。如需查看系统不会记录的请求的确切列表，请参阅政策脚本中的 level: None 规则。截至撰写本文时为止，以下请求将不会进行记录： system:kube-proxy 发出的监视 endpoints 资源、services 资源或 services/status 资源的请求； system:unsecured 发出的针对 kube-system 命名空间中 configmaps 资源的 get 请求。 kubelet 发出的针对 nodes 资源或 nodes/status 资源的 get 请求。 system:nodes 组中的任何身份发出的针对 nodes 资源或 nodes/status 资源的 get 请求。 system:kube-controller-manager、system:kube-scheduler 或 system:serviceaccount:endpoint-controller 发出的针对 kube-system 命名空间中 endpoints 资源的 get 和 update 请求。 system:apiserver 发出的针对 namespaces 资源、namespaces/status 资源或 namespaces/finalize 资源的 get 请求。 system:kube-controller-manager 发出的针对 metrics.k8s.io 组中任何资源的 get 和 list 请求。 对与 /healthz*、/version 或 /swagger* 匹配的网址发出的请求。- 5. Auditing 后端 Auditing后端将 Auditing事件持久化到外部存储。开箱即用的 kube-apiserver 提供了两个后端： Log 后端，将事件写入文件系统 Webhook 后端，将事件发送到外部 HTTP API 在所有情况下， Auditing事件都遵循 audit.k8s.io API 组中 Kubernetes API 定义的结构。 5.1 Log 后端 日志后端将 Auditing事件写入JSONlines格式的文件。您可以使用以下kube-apiserver标志配置日志 Auditing后端： --audit-log-path：指定日志后端用于写入 Auditing事件的日志文件路径。不指定此标志会禁用日志后端。-表示标准输出 --audit-log-maxage：定义保留旧 Auditing日志文件的最大天数 --audit-log-maxbackup；定义要保留的 Auditing日志文件的最大数量 --audit-log-maxsize：定义 Auditing日志文件在轮换之前的最大大小（以 MB 为单位） 如果您的集群的控制平面将 kube-apiserver 作为 Pod 运行，请记住将其挂载hostPath 到策略文件和日志文件的位置，以便保留 Auditing记录。例如： --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-path=/var/log/kubernetes/audit/audit.log 然后挂载卷： ... volumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/kubernetes/audit/ name: audit-log readOnly: false 最后配置hostPath： ... volumes: - name: audit hostPath: path: /etc/kubernetes/audit-policy.yaml type: File - name: audit-log hostPath: path: /var/log/kubernetes/audit/ type: DirectoryOrCreate 5.2 Webhook 后端 webhook Auditing后端将 Auditing事件发送到远程 Web API，该 API 被假定为 Kubernetes API 的一种形式，包括身份验证方式。您可以使用以下 kube-apiserver 标志配置 webhook Auditing后端： --audit-webhook-config-file：使用 webhook 配置指定文件的路径。webhook 配置实际上是一个专门的 kubeconfig。 --audit-webhook-initial-backoff：指定在第一个失败请求之后重试之前等待的时间量。使用指数退避重试后续请求。 webhook 配置文件使用 kubeconfig 格式来指定服务的远程地址和用于连接它的凭据。 6. 事件批处理 日志和 webhook 后端都支持批处理。以 webhook 为例，这里是可用标志的列表。要为日志后端获取相同的标志，请在标志名称中webhook替换为。log默认情况下，批处理在 中启用webhook和禁用log。同样，默认情况下，在 中启用webhook和禁用节流log。 --audit-webhook-mode定义缓冲策略。以下之一： batch- 缓冲事件并批量异步处理它们。这是默认设置。 blocking- 在处理每个单独的事件时阻止 API 服务器响应。 blocking-strict- 与阻塞相同，但当在 RequestReceived 阶段 Auditing日志记录失败时，对 kube-apiserver 的整个请求都会失败。 以下标志仅在batch模式中使用： --audit-webhook-batch-buffer-size：定义批处理前要缓冲的事件数。如果传入事件的速率溢出缓冲区，则丢弃事件。 --audit-webhook-batch-max-size：定义一批中的最大事件数。 --audit-webhook-batch-max-wait：定义在队列中无条件批处理事件之前等待的最长时间。 --audit-webhook-batch-throttle-qps：定义每秒生成的最大平均批次数。 --audit-webhook-batch-throttle-burst：如果之前未充分利用允许的 QPS，则定义同时生成的最大批次数。 7. 参数调优 应设置参数以适应 API 服务器上的负载。 例如，如果 kube-apiserver 每秒接收 100 个请求，并且每个请求仅在ResponseStarted和ResponseComplete阶段进行 Auditing，则您应该考虑每秒生成 ≅200 个 Auditing事件。假设批处理中最多有 100 个事件，您应该将限制级别设置为每秒至少 2 个查询。假设后端最多可能需要 5 秒来写入事件，您应该将缓冲区大小设置为最多容纳 5 秒的事件；即：10 个批次，或 1000 个事件。 然而，在大多数情况下，默认参数就足够了，您不必担心手动设置它们。您可以查看 kube-apiserver 和日志中公开的以下 Prometheus 指标，以监控 Auditing子系统的状态。 apiserver_audit_event_totalmetric： 包含导出的 Auditing事件的总数。 apiserver_audit_error_total：指标包含由于导出过程中的错误而丢弃的事件总数。 7.1 日志条目截断 日志和 webhook 后端都支持限制记录的事件的大小。例如，以下是日志后端可用的标志列表： audit-log-truncate-enabled：是否启用事件和批处理截断。 audit-log-truncate-max-batch-size：发送到底层后端的批处理的最大大小（以字节为单位）。 audit-log-truncate-max-event-size：发送到底层后端的 Auditing事件的最大字节数。 默认情况下，truncate 在webhook和中都被禁用log，集群管理员应该设置 audit-log-truncate-enabled或audit-webhook-truncate-enabled启用该功能。 8. Auditing 日志格式示例 以下是 Kubernetes Auditing日志的示例。JSON 结构中的每个键都包含对了解集群中正在发生的事情至关重要的信息。 { \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1\", \"level\": \"RequestResponse\", \"auditID\": \"fbc474df-2466-4612-ae36-69af2c927f9d\", \"stage\": \"ResponseComplete\", \"requestURI\": \"/api/v1/namespaces/default/pods/nginx\", \"verb\": \"get\", \"user\": { \"username\": \"system:node:minikube\", \"groups\": [ \"system:nodes\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.49.2\" ], \"userAgent\": \"kubelet/v1.21.2 (linux/amd64) kubernetes/092fbfb\", \"objectRef\": { \"resource\": \"pods\", \"namespace\": \"default\", \"name\": \"nginx\", \"apiVersion\": \"v1\" }, \"responseStatus\": { \"metadata\": {}, \"code\": 200 }, \"responseObject\": { \"kind\": \"Pod\", \"apiVersion\": \"v1\", \"metadata\": {...}, \"spec\": {...}, \"status\": {...} }, \"requestReceivedTimestamp\": \"2022-01-18T06:57:18.944663Z\", \"stageTimestamp\": \"2022-01-18T06:57:18.968543Z\", \"annotations\": { \"authorization.k8s.io/decision\": \"allow\", \"authorization.k8s.io/reason\": \"\" } } 8.1 user 信息和 sourceIP user密钥告诉您是哪个用户或服务帐户通过 Kubernetes API 服务器发起了请求，而sourceIP密钥提供了该帐户的 IP。IP 地址可以告诉您位置（例如城市、邮政编码或区号）以及用户 ISP 的名称。IP 地址并不总是可靠的，因为它们可以被发出请求的用户更改或隐藏，但在尝试阻止源自 IP 地址范围的恶意活动时，此信息可能很有用。 8.2 有关已启动和执行的请求的信息 verb、requestURI和objectRef都提供有关用户在集群中发起的请求的信息。动词键表示执行的Kubernetes HTTP 请求方法：get、post、list、watch、patch或delete。requestURI提供有关您在集群中发起的 API 请求的信息——例如，获取所有 pod 或创建新部署的请求。objectRef包含有关与请求关联的Kubernetes 对象。objectRef、verb和requestURI提供有关用户发起的请求的完整信息。 8.3 已执行操作的响应状态 responseStatus和responseObject与annotations键相结合，可以深入了解对用户或服务帐户发起的请求的响应。annotations.authorization.k8s.io/decision提供了允许或拒绝值。在对 Kubernetes 集群执行 Auditing以检测异常行为时，这些键非常有用。 9. Auditing policy 场景 在大多数情况下，GKE 应用以下规则来记录来自 Kubernetes API 服务器的条目： 表示 create、delete 和 update 请求的条目将写入管理员活动日志。 表示 get、list 和 updateStatus 请求的条目将写入数据访问日志。 Kubernetes Auditing政策文件开头的规则将指定不记录哪些事件。例如，此规则指定不应记录 kubelet 发出的针对 nodes 资源或 nodes/status 资源的任何 get 请求。前文中提到过，None 级别意味着不应记录任何匹配事件： - level: None users: [\"kubelet\"] # legacy kubelet identity verbs: [\"get\"] resources: - group: \"\" # core resources: [\"nodes\", \"nodes/status\"] 在 level: None 规则列表之后，政策文件包含一列针对特殊情况的规则。例如，下面是一个特殊情况规则，指定在 Metadata 级别记录某些请求： - level: Metadata resources: - group: \"\" # core resources: [\"secrets\", \"configmaps\"] - group: authentication.k8s.io resources: [\"tokenreviews\"] omitStages: - \"RequestReceived\" 如果满足以下所有条件，则事件与规则匹配： 事件与政策文件中前面的任何规则都不匹配。 请求针对 secrets、configmaps 或 tokenreviews 类型的资源。 事件不是针对调用的 RequestReceived 阶段。 在特殊情况规则列表之后，政策文件列出一些通用规则。 如需查看脚本中的通用规则，您必须用 known_apis 的值替换 ${known_apis}。替换后，您会得到如下规则： - level: Request verbs: [\"get\", \"list\", \"watch\"] resources: - group: \"\" # core - group: \"admissionregistration.k8s.io\" - group: \"apiextensions.k8s.io\" - group: \"apiregistration.k8s.io\" - group: \"apps\" - group: \"authentication.k8s.io\" - group: \"authorization.k8s.io\" - group: \"autoscaling\" - group: \"batch\" - group: \"certificates.k8s.io\" - group: \"extensions\" - group: \"metrics.k8s.io\" - group: \"networking.k8s.io\" - group: \"policy\" - group: \"rbac.authorization.k8s.io\" - group: \"settings.k8s.io\" - group: \"storage.k8s.io\" omitStages: - \"RequestReceived\" 该规则适用于与政策文件中前面的任何规则均不匹配且不在 RequestReceived 阶段的事件。该规则指定，针对属于所列群组之一的任何资源发出的 get、list 和 watch 请求应该在 Request 级别记录。 注意：get、list 和 watch 请求实际上没有请求正文，因此该规则的真正作用是指定在 Metadata 级别创建日志条目。 - level: RequestResponse resources: - group: \"\" # core - group: \"admissionregistration.k8s.io\" - group: \"apiextensions.k8s.io\" - group: \"apiregistration.k8s.io\" - group: \"apps\" - group: \"authentication.k8s.io\" - group: \"authorization.k8s.io\" - group: \"autoscaling\" - group: \"batch\" - group: \"certificates.k8s.io\" - group: \"extensions\" - group: \"metrics.k8s.io\" - group: \"networking.k8s.io\" - group: \"policy\" - group: \"rbac.authorization.k8s.io\" - group: \"settings.k8s.io\" - group: \"storage.k8s.io\" omitStages: - \"RequestReceived\" 该规则适用于与政策文件中前面的任何规则均不匹配且不在 RequestReceived 阶段的事件。具体来说，此规则不适用于读取请求：get、list 和 watch。但此规则适用于写入请求，如 create、update 和 delete。该规则指定应该在 RequestResponse 级别记录写入请求。 参考： Kubernetes Auditing Google Kubernetes Engine (GKE) audit-policy 6 Best Practices for Kubernetes Audit Logging Kubernetes Audit Logs | Use Cases & Best Practices Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 07:57:53 "},"对象/Auditing/kubernetes-Auditing-practice.html":{"url":"对象/Auditing/kubernetes-Auditing-practice.html","title":"实战","keywords":"","body":"kubernetes Auditing 实战1. minikube 安装 kuberentes2. 配置 audit-policy.yaml3. 配置存储 Auditing 日志4. 测试 Auditing 功能5. ContainIQ 监控 Kubernetes 审计日志6. demo6.1 configure apiserver to store Audit Logs in json format6.2 create a secret and investigate the json audit log6.3 restrict amount of Audit logs to collect6.4 use Audit Logs to investigate access historykubernetes Auditing 实战 tagsstart Auditing tagsstop 1. minikube 安装 kuberentes minikube start Minikube 在 Centos 7 部署 Kubernetes Ubuntu 18.04 通过 Minikube 安装 Kubernetes v1.20 2. 配置 audit-policy.yaml 创建一个audit-policy.yaml文件 vim /etc/kubernetes/audit-policy.yaml 输入： apiVersion: audit.k8s.io/v1 # This is required. kind: Policy # Prevent requests in the RequestReceived stage from generating audit events. omitStages: - \"RequestReceived\" rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] # Log \"pods/log\", \"pods/status\" at Metadata level - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] # Exclude logging requests to a configmap called \"controller-config\" - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-config\"] # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] # Log deployment changes at RequestResponse level - level: RequestResponse resources: - group: \"\" resources: [\"deployments\"] # Log service changes at metadata level - level: Metadata resources: - group: \"\" resources: [\"services\"] # Log the request body of configmap changes in the kube-system namespace. - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] # You can use an empty string [\"\"] to select resources not associated with a namespace. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the Metadata level. - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the Request level. - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # A wild-card rule to log all other requests at the Metadata level. - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - \"RequestReceived\" 3. 配置存储 Auditing 日志 在以下目录中创建audit.log 。这是 Kubernetes 将保存您的审计日志的地方。 mkdir /var/log/kubernetes/ && cd /var/log/kubernetes/ mkdir audit/ && sudo touch audit.log 编辑kube-apiserver配置文件。 vim /etc/kubernetes/manifests/kube-apiserver.yaml 更新参数 - --audit-policy-file=/etc/kubernetes/audit-policy.yaml - --audit-log-path=/var/log/kubernetes/audit/audit.log 更新配置文件的卷挂载部分。 # ... volumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/kubernetes/audit/audit.log name: audit-log # ... 更新volume部分。 # ... volumes: - hostPath: path: /etc/kubernetes/audit-policy.yaml type: File name: audit - hostPath: path: /var/log/kubernetes/audit/audit.log type: FileOrCreate name: audit-log # ... 4. 测试 Auditing 功能 给 Kubernetes 几秒钟的时间来应用您对kube-apiserver配置文件所做的更改，然后您可以测试您的审计以确保其正常工作。 kubectl get pods -n kube-system -w 创建一个新的 pod，看看 Kubernetes 是否会将请求记录在日志文件中。 kubectl run nginx --image=nginx 返回minikube ssh终端并通过运行以下命令打开日志文件以查看其中是否有日志。 sudo cat /var/log/kubernetes/audit/audit.log | grep -i \"nginx\" 如果您在日志文件中看到类似于下图的内容，则说明您的 Kubernetes 集群上现已启用并配置了审计。 5. ContainIQ 监控 Kubernetes 审计日志 您可以在 Kubernetes 集群中安装多种工具，以在收集和查看日志时提供更丰富的体验。一个这样的工具是 ContainIQ，一个K8s 监控平台。ContainIQ 让您和您的团队可以轻松监控集群内的 Kubernetes 指标、日志和事件。它提供了改进的查看和过滤选项，以帮助您查看日志中真正重要的内容。 安装metrics server（如果尚未安装）。 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 下载 ContainIQ 部署文件。 curl -L -o deployment.yaml https://raw.githubusercontent.com/containiq/containiq-deployment/master/deployment.yaml 通过创建帐户获取您的 ContainIQ API 密钥，然后打开部署文件，滚动到底部，并将您的 API 密钥添加到密钥对象。 完成后，应用部署。 kubectl apply -f deployment.yaml 在集群上实施 ContainIQ 可使您的日志记录更上一层楼。虽然 Kubernetes 审计日志提供了有价值的信息，但它们仅以原始数据日志的形式提供访问，这些日志难以阅读且筛选耗时。但是，使用 ContainIQ，您拥有一个用户界面，允许您根据 pod、时间戳和搜索查询轻松过滤日志，从而更轻松地提取您要查找的信息。 6. demo 6.1 configure apiserver to store Audit Logs in json format mkdir /etc/kubernetes/auditing mkidr /etc/kubernetes/audit/logs $ cat /etc/kubernetes/audit/policy.yaml apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: Metadata $ vim /etc/kubernetes/manifests/kube-apiserver.yaml ..... - --audit-policy-file=/etc/kubernetes/audit/policy.yaml # add - --audit-log-path=/etc/kubernetes/audit/logs/audit.log # add - --audit-log-maxsize=500 # add - --audit-log-maxbackup=5 ......... # ... volumeMounts: - mountPath: /etc/kubernetes/audit/policy.yaml name: audit readOnly: true - mountPath: /var/log/kubernetes/audit/audit.log name: audit-log # ... volumes: - hostPath: path: /etc/kubernetes/audit/policy.yaml type: File name: audit - hostPath: path: /var/log/kubernetes/audit/audit.log type: FileOrCreate name: audit-log # ... 查看kube-apiserver 启动是否正常 $ k get pods -n kube-system |grep api kube-apiserver-master 1/1 Running 3 5m 查看audit.log日志 tail /etc/kubernetes/audit/logs/audit.log {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\",\"auditID\":\"3ea2f430-108b-4b17-b967-6e26619fda99\",\"stage\":\"RequestReceived\",\"requestURI\":\"/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\",\"verb\":\"update\",\"user\":{\"username\":\"system:kube-controller-manager\",\"groups\":[\"system:authenticated\"]},\"sourceIPs\":[\"192.168.211.40\"],\"userAgent\":\"kube-controller-manager/v1.20.7 (linux/amd64) kubernetes/132a687/leader-election\",\"objectRef\":{\"resource\":\"leases\",\"namespace\":\"kube-system\",\"name\":\"kube-controller-manager\",\"apiGroup\":\"coordination.k8s.io\",\"apiVersion\":\"v1\"},\"requestReceivedTimestamp\":\"2021-05-24T08:28:50.431353Z\",\"stageTimestamp\":\"2021-05-24T08:28:50.431353Z\"} {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\",\"auditID\":\"f6a3fc3b-96eb-4e2e-9ba2-66d2e345fb8a\",\"stage\":\"RequestReceived\",\"requestURI\":\"/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s\",\"verb\":\"update\",\"user\":{\"username\":\"system:kube-scheduler\",\"groups\":[\"system:authenticated\"]},\"sourceIPs\":[\"192.168.211.40\"],\"userAgent\":\"kube-scheduler/v1.20.7 (linux/amd64) kubernetes/132a687/leader-election\",\"objectRef\":{\"resource\":\"leases\",\"namespace\":\"kube-system\",\"name\":\"kube-scheduler\",\"apiGroup\":\"coordination.k8s.io\",\"apiVersion\":\"v1\"},\"requestReceivedTimestamp\":\"2021-05-24T08:28:50.435266Z\",\"stageTimestamp\":\"2021-05-24T08:28:50.435266Z\"} {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\",\"auditID\":\"3ea2f430-108b-4b17-b967-6e26619fda99\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\",\"verb\":\"update\",\"user\":{\"username\":\"system:kube-controller-manager\",\"groups\":[\"system:authenticated\"]},\"sourceIPs\":[\"192.168.211.40\"],\"userAgent\":\"kube-controller-manager/v1.20.7 (linux/amd64) kubernetes/132a687/leader-election\",\"objectRef\":{\"resource\":\"leases\",\"namespace\":\"kube-system\",\"name\":\"kube-controller-manager\",\"uid\":\"bcb35dd3-5cb0-4460-99af-dcedb37a6bfa\",\"apiGroup\":\"coordination.k8s.io\",\"apiVersion\":\"v1\",\"resourceVersion\":\"30280\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestReceivedTimestamp\":\"2021-05-24T08:28:50.431353Z\",\"stageTimestamp\":\"2021-05-24T08:28:50.438912Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:kube-controller-manager\\\" of ClusterRole \\\"system:kube-controller-manager\\\" to User \\\"system:kube-controller-manager\\\"\"}} 6.2 create a secret and investigate the json audit log $ k create secret generic very-secure --from-literal=user=admin secret/very-secure created 查看日志 $ cat /etc/kubernetes/audit/logs/audit.log |grep very-secure | jq . { \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1\", \"level\": \"Metadata\", \"auditID\": \"12831143-4615-4f4c-a443-6b80c946a0b1\", \"stage\": \"ResponseComplete\", \"requestURI\": \"/api/v1/namespaces/default/secrets?fieldManager=kubectl-create\", \"verb\": \"create\", \"user\": { \"username\": \"kubernetes-admin\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.211.40\" ], \"userAgent\": \"kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19\", \"objectRef\": { \"resource\": \"secrets\", \"namespace\": \"default\", \"name\": \"very-secure\", \"apiVersion\": \"v1\" }, \"responseStatus\": { \"metadata\": {}, \"code\": 201 }, \"requestReceivedTimestamp\": \"2021-05-24T08:30:54.109005Z\", \"stageTimestamp\": \"2021-05-24T08:30:54.114724Z\", \"annotations\": { \"authorization.k8s.io/decision\": \"allow\", \"authorization.k8s.io/reason\": \"\" } } 6.3 restrict amount of Audit logs to collect $ cat policy.yaml apiVersion: audit.k8s.io/v1 kind: Policy omitStages: - \"RequestReceived\" rules: - level: Metadata - level: None verbs: [\"get\",\"list\",\"watch\"] - level: Metadata resources: - group: \"\" resources: [\"secrets\"] - level: RequestResponse 重启kube-apiserver mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/ ps aux |grep api mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml ps aux |grep api 查看audit.log tail /etc/kubernetes/audit/logs/audit.log | jq . 6.4 use Audit Logs to investigate access history 创建 servicemount k create sa very-crazy-sa k get sa k get secret cat /etc/kubernetes/audit/logs/audit.log |grep very-crazy-sa 创建 pod $ k run accessor --image=nginx --dry-run=client -oyaml > pod.yaml $ vim pod3.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: accessor name: accessor spec: serviceAccountName: very-crazy-sa #添加此行 containers: - image: nginx name: accessor resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} k create -f pod.yaml k get pod accessor -w $ cat /etc/kubernetes/audit/logs/audit.log |grep accessor {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"RequestResponse\",\"auditID\":\"1025990a-e51c-4c51-9010-6436999a88cb\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/default/pods/accessor/status\",\"verb\":\"patch\",\"user\":{\"username\":\"system:node:node2\",\"groups\":[\"system:nodes\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.211.42\"],\"userAgent\":\"kubelet/v1.20.1 (linux/amd64) kubernetes/c4d7527\",\"objectRef\":{\"resource\":\"pods\",\"namespace\":\"default\",\"name\":\"accessor\",\"apiVersion\":\"v1\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"metadata\":{\"uid\":\"d67a9290-1dc8-4f14-ac84-88e9ae82d2a2\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2021-05-24T09:29:44Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2021-05-24T09:29:44Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://d80411bf54156c9f65c4887c4255dc545b8ebdf518d4d9470f23b0ad3f984b39\",\"image\":\"nginx:latest\",\"imageID\":\"docker-pullable://nginx@sha256:df13abe416e37eb3db4722840dd479b00ba193ac6606e7902331dcea50f4f1f2\",\"lastState\":{},\"name\":\"accessor\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2021-05-24T09:29:43Z\"}}}],\"phase\":\"Running\",\"podIP\":\"10.244.104.11\",\"podIPs\":[{\"ip\":\"10.244.104.11\"}]}},\"responseObject\":{\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"accessor\",\"namespace\":\"default\",\"uid\":\"d67a9290-1dc8-4f14-ac84-88e9ae82d2a2\",\"resourceVersion\":\"35224\",\"creationTimestamp\":\"2021-05-24T09:29:24Z\",\"labels\":{\"run\":\"accessor\"},\"annotations\":{\"cni.projectcalico.org/podIP\":\"10.244.104.11/32\",\"cni.projectcalico.org/podIPs\":\"10.244.104.11/32\"},\"managedFields\":[{\"manager\":\"kubectl-create\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2021-05-24T09:29:24Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:run\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"accessor\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:serviceAccount\":{},\"f:serviceAccountName\":{},\"f:terminationGracePeriodSeconds\":{}}}},{\"manager\":\"calico\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2021-05-24T09:29:26Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:cni.projectcalico.org/podIP\":{},\"f:cni.projectcalico.org/podIPs\":{}}}}},{\"manager\":\"kubelet\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2021-05-24T09:29:44Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:conditions\":{\"k:{\\\"type\\\":\\\"ContainersReady\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:status\":{},\"f:type\":{}},\"k:{\\\"type\\\":\\\"Initialized\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:status\":{},\"f:type\":{}},\"k:{\\\"type\\\":\\\"Ready\\\"}\":{\".\":{},\"f:lastProbeTime\":{},\"f:lastTransitionTime\":{},\"f:status\":{},\"f:type\":{}}},\"f:containerStatuses\":{},\"f:hostIP\":{},\"f:phase\":{},\"f:podIP\":{},\"f:podIPs\":{\".\":{},\"k:{\\\"ip\\\":\\\"10.244.104.11\\\"}\":{\".\":{},\"f:ip\":{}}},\"f:startTime\":{}}}}]},\"spec\":{\"volumes\":[{\"name\":\"very-crazy-sa-token-fr7sw\",\"secret\":{\"secretName\":\"very-crazy-sa-token-fr7sw\",\"defaultMode\":420}}],\"containers\":[{\"name\":\"accessor\",\"image\":\"nginx\",\"resources\":{},\"volumeMounts\":[{\"name\":\"very-crazy-sa-token-fr7sw\",\"readOnly\":true,\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"Always\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"serviceAccountName\":\"very-crazy-sa\",\"serviceAccount\":\"very-crazy-sa\",\"nodeName\":\"node2\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\",\"tolerations\":[{\"key\":\"node.kubernetes.io/not-ready\",\"operator\":\"Exists\",\"effect\":\"NoExecute\",\"tolerationSeconds\":300},{\"key\":\"node.kubernetes.io/unreachable\",\"operator\":\"Exists\",\"effect\":\"NoExecute\",\"tolerationSeconds\":300}],\"priority\":0,\"enableServiceLinks\":true,\"preemptionPolicy\":\"PreemptLowerPriority\"},\"status\":{\"phase\":\"Running\",\"conditions\":[{\"type\":\"Initialized\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2021-05-24T09:29:24Z\"},{\"type\":\"Ready\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2021-05-24T09:29:44Z\"},{\"type\":\"ContainersReady\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2021-05-24T09:29:44Z\"},{\"type\":\"PodScheduled\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2021-05-24T09:29:24Z\"}],\"hostIP\":\"192.168.211.42\",\"podIP\":\"10.244.104.11\",\"podIPs\":[{\"ip\":\"10.244.104.11\"}],\"startTime\":\"2021-05-24T09:29:24Z\",\"containerStatuses\":[{\"name\":\"accessor\",\"state\":{\"running\":{\"startedAt\":\"2021-05-24T09:29:43Z\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"nginx:latest\",\"imageID\":\"docker-pullable://nginx@sha256:df13abe416e37eb3db4722840dd479b00ba193ac6606e7902331dcea50f4f1f2\",\"containerID\":\"docker://d80411bf54156c9f65c4887c4255dc545b8ebdf518d4d9470f23b0ad3f984b39\",\"started\":true}],\"qosClass\":\"BestEffort\"}},\"requestReceivedTimestamp\":\"2021-05-24T09:29:44.384315Z\",\"stageTimestamp\":\"2021-05-24T09:29:44.426816Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}} 参考： Kubernetes Auditing Google Kubernetes Engine (GKE) audit-policy 6 Best Practices for Kubernetes Audit Logging Kubernetes Audit Logs | Use Cases & Best Practices Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-16 07:22:10 "},"存储/":{"url":"存储/","title":"存储","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"存储/kubernetes-storage-introduce-ClexVolume-and-CSI.html":{"url":"存储/kubernetes-storage-introduce-ClexVolume-and-CSI.html","title":"FlexVolume CSI","keywords":"","body":"如何理解 kubernetes 存储插件：FlexVolume、CSI1. FlexVolume原理2. Container Storage Interface（CSI）原理2.1 三个 External Components2.2 CSI 插件的里三个服务3. 总结如何理解 kubernetes 存储插件：FlexVolume、CSI tagsstart 存储 tagsstop 1. FlexVolume原理 在 Kubernetes 中，存储插件的开发有两种方式：FlexVolume 和 CSI。 举个例子，现在我们要编写的是一个使用 NFS 实现的 FlexVolume 插件。 对于一个 FlexVolume 类型的 PV 来说，它的 YAML 文件如下所示： apiVersion: v1 kind: PersistentVolume metadata: name: pv-flex-nfs spec: capacity: storage: 10Gi accessModes: - ReadWriteMany flexVolume: driver: \"k8s/nfs\" fsType: \"nfs\" options: server: \"10.10.0.25\" # 改成你自己的NFS服务器地址 share: \"export\" Volume 的 options 字段:类型是 map[string]string options 字段指定了 NFS 服务器的地址（server: “10.10.0.25”），以及 NFS 共享目录的名字（share: “export”）,当然，你这里定义的所有参数，后面都会被 FlexVolume 拿到。 你可以使用这个 Docker 镜像轻松地部署一个试验用的 NFS 服务器。 这样的一个 PV 被创建后，一旦和某个 PVC 绑定起来，这个 FlexVolume 类型的 Volume 就会进入到“两阶段处理”，即“Attach 阶段”和“Mount 阶段”。它们的主要作用，是在 Pod 所绑定的宿主机上，完成这个 Volume 目录的持久化过程，比如为虚拟机挂载磁盘（Attach），或者挂载一个 NFS 的共享目录（Mount）。 而在具体的控制循环中，这两个操作实际上调用的，正是 Kubernetes 的 pkg/volume 目录下的存储插件（Volume Plugin）。在我们这个例子里，就是 pkg/volume/flexvolume 这个目录里的代码。 当然了，这个目录其实只是 FlexVolume 插件的入口。以“Mount 阶段”为例，在 FlexVolume 目录里，它的处理过程非常简单，如下所示： // SetUpAt creates new directory. func (f *flexVolumeMounter) SetUpAt(dir string, fsGroup *int64) error { ... call := f.plugin.NewDriverCall(mountCmd) // Interface parameters call.Append(dir) extraOptions := make(map[string]string) // pod metadata extraOptions[optionKeyPodName] = f.podName extraOptions[optionKeyPodNamespace] = f.podNamespace ... call.AppendSpec(f.spec, f.plugin.host, extraOptions) _, err = call.Run() ... return nil } 面这个名叫 SetUpAt() 的方法，正是 FlexVolume 插件对“Mount 阶段”的实现位置。而 SetUpAt() 实际上只做了一件事，那就是封装出了一行命令（即：NewDriverCall），由 kubelet 在“Mount 阶段”去执行。在我们这个例子中，kubelet 要通过插件在宿主机上执行的命令，如下所示： /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs 就是插件的可执行文件的路径 nfs 的文件，正是你要编写的插件的实现。它可以是一个二进制文件，也可以是一个脚本。总之，只要能在宿主机上被执行起来即可。 k8s~nfs 部分，正是这个插件在 Kubernetes 里的名字。它是从 driver=\"k8s/nfs\"字段解析出来的。这个 driver 字段的格式是：vendor/driver。比如，一家存储插件的提供商（vendor）的名字叫作 k8s，提供的存储驱动（driver）是 nfs，那么 Kubernetes 就会使用 k8s~nfs 来作为插件名。 所以说，当你编写完了 FlexVolume 的实现之后，一定要把它的可执行文件放在每个节点的插件目录下。 可执行文件后面的“mount”参数，定义的就是当前的操作。在 FlexVolume 里，这些操作参数的名字是固定的，比如 init、mount、unmount、attach，以及 dettach 等等，分别对应不同的 Volume 处理操作。 而跟在 mount 参数后面的两个字段：和，则是 FlexVolume 必须提供给这条命令的两个执行参数。 其中第一个执行参数，正是 kubelet 调用 SetUpAt() 方法传递来的 dir 的值。它代表的是当前正在处理的 Volume 在宿主机上的目录。在我们的例子里，这个路径如下所示： /var/lib/kubelet/pods//volumes/k8s~nfs/test test 正是我们前面定义的 PV 的名字；而 k8s~nfs，则是插件的名字。可以看到，插件的名字正是从你声明的 driver=\"k8s/nfs\"字段里解析出来的。 第二个执行参数，则是一个 JSON Map 格式的参数列表。我们在前面 PV 里定义的 options字段的值，都会被追加在这个参数里。此外，在 SetUpAt() 方法里可以看到，这个参数列表里还包括了 Pod 的名字、Namespace等元数据（Metadata）。 编写了一个简单的 shell 脚本来作为插件的实现，它对“Mount 阶段”的处理过程，如下所示： # Notes: # - Please install \"jq\" package before using this driver. usage() { err \"Invalid usage. Usage: \" err \"\\t$0 init\" err \"\\t$0 mount \" err \"\\t$0 unmount \" exit 1 } err() { echo -ne $* 1>&2 } log() { echo -ne $* >&1 } ismounted() { MOUNT=`findmnt -n ${MNTPATH} 2>/dev/null | cut -d' ' -f1` if [ \"${MOUNT}\" == \"${MNTPATH}\" ]; then echo \"1\" else echo \"0\" fi } domount() { MNTPATH=$1 local NFS_SERVER=$(echo $2 | jq -r '.server') local SHARE=$(echo $2 | jq -r '.share') local PROTOCOL=$(echo $2 | jq -r '.protocol') local ATIME=$(echo $2 | jq -r '.atime') local READONLY=$(echo $2 | jq -r '.readonly') if [ -n \"${PROTOCOL}\" ]; then PROTOCOL=\"tcp\" fi if [ -n \"${ATIME}\" ]; then ATIME=\"0\" fi if [ -n \"${READONLY}\" ]; then READONLY=\"0\" fi if [ \"${PROTOCOL}\" != \"tcp\" ] && [ \"${PROTOCOL}\" != \"udp\" ] ; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"Invalid protocol ${PROTOCOL}\\\"}\" exit 1 fi if [ $(ismounted) -eq 1 ] ; then log '{\"status\": \"Success\"}' exit 0 fi mkdir -p ${MNTPATH} &> /dev/null local NFSOPTS=\"${PROTOCOL},_netdev,soft,timeo=10,intr\" if [ \"${ATIME}\" == \"0\" ]; then NFSOPTS=\"${NFSOPTS},noatime\" fi if [ \"${READONLY}\" != \"0\" ]; then NFSOPTS=\"${NFSOPTS},ro\" fi mount -t nfs -o${NFSOPTS} ${NFS_SERVER}:/${SHARE} ${MNTPATH} &> /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } unmount() { MNTPATH=$1 if [ $(ismounted) -eq 0 ] ; then log '{\"status\": \"Success\"}' exit 0 fi umount ${MNTPATH} &> /dev/null if [ $? -ne 0 ]; then err \"{ \\\"status\\\": \\\"Failed\\\", \\\"message\\\": \\\"Failed to unmount volume at ${MNTPATH}\\\"}\" exit 1 fi log '{\"status\": \"Success\"}' exit 0 } op=$1 if ! command -v jq >/dev/null 2>&1; then err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"'jq' binary not found. Please install jq package before using this driver\\\"}\" exit 1 fi if [ \"$op\" = \"init\" ]; then log '{\"status\": \"Success\", \"capabilities\": {\"attach\": false}}' exit 0 fi if [ $# -lt 2 ]; then usage fi shift case \"$op\" in mount) domount $* ;; unmount) unmount $* ;; *) log '{\"status\": \"Not supported\"}' exit 0 esac exit 1 解释 当 kubelet 在宿主机上执行“nfs mount ”的时候，这个名叫 nfs 的脚本，就可以直接从参数里拿到 Volume 在宿主机上的目录，即：MNTPATH=$1。而你在 PV 的 options 字段里定义的 NFS 的服务器地址（options.server）和共享目录名字（options.share），则可以从第二个参数里解析出来。 需要注意的是，当这个 mount -t nfs 操作完成后，你必须把一个 JOSN 格式的字符串，比如：{“status”: “Success”}，返回给调用者，也就是 kubelet。这是 kubelet 判断这次调用是否成功的唯一依据。 综上所述，在“Mount 阶段”，kubelet 的 VolumeManagerReconcile 控制循环里的一次“调谐”操作的执行流程，如下所示： kubelet --> pkg/volume/flexvolume.SetUpAt() --> /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount 备注：这个 NFS 的 FlexVolume 的完整实现，在这个 GitHub 库里。而你如果想用 Go 语言编写 FlexVolume的话，我也有一个很好的例子供你参考。 当然，在前面文章中我也提到过，像 NFS 这样的文件系统存储，并不需要在宿主机上挂载磁盘或者块设备。所以，我们也就不需要实现 attach 和 dettach 操作了。 比如，跟 Kubernetes 内置的 NFS 插件类似，这个 NFS FlexVolume 插件，也不能支持 Dynamic Provisioning（即：为每个 PVC 自动创建 PV 和对应的 Volume）。除非你再为它编写一个专门的 External Provisioner。 再比如，我的插件在执行 mount 操作的时候，可能会生成一些挂载信息。这些信息，在后面执行 unmount 操作的时候会被用到。可是，在上述 FlexVolume 的实现里，你没办法把这些信息保存在一个变量里，等到 unmount 的时候直接使用。 这个原因也很容易理解：FlexVolume 每一次对插件可执行文件的调用，都是一次完全独立的操作。所以，我们只能把这些信息写在一个宿主机上的临时文件里，等到 unmount 的时候再去读取。 这也是为什么，我们需要有 Container Storage Interface（CSI）这样更完善、更编程友好的插件方式。 2. Container Storage Interface（CSI）原理 其实，通过前面对 FlexVolume 的讲述，你应该可以明白，默认情况下，Kubernetes 里通过存储插件管理容器持久化存储的原理，可以用如下所示的示意图来描述： 在上述体系下，无论是 FlexVolume，还是 Kubernetes 内置的其他存储插件，它们实际上担任的角色，仅仅是 Volume 管理中的“Attach 阶段”和“Mount 阶段”的具体执行者。而像 Dynamic Provisioning 这样的功能，就不是存储插件的责任，而是 Kubernetes 本身存储管理功能的一部分。 相比之下，CSI 插件体系的设计思想，就是把这个 Provision 阶段，以及 Kubernetes 里的一部分存储管理功能，从主干代码里剥离出来，做成了几个单独的组件。这些组件会通过 Watch API 监听 Kubernetes 里与存储相关的事件变化，比如 PVC 的创建，来执行具体的存储管理动作。 而这些管理动作，比如“Attach 阶段”和“Mount 阶段”的具体操作，实际上就是通过调用 CSI 插件来完成的。这种设计思路，我可以用如下所示的一幅示意图来表示： 这套存储插件体系多了三个独立的外部组件（External Components），即：Driver Registrar、External Provisioner 和 External Attacher，对应的正是从Kubernetes 项目里面剥离出来的那部分存储管理功能。 图中最右侧的部分，就是需要我们编写代码来实现的 CSI 插件。一个 CSI 插件只有一个二进制文件，但它会以 gRPC的方式对外提供三个服务（gRPC Service），分别叫作：CSI Identity、CSI Controller 和 CSI Node。 2.1 三个 External Components Driver Registrar 组件，负责将插件注册到 kubelet里面（这可以类比为，将可执行文件放在插件目录下）。而在具体实现上，Driver Registrar 需要请求 CSI 插件的Identity 服务来获取插件信息。 External Provisioner 组件，负责的正是 Provision 阶段。在具体实现上，External Provisioner 监听（Watch）了 APIServer 里的 PVC 对象。当一个 PVC 被创建时，它就会调用 CSI Controller 的 CreateVolume 方法，为你创建对应 PV。此外，如果你使用的存储是公有云提供的磁盘（或者块设备）的话，这一步就需要调用公有云（或者块设备服务）的 API 来创建这个 PV 所描述的磁盘（或者块设备）了。 不过，由于 CSI 插件是独立于 Kubernetes 之外的，所以在 CSI 的 API 里不会直接使用 Kubernetes 定义的 PV 类型，而是会自己定义一个单独的 Volume 类型。为了方便叙述，我会把 Kubernetes 里的持久化卷类型叫作 PV，把 CSI 里的持久化卷类型叫作 CSI Volume。 External Attacher 组件，负责的正是“Attach 阶段”。在具体实现上，它监听了 APIServer VolumeAttachment 对象的变化。VolumeAttachment 对象是 Kubernetes 确认一个 Volume可以进入“Attach 阶段”的重要标志，一旦出现了 VolumeAttachment 对象，External Attacher 就会调用 CSI Controller 服务的 ControllerPublish 方法，完成它所对应的 Volume 的 Attach 阶段。 而 Volume 的“**Mount 阶段**”，并不属于 External Components 的职责。当 kubelet 的 VolumeManagerReconciler 控制循环检查到它需要执行 Mount 操作的时候，会通过 pkg/volume/csi 包，直接调用 CSI Node 服务完成 Volume 的“Mount 阶段”。 在实际使用 CSI 插件的时候，我们会将这三个 External Components 作为 sidecar 容器和 CSI 插件放置在同一个 Pod 中。由于 External Components 对 CSI 插件的调用非常频繁，所以这种 sidecar 的部署方式非常高效。 2.2 CSI 插件的里三个服务 CSI Identity、CSI Controller 和 CSI Node CSI 插件的 CSI Identity 服务，负责对外暴露这个插件本身的信息，如下所示： service Identity { // return the version and name of the plugin rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {} // reports whether the plugin has the ability of serving the Controller interface rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {} // called by the CO just to check whether the plugin is running or not rpc Probe (ProbeRequest) returns (ProbeResponse) {} } 而 CSI Controller 服务，定义的则是对 CSI Volume（对应 Kubernetes 里的 PV）的管理接口，比如：创建和删除 CSI Volume、对 CSI Volume 进行 Attach/Dettach（在 CSI 里，这个操作被叫作 Publish/Unpublish），以及对 CSI Volume 进行 Snapshot 等，它们的接口定义如下所示： service Controller { // provisions a volume rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} // deletes a previously provisioned volume rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {} // make a volume available on some required node rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {} // make a volume un-available on some required node rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {} ... // make a snapshot rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {} // Delete a given snapshot rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {} ... } 不难发现，CSI Controller 服务里定义的这些操作有个共同特点，那就是它们都无需在宿主机上进行，而是属于 Kubernetes 里 Volume Controller 的逻辑，也就是属于 Master 节点的一部分。 需要注意的是，正如我在前面提到的那样，CSI Controller 服务的实际调用者，并不是 Kubernetes（即：通过 pkg/volume/csi 发起 CSI 请求），而是 External Provisioner 和 External Attacher。这两个 External Components，分别通过监听 PVC 和 VolumeAttachement 对象，来跟 Kubernetes 进行协作。 而 CSI Volume 需要在宿主机上执行的操作，都定义在了 CSI Node 服务里面，如下所示： service Node { // temporarily mount the volume to a staging path rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} // unmount the volume from staging path rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {} // mount the volume from staging to target path rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {} // unmount the volume from staging path rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {} // stats for the volume rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {} ... // Similar to NodeGetId rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {} } 需要注意的是，“Mount 阶段”在 CSI Node 里的接口，是由 NodeStageVolume 和 NodePublishVolume 两个接口共同实现的。 3. 总结 可以看到，相比于 FlexVolume，CSI 的设计思想，把插件的职责从“两阶段处理”，扩展成了 Provision、Attach 和 Mount 三个阶段。其中，Provision 等价于“创建磁盘”，Attach 等价于“挂载磁盘到虚拟机”，Mount 等价于“将该磁盘格式化后，挂载在 Volume 的宿主机目录上”。 在有了 CSI 插件之后，与Kubernetes 本身管理storageclass区别： 当 AttachDetachController 需要进行“Attach”操作时（“Attach 阶段”），它实际上会执行到 pkg/volume/csi 目录中，创建一个 VolumeAttachment 对象，从而触发 External Attacher 调用 CSI Controller 服务的 ControllerPublishVolume 方法。 当 VolumeManagerReconciler 需要进行“Mount”操作时（“Mount 阶段”），它实际上也会执行到 pkg/volume/csi 目录中，直接向 CSI Node 服务发起调用 NodePublishVolume 方法的请求。 参考： 张磊 极客时间 深入浅出 kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 09:08:17 "},"存储/kubernetes-storage-write-CSI.html":{"url":"存储/kubernetes-storage-write-CSI.html","title":"编写 CSI","keywords":"","body":"kubernetes 编写CSI插件总结kubernetes 编写CSI插件 tagsstart 存储 tagsstop 上一篇是：如何理解存储插件：FlexVolume与CSI 以DigitalOcean 的块存储（Block Storage）服务，来作为实践对象。 DigitalOcean 是业界知名的“最简”公有云服务，即：它只提供虚拟机、存储、网络等为数不多的几个基础功能，其他功能一概不管。而这，恰恰就使得 DigitalOcean 成了我们在公有云上实践 Kubernetes 的最佳选择。 目标：编写的 CSI 插件的功能，就是：让我们运行在 DigitalOcean 上的 Kubernetes 集群能够使用它的块存储服务，作为容器的持久化存储。 备注：在 DigitalOcean 上部署一个 Kubernetes 集群的过程，也很简单。你只需要先在 DigitalOcean 上创建几个虚拟机，然后按照我们在第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》中从 0 到 1的步骤直接部署即可。 而有了 CSI 插件之后，持久化存储的用法就非常简单了，你只需要创建一个如下所示的 StorageClass 对象即可： kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: do-block-storage namespace: kube-system annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: com.digitalocean.csi.dobs 有了这个 StorageClass，External Provisoner 就会为集群中新出现的 PVC 自动创建出 PV，然后调用 CSI 插件创建出这个 PV 对应的 Volume，这正是 CSI 体系中 Dynamic Provisioning 的实现方式。 storageclass.kubernetes.io/is-default-class: \"true\"的意思，是使用这个StorageClass 作为默认的持久化存储提供者。 provisioner=com.digitalocean.csi.dobs 这个字段。显然，这个字段告诉了Kubernetes，请使用名叫 com.digitalocean.csi.dobs 的 CSI 插件来为我处理这个StorageClass 相关的所有操作。 那么，Kubernetes 又是如何知道一个 CSI 插件的名字的呢？ 这就需要从 CSI 插件的第一个服务 CSI Identity 说起了。 其实，一个 CSI 插件的代码结构非常简单，如下所示： tree $GOPATH/src/github.com/digitalocean/csi-digitalocean/driver $GOPATH/src/github.com/digitalocean/csi-digitalocean/driver ├── controller.go ├── driver.go ├── identity.go ├── mounter.go └── node.go CSI Identity 服务的实现，就定义在了 driver 目录下的 identity.go 文件里。 当然，为了能够让 Kubernetes 访问到 CSI Identity 服务，我们需要先在 driver.go 文件里，定义一个标准的 gRPC Server，如下所示： // Run starts the CSI plugin by communication over the given endpoint func (d *Driver) Run() error { ... listener, err := net.Listen(u.Scheme, addr) ... d.srv = grpc.NewServer(grpc.UnaryInterceptor(errHandler)) csi.RegisterIdentityServer(d.srv, d) csi.RegisterControllerServer(d.srv, d) csi.RegisterNodeServer(d.srv, d) d.ready = true // we're now ready to go! ... return d.srv.Serve(listener) } 可以看到，只要把编写好的 gRPC Server 注册给 CSI，它就可以响应来自 External Components 的 CSI 请求了。CSI Identity 服务中，最重要的接口是 GetPluginInfo，它返回的就是这个插件的名字和版本号，如下所示： 备注：CSI 各个服务的接口我在上一篇文章中已经介绍过，你也可以在这里找到它的 protoc 文件。 func (d *Driver) GetPluginInfo(ctx context.Context, req *csi.GetPluginInfoRequest) (*csi.GetPluginInfoResponse, error) { resp := &csi.GetPluginInfoResponse{ Name: driverName, VendorVersion: version, } ... } 其中，driverName 的值，正是\"com.digitalocean.csi.dobs\"。所以说，Kubernetes 正是通过 GetPluginInfo 的返回值，来找到你在 StorageClass 里声明要使用的 CSI 插件的。 注：CSI 要求插件的名字遵守“反向 DNS”格式。 另外一个 GetPluginCapabilities 接口也很重要。这个接口返回的是这个 CSI 插件的“能力”。 比如，当你编写的 CSI 插件不准备实现“Provision 阶段”和“Attach 阶段”（比如，一个最简单的 NFS 存储插件就不需要这两个阶段）时，你就可以通过这个接口返回：本插件不提供 CSI Controller 服务，即：没有 csi.PluginCapability_Service_CONTROLLER_SERVICE 这个“能力”。这样，Kubernetes 就知道这个信息了。 最后，CSI Identity 服务还提供了一个 Probe 接口。Kubernetes 会调用它来检查这个 CSI 插件是否正常工作。 一般情况下，我建议你在编写插件时给它设置一个 Ready 标志，当插件的 gRPC Server 停止的时候，把这个 Ready 标志设置为 false。或者，你可以在这里访问一下插件的端口，类似于健康检查的做法。 然后，我们要开始编写 CSI 插件的第二个服务，即 CSI Controller 服务了。它的代码实现，在 controller.go 文件里。这个服务主要实现的就是 Volume 管理流程中的“Provision 阶段”和“Attach 阶段” 。 Provision 阶段”对应的接口，是 CreateVolume 和 DeleteVolume，它们的调用者是 External Provisoner。以 CreateVolume 为例，它的主要逻辑如下所示： func (d *Driver) CreateVolume(ctx context.Context, req *csi.CreateVolumeRequest) (*csi.CreateVolumeResponse, error) { ... volumeReq := &godo.VolumeCreateRequest{ Region: d.region, Name: volumeName, Description: createdByDO, SizeGigaBytes: size / GB, } ... vol, _, err := d.doClient.Storage.CreateVolume(ctx, volumeReq) ... resp := &csi.CreateVolumeResponse{ Volume: &csi.Volume{ Id: vol.ID, CapacityBytes: size, AccessibleTopology: []*csi.Topology{ { Segments: map[string]string{ \"region\": d.region, }, }, }, }, } return resp, nil } 可以看到，对于 DigitalOcean 这样的公有云来说，CreateVolume 需要做的操作，就是调用 DigitalOcean 块存储服务的 API，创建出一个存储卷（d.doClient.Storage.CreateVolume）。果你使用的是其他类型的块存储（比如 Cinder、Ceph RBD 等），对应的操作也是类似地调用创建存储卷的 API。 而“Attach 阶段”对应的接口是 ControllerPublishVolume 和 ControllerUnpublishVolume，它们的调用者是 External Attacher。以 ControllerPublishVolume 为例，它的逻辑如下所示： func (d *Driver) ControllerPublishVolume(ctx context.Context, req *csi.ControllerPublishVolumeRequest) (*csi.ControllerPublishVolumeResponse, error) { ... dropletID, err := strconv.Atoi(req.NodeId) // check if volume exist before trying to attach it _, resp, err := d.doClient.Storage.GetVolume(ctx, req.VolumeId) ... // check if droplet exist before trying to attach the volume to the droplet _, resp, err = d.doClient.Droplets.Get(ctx, dropletID) ... action, resp, err := d.doClient.StorageActions.Attach(ctx, req.VolumeId, dropletID) ... if action != nil { ll.Info(\"waiting until volume is attached\") if err := d.waitAction(ctx, req.VolumeId, action.ID); err != nil { return nil, err } } ll.Info(\"volume is attached\") return &csi.ControllerPublishVolumeResponse{}, nil } 可以看到，对于 DigitalOcean 来说，ControllerPublishVolume 在“Attach 阶段”需要做的工作，是调用 DigitalOcean 的 API，将我们前面创建的存储卷，挂载到指定的虚拟机上（d.doClient.StorageActions.Attach） 其中，存储卷由请求中的 VolumeId 来指定。而虚拟机，也就是将要运行 Pod 的宿主机，则由请求中的 NodeId 来指定。这些参数，都是 External Attacher 在发起请求时需要设置的。 External Attacher 的工作原理，是监听（Watch）了一种名叫 VolumeAttachment 的 API 对象。这种 API 对象的主要字段如下所示： // VolumeAttachmentSpec is the specification of a VolumeAttachment request. type VolumeAttachmentSpec struct { // Attacher indicates the name of the volume driver that MUST handle this // request. This is the name returned by GetPluginName(). Attacher string // Source represents the volume that should be attached. Source VolumeAttachmentSource // The node that the volume should be attached to. NodeName string } 而这个对象的生命周期，正是由 AttachDetachController 负责管理的 这个控制循环的职责，是不断检查 Pod 所对应的 PV，在它所绑定的宿主机上的挂载情况，从而决定是否需要对这个 PV 进行 Attach（或者 Dettach）操作。 而这个 Attach 操作，在 CSI 体系里，就是创建出上面这样一个 VolumeAttachment 对象。可以看到，Attach 操作所需的 PV 的名字（Source）、宿主机的名字（NodeName）、存储插件的名字（Attacher），都是这个 VolumeAttachment 对象的一部分。 而当 External Attacher 监听到这样的一个对象出现之后，就可以立即使用 VolumeAttachment 里的这些字段，封装成一个 gRPC 请求调用 CSI Controller 的 ControllerPublishVolume 方法。最后，我们就可以编写 CSI Node 服务了。 CSI Node 服务对应的，是 Volume 管理流程里的“Mount 阶段”。它的代码实现，在 node.go 文件里。kubelet 的 VolumeManagerReconciler 控制循环会直接调用 CSI Node 服务来完成 Volume 的“Mount 阶段”。过，在具体的实现中，这个“Mount 阶段”的处理其实被细分成了 NodeStageVolume 和 NodePublishVolume 这两个接口。 这里的原因其实也很容易理解：于磁盘以及块设备来说，它们被 Attach 到宿主机上之后，就成为了宿主机上的一个待用存储设备。而到了“Mount 阶段”，我们首先需要格式化这个设备，然后才能把它挂载到 Volume 对应的宿主机目录上。 在 kubelet 的 VolumeManagerReconciler 控制循环中，这两步操作分别叫作 MountDevice 和 SetUp。 其中，MountDevice 操作，就是直接调用了 CSI Node 服务里的 NodeStageVolume 接口。顾名思义，这个接口的作用，就是格式化 Volume 在宿主机上对应的存储设备，然后挂载到一个临时目录（Staging 目录）上。 对于 DigitalOcean 来说，它对 NodeStageVolume 接口的实现如下所示： func (d *Driver) NodeStageVolume(ctx context.Context, req *csi.NodeStageVolumeRequest) (*csi.NodeStageVolumeResponse, error) { ... vol, resp, err := d.doClient.Storage.GetVolume(ctx, req.VolumeId) ... source := getDiskSource(vol.Name) target := req.StagingTargetPath ... if !formatted { ll.Info(\"formatting the volume for staging\") if err := d.mounter.Format(source, fsType); err != nil { return nil, status.Error(codes.Internal, err.Error()) } } else { ll.Info(\"source device is already formatted\") } ... if !mounted { if err := d.mounter.Mount(source, target, fsType, options...); err != nil { return nil, status.Error(codes.Internal, err.Error()) } } else { ll.Info(\"source device is already mounted to the target path\") } ... return &csi.NodeStageVolumeResponse{}, nil } 可以看到，在 NodeStageVolume 的实现里，我们首先通过 DigitalOcean 的 API 获取到了这个 Volume 对应的设备路径（getDiskSource）；然后，我们把这个设备格式化成指定的格式（ d.mounter.Format）；最后，我们把格式化后的设备挂载到了一个临时的 Staging 目录（StagingTargetPath）下。 而 SetUp 操作则会调用 CSI Node 服务的 NodePublishVolume 接口。有了上述对设备的预处理工作后，它的实现就非常简单了，如下所示： func (d *Driver) NodePublishVolume(ctx context.Context, req *csi.NodePublishVolumeRequest) (*csi.NodePublishVolumeResponse, error) { ... source := req.StagingTargetPath target := req.TargetPath mnt := req.VolumeCapability.GetMount() options := mnt.MountFlag ... if !mounted { ll.Info(\"mounting the volume\") if err := d.mounter.Mount(source, target, fsType, options...); err != nil { return nil, status.Error(codes.Internal, err.Error()) } } else { ll.Info(\"volume is already mounted\") } return &csi.NodePublishVolumeResponse{}, nil } 以看到，在这一步实现中，我们只需要做一步操作，即：将 Staging 目录，绑定挂载到 Volume 对应的宿主机目录上。由于 Staging 目录，正是 Volume 对应的设备被格式化后挂载在宿主机上的位置，所以当它和 Volume 的宿主机目录绑定挂载之后，这个 Volume 宿主机目录的“持久化”处理也就完成了。 当然，我在前面也曾经提到过，对于文件系统类型的存储服务来说，比如 NFS 和 GlusterFS 等，它们并没有一个对应的磁盘“设备”存在于宿主机上，所以 kubelet 在 VolumeManagerReconciler 控制循环中，会跳过 MountDevice 操作而直接执行 SetUp 操作。所以对于它们来说，也就不需要实现 NodeStageVolume 接口了。 在编写完了 CSI 插件之后，我们就可以把这个插件和 External Components 一起部署起来。首先，我们需要创建一个 DigitalOcean client 授权需要使用的 Secret 对象，如下所示： apiVersion: v1 kind: Secret metadata: name: digitalocean namespace: kube-system stringData: access-token: \"a05dd2f26b9b9ac2asdas__REPLACE_ME____123cb5d1ec17513e06da\" 接下来，我们通过一句指令就可以将 CSI 插件部署起来： $ kubectl apply -f https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v0.2.0.yaml 这个 CSI 插件的 YAML 文件的主要内容如下所示： --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: do-block-storage namespace: kube-system annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: com.digitalocean.csi.dobs --- ############################################## ########### ############ ########### Controller plugin ############ ########### ############ ############################################## kind: StatefulSet apiVersion: apps/v1beta1 metadata: name: csi-do-controller namespace: kube-system spec: serviceName: \"csi-do\" replicas: 1 template: metadata: labels: app: csi-do-controller role: csi-do spec: priorityClassName: system-cluster-critical serviceAccount: csi-do-controller-sa containers: - name: csi-provisioner image: quay.io/k8scsi/csi-provisioner:v0.3.0 args: - \"--provisioner=com.digitalocean.csi.dobs\" - \"--csi-address=$(ADDRESS)\" - \"--v=5\" env: - name: ADDRESS value: /var/lib/csi/sockets/pluginproxy/csi.sock imagePullPolicy: \"Always\" volumeMounts: - name: socket-dir mountPath: /var/lib/csi/sockets/pluginproxy/ - name: csi-attacher image: quay.io/k8scsi/csi-attacher:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" env: - name: ADDRESS value: /var/lib/csi/sockets/pluginproxy/csi.sock imagePullPolicy: \"Always\" volumeMounts: - name: socket-dir mountPath: /var/lib/csi/sockets/pluginproxy/ - name: csi-do-plugin image: digitalocean/do-csi-plugin:v0.2.0 args : - \"--endpoint=$(CSI_ENDPOINT)\" - \"--token=$(DIGITALOCEAN_ACCESS_TOKEN)\" - \"--url=$(DIGITALOCEAN_API_URL)\" env: - name: CSI_ENDPOINT value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock - name: DIGITALOCEAN_API_URL value: https://api.digitalocean.com/ - name: DIGITALOCEAN_ACCESS_TOKEN valueFrom: secretKeyRef: name: digitalocean key: access-token imagePullPolicy: \"Always\" volumeMounts: - name: socket-dir mountPath: /var/lib/csi/sockets/pluginproxy/ volumes: - name: socket-dir emptyDir: {} --- apiVersion: v1 kind: ServiceAccount metadata: name: csi-do-controller-sa namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-do-controller-provisioner-binding namespace: kube-system subjects: - kind: ServiceAccount name: csi-do-controller-sa namespace: kube-system roleRef: kind: ClusterRole name: system:csi-external-provisioner apiGroup: rbac.authorization.k8s.io --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-do-controller-attacher-binding namespace: kube-system subjects: - kind: ServiceAccount name: csi-do-controller-sa namespace: kube-system roleRef: kind: ClusterRole name: system:csi-external-attacher apiGroup: rbac.authorization.k8s.io --- ######################################## ########### ############ ########### Node plugin ############ ########### ############ ######################################## kind: DaemonSet apiVersion: apps/v1beta2 metadata: name: csi-do-node namespace: kube-system spec: selector: matchLabels: app: csi-do-node template: metadata: labels: app: csi-do-node role: csi-do spec: priorityClassName: system-node-critical serviceAccount: csi-do-node-sa hostNetwork: true containers: - name: driver-registrar image: quay.io/k8scsi/driver-registrar:v0.3.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" env: - name: ADDRESS value: /csi/csi.sock - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: plugin-dir mountPath: /csi/ # TODO(arslan): the registrar is not implemented yet # - name: registrar-socket-dir # mountPath: /var/lib/csi/sockets/ - name: csi-do-plugin image: digitalocean/do-csi-plugin:v0.2.0 args : - \"--endpoint=$(CSI_ENDPOINT)\" - \"--token=$(DIGITALOCEAN_ACCESS_TOKEN)\" - \"--url=$(DIGITALOCEAN_API_URL)\" env: - name: CSI_ENDPOINT value: unix:///csi/csi.sock - name: DIGITALOCEAN_API_URL value: https://api.digitalocean.com/ - name: DIGITALOCEAN_ACCESS_TOKEN valueFrom: secretKeyRef: name: digitalocean key: access-token imagePullPolicy: \"Always\" securityContext: privileged: true capabilities: add: [\"SYS_ADMIN\"] allowPrivilegeEscalation: true volumeMounts: - name: plugin-dir mountPath: /csi - name: pods-mount-dir mountPath: /var/lib/kubelet # needed so that any mounts setup inside this container are # propagated back to the host machine. mountPropagation: \"Bidirectional\" - name: device-dir mountPath: /dev volumes: # TODO(arslan): the registar is not implemented yet #- name: registrar-socket-dir # hostPath: # path: /var/lib/kubelet/device-plugins/ # type: DirectoryOrCreate - name: plugin-dir hostPath: path: /var/lib/kubelet/plugins/com.digitalocean.csi.dobs type: DirectoryOrCreate - name: pods-mount-dir hostPath: path: /var/lib/kubelet type: Directory - name: device-dir hostPath: path: /dev --- apiVersion: v1 kind: ServiceAccount metadata: name: csi-do-node-sa namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-do-driver-registrar-binding namespace: kube-system subjects: - kind: ServiceAccount name: csi-do-node-sa namespace: kube-system roleRef: kind: ClusterRole name: csi-do-driver-registrar-role apiGroup: rbac.authorization.k8s.io --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: csi-do-driver-registrar-role namespace: kube-system rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"update\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"] 可以看到，我们编写的 CSI 插件只有一个二进制文件，它的镜像是 digitalocean/do-csi-plugin:v0.2.0。 而我们部署 CSI 插件的常用原则是： 第一，通过 DaemonSet 在每个节点上都启动一个 CSI 插件，来为 kubelet 提供 CSI Node 服务。这是因为，CSI Node 服务需要被 kubelet 直接调用，所以它要和 kubelet“一对一”地部署起来。 此外，在上述 DaemonSet 的定义里面，除了 CSI 插件，我们还以 sidecar 的方式运行着 driver-registrar 这个外部组件。它的作用，是向 kubelet 注册这个 CSI 插件。这个注册过程使用的插件信息，则通过访问同一个 Pod 里的 CSI 插件容器的 Identity 服务获取到。 要注意的是，由于 CSI 插件运行在一个容器里，那么 CSI Node 服务在“Mount 阶段”执行的挂载操作，实际上是发生在这个容器的 Mount Namespace 里的。可是，我们真正希望执行挂载操作的对象，都是宿主机 /var/lib/kubelet 目录下的文件和目录。 所以，在定义 DaemonSet Pod 的时候，我们需要把宿主机的 /var/lib/kubelet 以 Volume 的方式挂载进 CSI 插件容器的同名目录下，然后设置这个 Volume 的 mountPropagation=Bidirectional，即开启双向挂载传播，从而将容器在这个目录下进行的挂载操作“传播”给宿主机，反之亦然。 第二，通过 StatefulSet 在任意一个节点上再启动一个 CSI 插件，为 External Components 提供 CSI Controller 服务。所以，作为 CSI Controller 服务的调用者，External Provisioner 和 External Attacher 这两个外部组件，就需要以 sidecar 的方式和这次部署的 CSI 插件定义在同一个 Pod 里。 你可能会好奇，为什么我们会用 StatefulSet 而不是 Deployment 来运行这个 CSI 插件呢。 这是因为，由于 StatefulSet 需要确保应用拓扑状态的稳定性，所以它对 Pod 的更新，是严格保证顺序的，即：只有在前一个 Pod 停止并删除之后，它才会创建并启动下一个 Pod。 而像我们上面这样将 StatefulSet 的 replicas 设置为 1 的话，StatefulSet 就会确保 Pod 被删除重建的时候，永远有且只有一个 CSI 插件的 Pod 运行在集群中。这对 CSI 插件的正确性来说，至关重要。 而在今天这篇文章一开始，我们就已经定义了这个 CSI 插件对应的 StorageClass（即：do-block-storage），所以你接下来只需要定义一个声明使用这个 StorageClass 的 PVC 即可，如下所示： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: do-block-storage 当你把上述 PVC 提交给 Kubernetes 之后，你就可以在 Pod 里声明使用这个 csi-pvc 来作为持久化存储了。 总结 以一个 DigitalOcean 的 CSI 插件为例，和你分享了编写 CSI 插件的具体流程。基于这些讲述，你现在应该已经对 Kubernetes 持久化存储体系有了一个更加全面和深入的认识。举个例子，对于一个部署了 CSI 存储插件的 Kubernetes 集群来说： 当用户创建了一个 PVC 之后，你前面部署的 StatefulSet 里的 External Provisioner 容器，就会监听到这个PVC 的诞生，然后调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 CreateVolume方法，为你创建出对应的 PV。 这时候，运行在 Kubernetes Master 节点上的 Volume Controller，就会通过PersistentVolumeController 控制循环，发现这对新创建出来的 PV 和PVC，并且看到它们声明的是同一个 StorageClass。所以，它会把这一对 PV 和 PVC 绑定起来，使 PVC 进入 Bound状态。 然后，用户创建了一个声明使用上述 PVC 的 Pod，并且这个 Pod 被调度器调度到了宿主机 A 上。这时候，Volume Controller 的 AttachDetachController 控制循环就会发现，上述 PVC 对应的 Volume，需要被 Attach 到宿主机 A 上。所以，AttachDetachController 会创建一个 VolumeAttachment 对象，这个对象携带了宿主机 A 和待处理的 Volume 的名字。 这样，StatefulSet 里的 External Attacher 容器，就会监听到这个 VolumeAttachment 对象的诞生。于是，它就会使用这个对象里的宿主机和 Volume 名字，调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 ControllerPublishVolume 方法，完成“Attach 阶段”。 上述过程完成后，运行在宿主机 A 上的 kubelet，就会通过 VolumeManagerReconciler 控制循环，发现当前宿主机上有一个 Volume 对应的存储设备（比如磁盘）已经被 Attach 到了某个设备目录下。于是 kubelet 就会调用同一台宿主机上的 CSI 插件的 CSI Node 服务的 NodeStageVolume 和 NodePublishVolume 方法，完成这个 Volume 的“Mount 阶段”。 至此，一个完整的持久化 Volume 的创建和挂载流程就结束了。 参考： 张磊 极客时间 深入浅出 kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 09:13:36 "},"存储/kuberentes-storage-volumes-principle.html":{"url":"存储/kuberentes-storage-volumes-principle.html","title":"Volumes 原理","keywords":"","body":"Kubernetes Persistent Volume 原理2. 总结Kubernetes Persistent Volume 原理 tagsstart 存储 tagsstop 在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。 所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：Local Persistent Volume。 过，首先需要明确的是，Local Persistent Volume 并不适用于所有应用。事实上，它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。 其次，相比于正常的 PV，一旦这些节点宕机且不能恢复时，Local Persistent Volume 的数据就可能丢失。这就要求使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。 不难想象，Local Persistent Volume 的设计，主要面临两个难点。 第一个难点在于：如何把本地磁盘抽象成 PV。 可能你会说，Local Persistent Volume，不就等同于 hostPath 加 NodeAffinity 吗？ 比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？ 事实上，你绝不应该把一个宿主机上的目录当作 PV 使用。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。 所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为“一个 PV 一块盘”。 第二个难点在于：调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？ 造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。 可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。 所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。 这个原则，我们可以称为“在调度的时候考虑 Volume 分布”。在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。 基于上述讲述，在开始使用 Local Persistent Volume 之前，你首先需要在集群里配置好磁盘或者块设备。在公有云上，这个操作等同于给虚拟机额外挂载一个磁盘，比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子。而在我们部署的私有环境中，你有两种办法来完成这个步骤。 第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作； 第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。 接下来，我会使用第二种方法，在我们之前部署的 Kubernetes 集群上进行实践。首先，在名叫 node-1 的宿主机上创建一个挂载点，比如 /mnt/disks；然后，用几个 RAM Disk 来模拟本地磁盘，如下所示： # 在node-1上执行 $ mkdir /mnt/disks $ for vol in vol1 vol2 vol3; do mkdir /mnt/disks/$vol mount -t tmpfs $vol /mnt/disks/$vol done 需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，并且确保这些磁盘的名字（vol1、vol2 等）都不重复。 接下来，我们就可以为这些本地磁盘定义对应的 PV 了，如下所示： apiVersion: v1 kind: PersistentVolume metadata: name: example-pv spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/vol1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-1 可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。 当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。这正是 Kubernetes 实现“在调度的时候就考虑 Volume 分布”的主要方法。 接下来，我们就可以使用 kubect create 来创建这个 PV，如下所示： $ kubectl create -f local-pv.yaml persistentvolume/example-pv created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE example-pv 5Gi RWO Delete Available local-storage 16s 可以看到，这个 PV 创建后，进入了 Available（可用）状态。 而正如我在上一篇文章里所建议的那样，使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV，如下所示： kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。 与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：延迟绑定。 我们知道，当你提交了 PV 和 PVC 的 YAML 文件之后，Kubernetes 就会根据它们俩的属性，以及它们指定的 StorageClass 来进行绑定。只有绑定成功后，Pod 才能通过声明这个 PVC 来使用对应的 PV。 可是，如果你使用的是 Local Persistent Volume 的话，就会发现，这个流程根本行不通。比如，现在你有一个 Pod，它声明使用的 PVC 叫作 pvc-1。并且，我们规定，这个 Pod 只能运行在 node-2 上。 而在 Kubernetes 集群中，有两个属性（比如：大小、读写权限）相同的 Local 类型的 PV。其中，第一个 PV 的名字叫作 pv-1，它对应的磁盘所在的节点是 node-1。而第二个 PV 的名字叫作 pv-2，它对应的磁盘所在的节点是 node-2。 假设现在，Kubernetes 的 Volume 控制循环里，首先检查到了 pvc-1 和 pv-1 的属性是匹配的，于是就将它们俩绑定在一起。然后，你用 kubectl create 创建了这个 Pod。这时候，问题就出现了。 调度器看到，这个 Pod 所声明的 pvc-1 已经绑定了 pv-1，而 pv-1 所在的节点是 node-1，根据“调度器必须在调度的时候考虑 Volume 分布”的原则，这个 Pod 自然会被调度到 node-1 上。 可是，我们前面已经规定过，这个 Pod 根本不允许运行在 node-1 上。所以。最后的结果就是，这个 Pod 的调度必然会失败。这就是为什么，在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。 那么，具体推迟到什么时候呢？答案是：推迟到调度的时候。 所以说，StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义，就是告诉 Kubernetes 里的 Volume 控制循环（“红娘”）：虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但请不要现在就执行绑定操作（即：设置 PVC 的 VolumeName 字段）。 而要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。这样，在上面的例子里，由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1，所以它的 PVC 最后会跟 pv-2 绑定，并且 Pod 也会被调度到 node-2 上。 所以，通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。 当然，在具体实现中，调度器实际上维护了一个与 Volume Controller 类似的控制循环，专门负责为那些声明了“延迟绑定”的 PV 和 PVC 进行绑定工作。通过这样的设计，这个额外的绑定操作，并不会拖慢调度器的性能。而当一个 Pod 的 PVC 尚未完成绑定时，调度器也不会等待，而是会直接把这个 Pod 重新放回到待调度队列，等到下一个调度周期再做处理。在明白了这个机制之后，我们就可以创建 StorageClass 了，如下所示： $ kubectl create -f local-sc.yaml storageclass.storage.k8s.io/local-storage created 接下来，我们只需要定义一个非常普通的 PVC，就可以让 Pod 使用到上面定义好的 Local Persistent Volume 了，如下所示： kind: PersistentVolumeClaim apiVersion: v1 metadata: name: example-local-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage 可以看到，这个 PVC 没有任何特别的地方。唯一需要注意的是，它声明的 storageClassName 是 local-storage。所以，将来 Kubernetes 的 Volume Controller 看到这个 PVC 的时候，不会为它进行绑定操作。现在，我们来创建这个 PVC： $ kubectl create -f local-pvc.yaml persistentvolumeclaim/example-local-claim created $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-local-claim Pending local-storage 7s 可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。然后，我们编写一个 Pod 来声明使用这个 PVC，如下所示： kind: Pod apiVersion: v1 metadata: name: example-pv-pod spec: volumes: - name: example-pv-storage persistentVolumeClaim: claimName: example-local-claim containers: - name: example-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: example-pv-storage 这个 Pod 没有任何特别的地方，你只需要注意，它的 volumes 字段声明要使用前面定义的、名叫 example-local-claim 的 PVC 即可。而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示： $ kubectl create -f local-pod.yaml pod/example-pv-pod created $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-local-claim Bound example-pv 5Gi RWO local-storage 6h 也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如： $ kubectl exec -it example-pv-pod -- /bin/sh # cd /usr/share/nginx/html # touch test.txt 然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件： # 在node-1上 $ ls /mnt/disks/vol1 test.txt 而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中： $ kubectl delete -f local-pod.yaml $ kubectl create -f local-pod.yaml $ kubectl exec -it example-pv-pod -- /bin/sh # ls /usr/share/nginx/html # touch test.txt 这就说明，像 Kubernetes 这样构建出来的、基于本地存储的 Volume，完全可以提供容器持久化存储的功能。所以，像 StatefulSet 这样的有状态编排工具，也完全可以通过声明 Local 类型的 PV 和 PVC，来管理应用的存储状态。 需要注意的是，我们上面手动创建 PV 的方式，即 Static 的 PV 管理方式，在删除 PV 时需要按如下流程执行操作： 删除使用这个 PV 的 Pod； 从宿主机移除本地磁盘（比如，umount 它）； 删除 PVC； 删除 PV。 如果不按照这个流程的话，这个 PV 的删除就会失败。当然，由于上面这些创建 PV 和删除 PV 的操作比较繁琐，Kubernetes 其实提供了一个 Static Provisioner 来帮助你管理这些 PV。 比如，我们现在的所有磁盘，都挂载在宿主机的 /mnt/disks 目录下。 那么，当 Static Provisioner 启动后，它就会通过 DaemonSet，自动检查每个宿主机的 /mnt/disks 目录。然后，调用 Kubernetes API，为这些目录下面的每一个挂载，创建一个对应的 PV 对象出来。这些自动创建的 PV，如下所示： $ kubectl get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE local-pv-ce05be60 1024220Ki RWO Delete Available local-storage 26s $ kubectl describe pv local-pv-ce05be60 Name: local-pv-ce05be60 ... StorageClass: local-storage Status: Available Claim: Reclaim Policy: Delete Access Modes: RWO Capacity: 1024220Ki NodeAffinity: Required Terms: Term 0: kubernetes.io/hostname in [node-1] Message: Source: Type: LocalVolume (a persistent volume backed by local storage on a node) Path: /mnt/disks/vol1 这个 PV 里的各种定义，比如 StorageClass 的名字、本地磁盘挂载点的位置，都可以通过 provisioner 的配置文件指定。当然，provisioner 也会负责前面提到的 PV 的删除工作。 而这个 provisioner 本身，其实也是一个我们前面提到过的External Provisioner，它的部署方法，在对应的文档里有详细描述。这部分内容，就留给你课后自行探索了。 2. 总结 可以看到，正是通过 PV 和 PVC，以及 StorageClass 这套存储体系，这个后来新添加的持久化存储方案，对 Kubernetes 已有用户的影响，几乎可以忽略不计。作为用户，你的 Pod 的 YAML 和 PVC 的 YAML，并没有任何特殊的改变，这个特性所有的实现只会影响到 PV 的处理，也就是由运维人员负责的那部分工作。而这，正是这套存储体系带来的“解耦”的好处。其实，Kubernetes 很多看起来比较“繁琐”的设计（比如“声明式 API”，以及我今天讲解的“PV、PVC 体系”）的主要目的，都是希望为开发者提供更多的“可扩展性”，给使用者带来更多的“稳定性”和“安全感”。这两个能力的高低，是衡量开源基础设施项目水平的重要标准。 参考： 张磊 极客时间 深入浅出 kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 10:16:48 "},"策略/":{"url":"策略/","title":"策略","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 02:59:28 "},"策略/OPA/":{"url":"策略/OPA/","title":"OPA","keywords":"","body":"Open Policy Agent(OPA)1. OPA 介绍2. OPA 解决了哪些问题3. rego介绍4. OPA 安装5. OPA 运行6. OPA run（互动式）7. OPA run（服务器）8. Rego 语法8.1 参考8.2 表达式（逻辑与）8.3 逻辑或8.4 Variables变量8.5 迭代8.6 规则8.7 语法示例9. 将 OPA 用作Go库Open Policy Agent(OPA) tagsstart OPA 策略 tagsstop 1. OPA 介绍 开放策略代理（OPA，发音为“ oh-pa”）是一个开放源代码的通用策略引擎，它统一了整个堆栈中的策略执行。OPA提供了一种高级的声明性语言，使您可以将策略指定为代码和简单的API，以减轻软件决策的负担。您可以使用OPA在微服务，Kubernetes，CI / CD管道，API网关等中实施策略。 OPA 最初是由 Styra 公司在 2016 年创建并开源的项目，目前该公司的主要产品就是提供可视化策略控制及策略执行的可视化 Dashboard 服务的。 OPA 首次进入 CNCF 并成为 sandbox 级别的项目是在 2018 年， 在 2021 年的 2 月份便已经从 CNCF 毕业，这个过程相对来说还是比较快的，由此也可以看出 OPA 是一个比较活跃且应用广泛的项目。 透过现象看本质，策略就是一组规则，请求发送到引擎，引擎根据规则来进行决策。OPA 并不负责具体任务的执行，它仅负责决策，需要决策的请求通过 JSON 的方式传递给 OPA ，在 OPA 决策后，也会将结果以 JSON 的形式返回。 2. OPA 解决了哪些问题 OPA通过评估查询输入以及针对策略和数据来生成策略决策。OPA和Rego是域无关的，因此您可以描述策略中几乎所有类型的不变式。例如： 哪些用户可以访问哪些资源； 允许哪些子网出口流量； 必须将工作负载部署到哪个群集； 可以从哪些注册表二进制文件下载； 容器可以执行哪些OS功能； 可以在一天的哪个时间访问系统； 需要策略控制用户是否可登陆服务器或者做一些操作； 需要策略控制哪些项目/哪些组件可进行部署； 需要策略控制如何访问数据库； 10.需要策略控制哪些资源可部署到 Kubernetes 中； 策略决策不仅限于简单的是/否或允许/拒绝答案。像查询输入一样，您的策略可以生成任意结构化数据作为输出。 但是对于这些场景或者软件来说，配置它们的策略是需要与该软件进行耦合的，彼此是不统一，不通用的。管理起来也会比较混乱，带来了不小的维护成本。 OPA 的出现可以将各处配置的策略进行统一，极大的降低了维护成本。以及将策略与对应的软件/服务进行解耦，方便进行移植/复用。 假设您在具有以下系统的组织中工作： 系统中包含三种组件： 服务器暴露零层或多个协议（例如，http，ssh等等） 网络连接服务器，可以是公共的或私有的。公共网络已连接到Internet。 端口将服务器连接到网络。 所有服务器，网络和端口均由脚本设置。该脚本接收系统的JSON表示作为输入： { \"servers\": [ {\"id\": \"app\", \"protocols\": [\"https\", \"ssh\"], \"ports\": [\"p1\", \"p2\", \"p3\"]}, {\"id\": \"db\", \"protocols\": [\"mysql\"], \"ports\": [\"p3\"]}, {\"id\": \"cache\", \"protocols\": [\"memcache\"], \"ports\": [\"p3\"]}, {\"id\": \"ci\", \"protocols\": [\"http\"], \"ports\": [\"p1\", \"p2\"]}, {\"id\": \"busybox\", \"protocols\": [\"telnet\"], \"ports\": [\"p1\"]} ], \"networks\": [ {\"id\": \"net1\", \"public\": false}, {\"id\": \"net2\", \"public\": false}, {\"id\": \"net3\", \"public\": true}, {\"id\": \"net4\", \"public\": true} ], \"ports\": [ {\"id\": \"p1\", \"network\": \"net1\"}, {\"id\": \"p2\", \"network\": \"net3\"}, {\"id\": \"p3\", \"network\": \"net2\"} ] } 当天早些时候，您的老板告诉您必须实施的新安全策略： Servers reachable from the Internet must not expose the insecure 'http' protocol. 从Internet可访问的服务器不能暴露不安全的“http”协议。 Servers are not allowed to expose the 'telnet' protocol. 服务器不允许公开'telnet'协议 配置了服务器，网络和端口并且合规团队希望定期审核系统以查找违反策略的服务器时，必须执行该策略。 您的老板已要求您确定OPA是否适合实施该政策。 3. rego介绍 OPA 中的策略是以 Rego 这种 DSL(Domain Specific Language) 来表示的。 Rego 受 Datalog（https://en.wikipedia.org/wiki/Datalog） 的启发，并且扩展了 Datalog 对于结构化文档模型的支持，方便以 JSON 的方式对数据进行处理。 Rego 允许策略制定者可以专注于返回内容的查询而不是如何执行查询。同时 OPA 中也内置了执行规则时的优化，用户可以默认使用。 Rego 允许我们使用规则（if-then）封装和重用逻辑，规则可以是完整的或者是部分的。 每个规则都是由头部和主体组成。在 Rego 中，如果规则主体对于某些变量赋值为真，那么我们说规则头为真。可以通过绝对路径引用任何加载到 OPA 中的规则来查询它的值。规则的路径总是：data..（规则生成的所有值都可以通过全局 data 变量进行查询。例如，下方示例中的 data.example.rules.any_public_networks 完整规则是将单个值分配给变量的 if-then 语句。 部分规则是生成一组值并将该组分配给变量的 if-then 语句 逻辑或是要在 Rego 中定义多个具有相同名称的规则。（查询中将多个表达式连接在一起时，表示的是逻辑 AND） 4. OPA 安装 本节说明如何直接查询OPA并在自己的计算机上与其交互。 1.下载OPA 要开始从GitHub版本下载适用于您平台的OPA二进制文件，请执行以下操作： 在macOS（64位）上： curl -L -o opa https://openpolicyagent.org/downloads/v0.28.0/opa_darwin_amd64 在Linux（64位）上： curl -L -o opa https://openpolicyagent.org/downloads/v0.28.0/opa_linux_amd64 chmod 755 ./opa cp opa /usr/local/bin/ $ opa version Version: 0.35.0 Build Commit: a54537a Build Timestamp: 2021-12-01T02:11:47Z Build Hostname: 9e4cf671a460 Go Version: go1.17.3 WebAssembly: unavailable 容器 $ docker run --rm openpolicyagent/opa:0.35.0 version Version: 0.35.0 Build Commit: a54537a Build Timestamp: 2021-12-01T02:10:31Z Build Hostname: 4ee9b086e5de Go Version: go1.17.3 WebAssembly: available 5. OPA 运行 与OPA交互的最简单方法是通过命令行使用opa eval子命令。opa eval是一把瑞士军刀，可用于评估任意的Rego表达式和策略。opa eval支持大量用于控制评估的选项。常用标志包括： 旗帜 短的 描述 --bundle -b 将捆绑包文件或目录加载到OPA中。该标志可以重复。 --data -d 将策略或数据文件加载到OPA中。该标志可以重复。 --input -i 加载数据文件并将其用作input。该标志不能重复。 --format -f 设置要使用的输出格式。默认值为json，并且旨在用于程序设计。该pretty格式发出了更多人类可读的输出。 --fail 不适用 如果查询未定义，则以非零的退出代码退出。 --fail-defined 不适用 如果查询不是未定义的，则以非零的退出代码退出。 例如： input.json： { \"servers\": [ {\"id\": \"app\", \"protocols\": [\"https\", \"ssh\"], \"ports\": [\"p1\", \"p2\", \"p3\"]}, {\"id\": \"db\", \"protocols\": [\"mysql\"], \"ports\": [\"p3\"]}, {\"id\": \"cache\", \"protocols\": [\"memcache\"], \"ports\": [\"p3\"]}, {\"id\": \"ci\", \"protocols\": [\"http\"], \"ports\": [\"p1\", \"p2\"]}, {\"id\": \"busybox\", \"protocols\": [\"telnet\"], \"ports\": [\"p1\"]} ], \"networks\": [ {\"id\": \"net1\", \"public\": false}, {\"id\": \"net2\", \"public\": false}, {\"id\": \"net3\", \"public\": true}, {\"id\": \"net4\", \"public\": true} ], \"ports\": [ {\"id\": \"p1\", \"network\": \"net1\"}, {\"id\": \"p2\", \"network\": \"net3\"}, {\"id\": \"p3\", \"network\": \"net2\"} ] } example.rego: package example default allow = false # unless otherwise defined, allow is false allow = true { # allow is true if... count(violation) == 0 # there are zero violations. } violation[server.id] { # a server is in the violation set if... some server public_server[server] # it exists in the 'public_server' set and... server.protocols[_] == \"http\" # it contains the insecure \"http\" protocol. } violation[server.id] { # a server is in the violation set if... server := input.servers[_] # it exists in the input.servers collection and... server.protocols[_] == \"telnet\" # it contains the \"telnet\" protocol. } public_server[server] { # a server exists in the public_server set if... some i, j server := input.servers[_] # it exists in the input.servers collection and... server.ports[_] == input.ports[i].id # it references a port in the input.ports collection and... input.ports[i].network == input.networks[j].id # the port references a network in the input.networks collection and... input.networks[j].public # the network is public. } 执行： root@master:~/cks/opa# ./opa eval \"1*2+3\" { \"result\": [ { \"expressions\": [ { \"value\": 5, \"text\": \"1*2+3\", \"location\": { \"row\": 1, \"col\": 1 } } ] } ] } root@master:~/cks/opa# ./opa eval -i input.json -d example.rego \"data.example.violation[x]\" { \"result\": [ { \"expressions\": [ { \"value\": \"ci\", \"text\": \"data.example.violation[x]\", \"location\": { \"row\": 1, \"col\": 1 } } ], \"bindings\": { \"x\": \"ci\" } }, { \"expressions\": [ { \"value\": \"busybox\", \"text\": \"data.example.violation[x]\", \"location\": { \"row\": 1, \"col\": 1 } } ], \"bindings\": { \"x\": \"busybox\" } } ] } root@master:~/cks/opa# ./opa eval --fail-defined -i input.json -d example.rego \"data.example.violation[x]\" { \"result\": [ { \"expressions\": [ { \"value\": \"ci\", \"text\": \"data.example.violation[x]\", \"location\": { \"row\": 1, \"col\": 1 } } ], \"bindings\": { \"x\": \"ci\" } }, { \"expressions\": [ { \"value\": \"busybox\", \"text\": \"data.example.violation[x]\", \"location\": { \"row\": 1, \"col\": 1 } } ], \"bindings\": { \"x\": \"busybox\" } } ] } root@master:~/cks/opa# echo $? 1 6. OPA run（互动式） OPA包括一个交互式外壳程序或REPL（读取-评估-打印循环）。您可以使用REPL来试验策略并为新策略创建原型。 要启动REPL，只需： ./opa run 当您在REPL中输入语句时，OPA会对它们进行评估并打印结果。 > true true > 3.14 3.14 > [\"hello\", \"world\"] [ \"hello\", \"world\" ] 大多数REPL允许您定义以后可以引用的变量。OPA允许您执行类似的操作。例如，您可以定义一个pi常量，如下所示： > pi := 3.14 定义“ pi”后，您将查询该值并根据该值编写表达式： > pi 3.14 > pi > 3 true 通过按Control-D或键入exit以下命令退出REPL ： > exit 您可以通过在命令行上传递策略和数据文件来将它们加载到REPL中。默认情况下，JSON和YAML文件植于下data。 opa run input.json 运行一些查询以查找数据： > data.server[0].protocols[1] undefined > data.servers[0].protocols[1] \"ssh\" > data.servers[i].protocols[j] +---+---+------------------------------+ | i | j | data.servers[i].protocols[j] | +---+---+------------------------------+ | 0 | 0 | \"https\" | | 0 | 1 | \"ssh\" | | 1 | 0 | \"mysql\" | | 2 | 0 | \"memcache\" | | 3 | 0 | \"http\" | | 4 | 0 | \"telnet\" | +---+---+------------------------------+ > net := data.networks[_]; net.public +-----------------------------+ | net | +-----------------------------+ | {\"id\":\"net3\",\"public\":true} | | {\"id\":\"net4\",\"public\":true} | +-----------------------------+ 要将数据文件设置为inputREPL中的文档，请在文件路径前添加前缀： opa run example.rego repl.input:input.json > data.example.public_server[s] +-------------------------------------------------------------------+-------------------------------------------------------------------+ | s | data.example.public_server[s] | +-------------------------------------------------------------------+-------------------------------------------------------------------+ | {\"id\":\"app\",\"ports\":[\"p1\",\"p2\",\"p3\"],\"protocols\":[\"https\",\"ssh\"]} | {\"id\":\"app\",\"ports\":[\"p1\",\"p2\",\"p3\"],\"protocols\":[\"https\",\"ssh\"]} | | {\"id\":\"ci\",\"ports\":[\"p1\",\"p2\"],\"protocols\":[\"http\"]} | {\"id\":\"ci\",\"ports\":[\"p1\",\"p2\"],\"protocols\":[\"http\"]} | +-------------------------------------------------------------------+-------------------------------------------------------------------+ 7. OPA run（服务器） 要与OPA集成，您可以将其作为服务器运行并通过HTTP执行查询。您可以使用-s或将OPA作为服务器启动--server： ./opa run --server ./example.rego 默认情况下，OPA在上侦听HTTP连接0.0.0.0:8181。请参阅参考资料opa run --help，以获取用于更改侦听地址，启用TLS等的选项的列表。 在另一个终端内部使用curl（或类似的工具）来访问OPA的HTTP API。查询/v1/dataHTTP API时，必须将输入数据包装在JSON对象内： { \"input\": } 创建输入文件的副本，以通过发送curl： cat v1-data-input.json { \"input\": $(cat input.json) } EOF 执行一些curl请求并检查输出： curl localhost:8181/v1/data/example/violation -d @v1-data-input.json -H 'Content-Type: application/json' curl localhost:8181/v1/data/example/allow -d @v1-data-input.json -H 'Content-Type: application/json' 默认情况下data.system.main，用于不带路径的策略查询。当您在不提供路径的情况下执行查询时，不必包装输入。如果该data.system.main决定未定义，则将其视为错误： curl localhost:8181 -i -d @input.json -H 'Content-Type: application/json' 您可以重新启动OPA并将其配置为使用任何决策作为默认决策： ./opa run --server --set=default_decision=example/allow ./example.rego curl从上面重新运行最后一个命令： curl localhost:8181 -i -d @input.json -H 'Content-Type: application/json' 8. Rego 语法 OPA策略以称为Rego的高级声明性语言表示。Rego（发音为“ ray-go”）是专门为在复杂的分层数据结构上表达策略而构建的。有关Rego的详细信息，请参阅策略语言文档。 below以下示例是交互式的！如果在包含服务器，网络和端口的上方编辑输入数据，则输出将在下面更改。同样，如果您在下面的示例中编辑查询或规则，则输出将更改。在通读本节时，请尝试更改输入，查询和规则，并观察输出的差异。 也可以使用以下命令在您的计算机上本地运行它们opa eval，这是设置说明。 8.1 参考 当OPA评估策略时，它将查询中提供的数据绑定到名为的全局变量input。您可以使用.（点）运算符在输入中引用数据。 input.servers [ { \"id\": \"app\", \"ports\": [ \"p1\", \"p2\", \"p3\" ], \"protocols\": [ \"https\", \"ssh\" ] }, { \"id\": \"db\", \"ports\": [ \"p3\" ], \"protocols\": [ \"mysql\" ] }, { \"id\": \"cache\", \"ports\": [ \"p3\" ], \"protocols\": [ \"memcache\" ] }, { \"id\": \"ci\", \"ports\": [ \"p1\", \"p2\" ], \"protocols\": [ \"http\" ] }, { \"id\": \"busybox\", \"ports\": [ \"p1\" ], \"protocols\": [ \"telnet\" ] } ] 要引用数组元素，可以使用熟悉的方括号语法： input.servers[0].protocols[0] \"https\" keys如果键包含以外的其他字符，则可以使用相同的方括号语法 [a-zA-Z0-9_]。例如input[\"foo~bar\"]。 如果引用的值不存在，则OPA返回undefined。未定义表示OPA无法找到任何结果。 input.deadbeef undefined decision 8.2 表达式（逻辑与） 要在Rego中制定政策决策，您要针对输入和其他数据编写表达式。 input.servers[0].id == \"app\" true OPA包括一组内置函数，可用于执行常见操作，例如字符串操作，正则表达式匹配，算术，聚合等。 count(input.servers[0].protocols) >= 1 true 有关现成的OPA支持的内置功能的完整列表，请参阅“策略参考”页面。 多个表达式通过;（AND）运算符连接在一起。为了使查询产生结果，查询中的所有表达式必须为真或已定义。表达式的顺序无关紧要。 input.servers[0].id == \"app\"; input.servers[0].protocols[0] == \"https\" true 您可以;通过将表达式分成多行来省略（AND）运算符。以下查询与上一个查询具有相同的含义： input.servers[0].id == \"app\" input.servers[0].protocols[0] == \"https\" true 如果查询中的任何表达式都不为真（或未定义），则结果为未定义。在下面的示例中，第二个表达式为false： input.servers[0].id == \"app\" input.servers[0].protocols[0] == \"telnet\" undefined decision 8.3 逻辑或 在查询中将多个表达式连接在一起时，您表示的是逻辑与。要在Rego中表达逻辑OR，您可以定义多个具有相同名称的规则。让我们来看一个例子。 想象一下，您想知道是否有任何服务器公开允许客户端外壳访问的协议。为了确定这一点，你可以定义声明了一个完整的规则 shell_accessible是true，如果任何服务器暴露\"telnet\"或\"ssh\" 协议： package example.logical_or default shell_accessible = false shell_accessible = true { input.servers[_].protocols[_] == \"telnet\" } shell_accessible = true { input.servers[_].protocols[_] == \"ssh\" } { \"servers\": [ { \"id\": \"busybox\", \"protocols\": [\"http\", \"telnet\"] }, { \"id\": \"web\", \"protocols\": [\"https\"] } ] } shell_accessible true defaultkeyword如果未定义具有相同名称的所有其他规则，则该关键字告诉OPA为该变量分配一个值。 当您将逻辑或与部分规则一起使用时，每个规则定义都会影响分配给变量的一组值。例如，可以将上面的示例修改为生成一组公开\"telnet\"或 的服务器\"ssh\"。 package example.logical_or shell_accessible[server.id] { server := input.servers[_] server.protocols[_] == \"telnet\" } shell_accessible[server.id] { server := input.servers[_] server.protocols[_] == \"ssh\" } { \"servers\": [ { \"id\": \"busybox\", \"protocols\": [\"http\", \"telnet\"] }, { \"id\": \"db\", \"protocols\": [\"mysql\", \"ssh\"] }, { \"id\": \"web\", \"protocols\": [\"https\"] } ] } shell_accessible [ \"busybox\", \"db\" ] 8.4 Variables变量 您可以使用:=（赋值）运算符将值存储在中间变量中。可以像一样引用变量input s := input.servers[0] s.id == \"app\" p := s.protocols[0] p == \"https\" +---------+-------------------------------------------------------------------+ | p | s | +---------+-------------------------------------------------------------------+ | \"https\" | {\"id\":\"app\",\"ports\":[\"p1\",\"p2\",\"p3\"],\"protocols\":[\"https\",\"ssh\"]} | +---------+-------------------------------------------------------------------+ 当OPA评估表达式时，它将查找使所有表达式都为真的变量的值。如果没有使所有表达式都为真的变量赋值，则结果是不确定的。 s := input.servers[0] s.id == \"app\" s.protocols[1] == \"telnet\" undefined decision 变量是不可变的。如果您尝试两次分配相同的变量，OPA将报告错误。 s := input.servers[0] s := input.servers[1] 1 error occurred: 2:1: rego_compile_error: var s assigned above OPA必须能够枚举所有表达式中所有变量的值。如果OPA无法枚举任何表达式中变量的值，则OPA将报告错误。 x := 1 x != y # y has not been assigned a value 2 errors occurred: 2:1: rego_unsafe_var_error: var _ is unsafe 2:1: rego_unsafe_var_error: var y is unsafe 8.5 迭代 像其他声明性语言（例如SQL）一样，Rego没有显式的循环或迭代构造。而是将变量注入表达式中时，隐式发生迭代。 要了解Rego中迭代的工作原理，请想象您需要检查是否有任何公共网络。回想一下，网络是在数组中提供的： input.networks [ { \"id\": \"net1\", \"public\": false }, { \"id\": \"net2\", \"public\": false }, { \"id\": \"net3\", \"public\": true }, { \"id\": \"net4\", \"public\": true } ] 一种选择是测试输入中的每个网络： input.networks[0].public == true false input.networks[1].public == true false input.networks[2].public == true true 这种方法是有问题的，因为可能有太多网络无法静态列出，或更重要的是，可能无法事先知道网络的数量。 在Rego中，解决方案是将数组索引替换为变量。 some i; input.networks[i].public == true +---+ | i | +---+ | 2 | | 3 | +---+ 现在，查询将要求该值i使整个表达式为真。当您在引用中替换变量时，OPA会自动查找满足查询中所有表达式的变量分配。就像中间变量一样，OPA返回变量的值。 您可以根据需要替换任意多个变量。例如，要确定是否有服务器公开了不安全的\"http\"协议，您可以编写： some i, j; input.servers[i].protocols[j] == \"http\" +---+---+ | i | j | +---+---+ | 3 | 0 | +---+---+ 如果变量出现多次，则分配满足所有表达式。例如，要查找连接到公共网络的端口的ID，可以编写： some i, j id := input.ports[i].id input.ports[i].network == input.networks[j].id input.networks[j].public +---+------+---+ | i | id | j | +---+------+---+ | 1 | \"p2\" | 2 | +---+------+---+ 为变量提供好名字可能很难。如果仅引用一次变量，则可以将其替换为特殊的（通配符变量）运算符。从概念上讲，的每个实例都是一个唯一变量。 input.servers[_].protocols[_] == \"http\" true 就像引用不存在的字段或无法匹配的表达式的引用一样，如果OPA无法找到满足所有表达式的任何变量赋值，则结果是不确定的。 some i; input.servers[i].protocols[i] == \"ssh\" # there is no assignment of i that satisfies the expression undefined decision 8.6 规则 Rego使您可以使用规则封装和重用逻辑。规则只是if-then逻辑语句。规则可以是“完整”或“部分”。 8.6.1 完整规则 完整的规则是if-then语句，这些语句将单个值分配给变量。例如： package example.rules any_public_networks = true { # is true if... net := input.networks[_] # some network exists and.. net.public # it is public. } 每条规则都由一个头和一个身体组成。在Rego中，如果规则主体对于某些变量分配集为true，则说规则标题为true 。在上面的示例any_public_networks = true中，头是net := input.networks[_]; net.public是身体。 您可以查询规则生成的值，就像其他任何值一样： any_public_networks true 规则生成的所有值都可以通过全局data变量查询。 data.example.rules.any_public_networks true 您可以通过使用绝对路径引用OPA加载的任何规则来查询其值。规则的路径始终为： data..。 如果您省略= 规则标题的一部分，则该值默认为true。您可以按以下方式重写上面的示例，而无需更改其含义： package example.rules any_public_networks { net := input.networks[_] net.public } 要定义常量，请省略规则主体。省略规则正文时，默认为true。由于规则主体为true，因此规则标头始终为true / defined。 package example.constants pi = 3.14 可以像其他任何值一样查询这样定义的常量： pi > 3 true 如果OPA无法找到满足规则主体的变量分配，则可以说该规则是未定义的。例如，如果input提供给OPA的不包括公共网络，any_public_networks则将是未定义的（与false相同）。下面，为OPA提供了一组不同的输入网络（都不是公共的）： { \"networks\": [ {\"id\": \"n1\", \"public\": false}, {\"id\": \"n2\", \"public\": false} ] } any_public_networks undefined decision 8.6.2 部分规则 部分规则是if-then语句，它们生成一组值并将该组值分配给变量。例如： package example.rules public_network[net.id] { # net.id is in the public_network set if... net := input.networks[_] # some network exists and... net.public # it is public. } 在上面的示例中public_network[net.id]是规则头，并且net := input.networks[_]; net.public是规则主体。您可以像查询其他任何值一样查询整个值集： public_network [ \"net3\", \"net4\" ] 您可以通过使用变量引用set元素来遍历值集： some n; public_network[n] +--------+-------------------+ | n | public_network[n] | +--------+-------------------+ | \"net3\" | \"net3\" | | \"net4\" | \"net4\" | +--------+-------------------+ 最后，您可以使用相同的语法检查集合中是否存在值： public_network[\"net3\"] \"net3\" 除了部分定义集合外，您还可以部分定义键/值对（也称为对象）。有关更多信息，请参见 语言指南中的规则。 8.7 语法示例 以上各节介绍了Rego的核心概念。综上所述，让我们回顾一下所需的策略（英语）： Servers reachable from the Internet must not expose the insecure 'http' protocol. 从Internet可访问的服务器不能暴露不安全的“http”协议。 Servers are not allowed to expose the 'telnet' protocol. 服务器不允许公开'telnet'协议。 在较高级别，该策略需要识别违反某些条件的服务器。为了实施此策略，我们可以定义称为的规则violation ，这些规则生成一组违反的服务器。 例如： package example allow = true { # allow is true if... count(violation) == 0 # there are zero violations. } violation[server.id] { # a server is in the violation set if... some server public_server[server] # it exists in the 'public_server' set and... server.protocols[_] == \"http\" # it contains the insecure \"http\" protocol. } violation[server.id] { # a server is in the violation set if... server := input.servers[_] # it exists in the input.servers collection and... server.protocols[_] == \"telnet\" # it contains the \"telnet\" protocol. } public_server[server] { # a server exists in the public_server set if... some i, j server := input.servers[_] # it exists in the input.servers collection and... server.ports[_] == input.ports[i].id # it references a port in the input.ports collection and... input.ports[i].network == input.networks[j].id # the port references a network in the input.networks collection and... input.networks[j].public # the network is public. } some x; violation[x] +-----------+--------------+ | x | violation[x] | +-----------+--------------+ | \"ci\" | \"ci\" | | \"busybox\" | \"busybox\" | +-----------+--------------+ 9. 将 OPA 用作Go库 OPA可以作为库嵌入到Go程序中。将OPA嵌入为库的最简单方法是导入github.com/open-policy-agent/opa/rego 软件包。 import \"github.com/open-policy-agent/opa/rego\" 调用该rego.New函数以创建可以准备或评估的对象： r := rego.New( rego.Query(\"x = data.example.allow\"), rego.Load([]string{\"./example.rego\"}, nil)) 支持多种选项自定义的评价。有关详细信息，请参见GoDoc页面。构造新rego.Rego对象后，您可以调用 PrepareForEval()以获得可执行查询。如果PrepareForEval()失败，则表明传递给rego.New()调用的选项之一无效（例如，解析错误，编译错误等） ctx := context.Background() query, err := r.PrepareForEval(ctx) if err != nil { // handle error } 可以将准备好的查询对象缓存在内存中，在多个goroutine中共享，并使用不同的输入重复调用。调用Eval()以执行准备好的查询。 bs, err := ioutil.ReadFile(\"./input.json\") if err != nil { // handle error } var input interface{} if err := json.Unmarshal(bs, &input); err != nil { // handle error } rs, err := query.Eval(ctx, rego.EvalInput(input)) if err != nil { // handle error } 该策略决策包含在Eval()调用返回的结果中。您可以检查该决定并进行相应处理： // In this example we expect a single result (stored in the variable 'x'). fmt.Println(\"Result:\", rs[0].Bindings[\"x\"]) 您可以将上述步骤组合到一个简单的命令行程序中，该程序可以评估策略并输出结果： main.go： package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/open-policy-agent/opa/rego\" ) func main() { ctx := context.Background() // Construct a Rego object that can be prepared or evaluated. r := rego.New( rego.Query(os.Args[2]), rego.Load([]string{os.Args[1]}, nil)) // Create a prepared query that can be evaluated. query, err := r.PrepareForEval(ctx) if err != nil { log.Fatal(err) } // Load the input document from stdin. var input interface{} dec := json.NewDecoder(os.Stdin) dec.UseNumber() if err := dec.Decode(&input); err != nil { log.Fatal(err) } // Execute the prepared query. rs, err := query.Eval(ctx, rego.EvalInput(input)) if err != nil { log.Fatal(err) } // Do something with the result. fmt.Println(rs) } 运行以下代码，如下所示： go run main.go example.rego 'data.example.violation' 参考: openpolicyagent官网 Open Policy Agent: What Is OPA and How It Works (Examples) Open Policy Agent: Authorization in a Cloud Native World moeLove Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-16 11:11:04 "},"策略/kubernetes-policy-ResourceQuota.html":{"url":"策略/kubernetes-policy-ResourceQuota.html","title":"ResourceQuota","keywords":"","body":"Kubernetes 资源管理策略 ResourceQuota1. 简介2. 场景3. 架构图4. 启用资源配额5. 计算资源配额6. 资源的资源配额7. 存储资源配额8. 对象计数配额9. 配额范围10. pod 配置 PriorityClass 优先级消耗资源11. 跨命名空间 Pod Affinity Quota12. 创建与查看 ResourceQuota12.1 创建12.2 查看13. 默认限制优先级消耗14. 实例14.1 pod 请求或限制 cpu、mem 超出资源配置Kubernetes 资源管理策略 ResourceQuota tagsstart ResourceQuota 策略 tagsstop 《死亡诗社》讲述了威尔顿预科学院一向都是以传统、守旧的方法来教授学生，可是新学期来校的新文学老师基廷（Keating）却一改学校的常规，让自己班上的学生们解放思想，充分发挥学生们的能力。告诉学生们要“活在当下”，并以该原则行事。 1. 简介 简而言之，ResourceQuota 提供了限制每个命名空间的资源消耗的约束。它们只能应用于命名空间级别，这意味着它们可以应用于计算资源并限制命名空间内的对象数量。 Kubernetes 资源配额由ResourceQuota对象定义。当应用于命名空间时，它可以限制 CPU 和内存等计算资源以及以下对象的创建： Pods Services Secrets Persistent Volume Claims (PVCs) ConfigMaps 2. 场景 当多个用户或团队共享具有固定数量节点的集群时，一个团队可能会使用超过其公平份额的资源。 资源配额是管理员解决此问题的工具。 由ResourceQuota对象定义的资源配额提供限制每个命名空间的聚合资源消耗的约束。它可以按类型限制可以在命名空间中创建的对象数量，以及该命名空间中的资源可能消耗的计算资源总量。 资源配额的工作方式如下： 不同的团队在不同的命名空间中工作。这可以通过RBAC强制执行。 管理员为每个命名空间创建一个 ResourceQuota。 用户在命名空间中创建资源（pod、服务等），配额系统会跟踪使用情况，以确保它不超过 ResourceQuota 中定义的硬资源限制。 如果创建或更新资源违反了配额约束，则请求将失败并显示 HTTP 状态代码403 FORBIDDEN，并显示一条消息，解释可能违反的约束。 cpu如果在命名空间中为和之类的计算资源启用了配额memory，则用户必须为这些值指定请求或限制；否则，配额系统可能会拒绝创建 pod。提示：使用LimitRanger准入控制器强制对没有计算资源要求的 pod 使用默认值。 ResourceQuota 对象的名称必须是有效的 DNS 子域名。 可以使用命名空间和配额创建的策略示例如下： 在容量为 32 GiB RAM 和 16 CPU 的集群中，让团队 A 使用 20 GiB 和 10 CPU，让 B 使用 10GiB 和 4 CPU，并保留 2GiB 和 2 CPU以供将来分配。 将“测试”命名空间限制为使用 1 CPU 和 1GiB RAM。让“生产”命名空间使用任意数量。 在集群总容量小于命名空间配额之和的情况下，可能会发生资源争用。这是按照先到先得的原则处理的。 争用或更改配额都不会影响已创建的资源。 3. 架构图 4. 启用资源配额 许多 Kubernetes 发行版默认启用资源配额支持。它启用时API 服务器 --enable-admission-plugins=flagResourceQuota作为其参数之一。 当特定命名空间中存在 ResourceQuota 时，将在该命名空间中强制执行资源配额。 5. 计算资源配额 资源名称 描述 limits.cpu 在所有处于非终端状态的 Pod 中，CPU 限制的总和不能超过此值。 limits.memory 在所有处于非终端状态的 Pod 中，内存限制的总和不能超过此值。 requests.cpu 在所有处于非终端状态的 Pod 中，CPU 请求的总和不能超过这个值。 requests.memory 在所有处于非终端状态的 Pod 中，内存请求的总和不能超过这个值。 hugepages-\\ 在所有处于非终端状态的 Pod 中，指定大小的大页面请求数不能超过该值。 cpu 如同requests.cpu memory 如同requests.memory 6. 资源的资源配额 除了上面提到的资源，在 1.10 版本中，增加了对 扩展资源的配额支持。 由于扩展资源不允许过度使用，因此在配额中同时指定requests 和limits为同一扩展资源指定是没有意义的。所以对于扩展资源，目前只requests.允许带前缀的配额项。 以 GPU 资源为例，如果资源名称为nvidia.com/gpu，并且您想限制一个命名空间中请求的 GPU 总数为 4，您可以定义配额如下： requests.nvidia.com/gpu: 4 7. 存储资源配额 您可以限制给定命名空间中可以请求的存储资源的总和。 此外，您可以根据关联的存储类限制存储资源的消耗。 资源名称 描述 requests.storage 在所有持久卷声明中，存储请求的总和不能超过此值。 persistentvolumeclaims 命名空间中可以存在的PersistentVolumeClaims总数。 .storageclass.storage.k8s.io/requests.storage 在与 关联的所有持久卷声明中，存储请求的总和不能超过此值。 .storageclass.storage.k8s.io/persistentvolumeclaims 在与 storage-class-name 关联的所有持久卷声明中，可以存在于命名空间中的持久卷声明的总数。 例如，如果运营商想要使用与gold存储类分开的bronze存储类来配额存储，则运营商可以定义如下配额： gold.storageclass.storage.k8s.io/requests.storage: 500Gi bronze.storageclass.storage.k8s.io/requests.storage: 100Gi 在 1.8 版中，添加了对本地临时存储的配额支持作为 alpha 功能： 资源名称 描述 requests.ephemeral-storage 在命名空间中的所有 Pod 中，本地临时存储请求的总和不能超过此值。 limits.ephemeral-storage 在命名空间中的所有 Pod 中，本地临时存储限制的总和不能超过此值。 ephemeral-storage 与 相同requests.ephemeral-storage。 注意：使用 CRI 容器运行时，容器日志将计入临时存储配额。这可能会导致已用完存储配额的 pod 被意外驱逐。 8. 对象计数配额 您可以使用以下语法为所有标准命名空间资源类型的某些资源的总数设置配额： count/.来自非核心组的资源 count/来自核心组的资源 以下是用户可能希望放在对象计数配额下的一组示例资源： count/persistentvolumeclaims count/services count/secrets count/configmaps count/replicationcontrollers count/deployments.apps count/replicasets.apps count/statefulsets.apps count/jobs.batch count/cronjobs.batch 相同的语法可用于自定义资源。例如，要在API 组中的widgets自定义资源上创建配额，请使用.example.comcount/widgets.example.com 使用count/*资源配额时，如果对象存在于服务器存储中，则按配额收费。这些类型的配额有助于防止存储资源耗尽。例如，鉴于服务器的大小，您可能希望限制服务器中的 Secret 数量。集群中过多的 Secrets 实际上会阻止服务器和控制器启动。您可以为作业设置配额以防止配置不当的 CronJob。在命名空间中创建过多作业的 CronJobs 可能会导致拒绝服务。 也可以对有限的资源集进行通用对象计数配额。支持以下类型： 资源名称 描述 configmaps 命名空间中可以存在的 ConfigMap 总数。 persistentvolumeclaims 命名空间中可以存在的PersistentVolumeClaims总数。 pods 命名空间中可以存在的处于非终端状态的 Pod 总数。如果.status.phase in (Failed, Succeeded)为真，则 Pod 处于终端状态。 replicationcontrollers 命名空间中可以存在的 ReplicationController 总数。 resourcequotas 命名空间中可以存在的 ResourceQuota 总数。 services 命名空间中可以存在的服务总数。 services.loadbalancers LoadBalancer命名空间中可以存在的类型的服务总数。 services.nodeports NodePort命名空间中可以存在的类型的服务总数。 secrets 命名空间中可以存在的 Secret 总数。 例如，pods配额计算并强制限制在pods 单个命名空间中创建的非终端数量。您可能希望pods 在命名空间上设置配额，以避免用户创建许多小型 Pod 并耗尽集群的 Pod IP 供应的情况。 9. 配额范围 每个配额可以有一组关联的scopes. 如果配额与枚举范围的交集匹配，则配额只会衡量资源的使用情况。 将范围添加到配额时，它会将其支持的资源数量限制为与该范围相关的资源。在允许集之外的配额上指定的资源会导致验证错误。 范围 描述 Terminating 在哪里匹配 pod .spec.activeDeadlineSeconds >= 0 NotTerminating 在哪里匹配 pod .spec.activeDeadlineSeconds is nil BestEffort 匹配具有最佳服务质量的 pod。 NotBestEffort 匹配没有尽力而为服务质量的 pod。 PriorityClass 匹配引用指定优先级类的 pod 。 范围将BestEffort配额限制为跟踪以下资源： pods Terminating和NotTerminating范围将配额限制NotBestEffort为PriorityClass 跟踪以下资源： pods cpu memory requests.cpu requests.memory limits.cpu limits.memory 支持字段中的scopeSelector以下值operator： In NotIn Exists DoesNotExist 当定义 scopeSelector时，使用以下值之一作为scopeName，operator必须是Exists Terminating NotTerminating BestEffort NotBestEffort 如果operator是In或Not In，则该values字段必须至少有一个值。例如： scopeSelector: matchExpressions: - scopeName: PriorityClass operator: In values: - middle 如果operator是Exists或DoesNotExist，则不得指定该values字段。 10. pod 配置 PriorityClass 优先级消耗资源 征状态： Kubernetes v1.17 [stable] 可以按特定的优先级创建 Pod 。scopeSelector 您可以使用配额规范中的字段，根据 Pod 的优先级控制 Pod 对系统资源的消耗。 只有scopeSelector在配额规范中选择 pod 时才会匹配和消耗配额。 当配额使用字段限定为优先级时scopeSelector，配额对象被限制为仅跟踪以下资源： pods cpu memory ephemeral-storage limits.cpu limits.memory limits.ephemeral-storage requests.cpu requests.memory requests.ephemeral-storage 此示例创建一个配额对象并将其与特定优先级的 pod 匹配。该示例的工作原理如下： 集群中的 Pod 具有三个优先级类别之一，“低”、“中”、“高”。 为每个优先级创建一个配额对象。 将以下 YAML 保存到文件quota.yml中。 apiVersion: v1 kind: List items: - apiVersion: v1 kind: ResourceQuota metadata: name: pods-high spec: hard: cpu: \"1000\" memory: 200Gi pods: \"10\" scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\"high\"] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-medium spec: hard: cpu: \"10\" memory: 20Gi pods: \"10\" scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\"medium\"] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-low spec: hard: cpu: \"5\" memory: 10Gi pods: \"10\" scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\"low\"] 创建 低、中、高 ResourceQuota kubectl create -f ./quota.yml 验证Used配额正在0使用kubectl describe quota. $ kubectl describe quota .... Name: pods-high Namespace: default Resource Used Hard -------- ---- ---- cpu 0 1k memory 0 200Gi pods 0 10 Name: pods-low Namespace: default Resource Used Hard -------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard -------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 创建一个优先级为“高”的 pod。将以下 YAML 保存到文件high-priority-pod.yml中。 apiVersion: v1 kind: Pod metadata: name: high-priority spec: containers: - name: high-priority image: ubuntu command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10;done\"] resources: requests: memory: \"10Gi\" cpu: \"500m\" limits: memory: \"10Gi\" cpu: \"500m\" priorityClassName: high 创建 high pod kubectl create -f ./high-priority-pod.yml 验证 ResourceQuota 使用情况 kubectl describe quota Name: pods-high Namespace: default Resource Used Hard -------- ---- ---- cpu 500m 1k memory 10Gi 200Gi pods 1 10 Name: pods-low Namespace: default Resource Used Hard -------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard -------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 11. 跨命名空间 Pod Affinity Quota 特征状态： Kubernetes v1.24 [stable] 运营商可以使用CrossNamespacePodAffinity配额范围来限制允许哪些命名空间拥有具有跨命名空间的亲和术语的 pod。具体来说，它控制允许设置哪些 pod namespaces或namespaceSelect 亲和性术语中的字段。 可能需要防止用户使用跨命名空间亲和性术语，因为具有反亲和性约束（anti-affinity constraints）的 pod 可能会阻止来自所有其他命名空间的 pod 在故障域中被调度。 使用此范围运算符可以防止某些命名空间（foo-ns在下面的示例中）具有使用跨命名空间 pod 亲和性的 pod，方法是在该命名空间中创建一个CrossNamespaceAffinity范围和硬限制为 0 的资源配额对象： apiVersion: v1 kind: ResourceQuota metadata: name: disable-cross-namespace-affinity namespace: foo-ns spec: hard: pods: \"0\" scopeSelector: matchExpressions: - scopeName: CrossNamespaceAffinity 如果运营商希望默认禁止使用namespaces and namespaceSelector并且只允许特定命名空间使用，他们可以CrossNamespaceAffinity 通过将 kube-apiserver 标志 --admission-control-config-file 设置为以下配置文件的路径来配置为有限资源： apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: \"ResourceQuota\" configuration: apiVersion: apiserver.config.k8s.io/v1 kind: ResourceQuotaConfiguration limitedResources: - resource: pods matchScopes: - scopeName: CrossNamespaceAffinity 12. 创建与查看 ResourceQuota 12.1 创建 kubectl create namespace myspace cat compute-resources.yaml apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi requests.nvidia.com/gpu: 4 EOF kubectl create -f ./compute-resources.yaml --namespace=myspace cat object-counts.yaml apiVersion: v1 kind: ResourceQuota metadata: name: object-counts spec: hard: configmaps: \"10\" persistentvolumeclaims: \"4\" pods: \"4\" replicationcontrollers: \"20\" secrets: \"10\" services: \"10\" services.loadbalancers: \"2\" EOF kubectl create -f ./object-counts.yaml --namespace=myspace 12.2 查看 $ kubectl get quota --namespace=myspace NAME AGE compute-resources 30s object-counts 32s $ kubectl describe quota compute-resources --namespace=myspace Name: compute-resources Namespace: myspace Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi requests.nvidia.com/gpu 0 4 $ kubectl describe quota object-counts --namespace=myspace Name: object-counts Namespace: myspace Resource Used Hard -------- ---- ---- configmaps 0 10 persistentvolumeclaims 0 4 pods 0 4 replicationcontrollers 0 20 secrets 1 10 services 0 10 services.loadbalancers 0 2 Kubectl 还使用以下语法支持所有标准命名空间资源的对象计数配额count/.： kubectl create namespace myspace kubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2 $ kubectl describe quota --namespace=myspace Name: test Namespace: myspace Resource Used Hard -------- ---- ---- count/deployments.apps 1 2 count/pods 2 3 count/replicasets.apps 1 4 count/secrets 1 4 13. 默认限制优先级消耗 可能希望 pod 具有特定优先级，例如 当且仅当存在匹配的配额对象时，应允许在命名空间中使用“cluster-services”。 通过这种机制，运营商能够将某些高优先级类的使用限制在有限数量的命名空间中，并且默认情况下并非每个命名空间都能够使用这些优先级类。 要强制执行此操作，应使用kube-apiserver flag将路径传递到以下配置文件：--admission-control-config-file apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: \"ResourceQuota\" configuration: apiVersion: apiserver.config.k8s.io/v1 kind: ResourceQuotaConfiguration limitedResources: - resource: pods matchScopes: - scopeName: PriorityClass operator: In values: [\"cluster-services\"] 然后，在命名空间中创建一个资源配额对象kube-system： apiVersion: v1 kind: ResourceQuota metadata: name: pods-cluster-services spec: scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\"cluster-services\"] 创建 kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system 在这种情况下，如果出现以下情况，将允许创建 pod： priorityClassName未指定Pod 。 Pod 的priorityClassName值被指定为cluster-services. PodpriorityClassName设置为cluster-services，将在kube-system命名空间中创建，并且已通过资源配额检查。 如果将 Pod 创建请求priorityClassName设置为cluster-services 并且将在kube-system 14. 实例 14.1 pod 请求或限制 cpu、mem 超出资源配置 创建 ResourceQuota 文件quota-mem-cpu.yaml apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi kubectl create namespace quota-mem-cpu-example kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace=quota-mem-cpu-example kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml 创建 pod 文件quota-mem-cpu-pod.yamlapiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo spec: containers: - name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: \"800Mi\" cpu: \"800m\" requests: memory: \"600Mi\" cpu: \"400m\" kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace=quota-mem-cpu-example kubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml 输出显示配额以及已使用了多少配额。您可以看到Pod的内存和CPU请求以及限制没有超过配额。 status: hard: limits.cpu: \"2\" limits.memory: 2Gi requests.cpu: \"1\" requests.memory: 1Gi used: limits.cpu: 800m limits.memory: 800Mi requests.cpu: 400m requests.memory: 600Mi 注意，已用内存请求和此新内存请求的总和超过了内存请求配额。600 MiB + 700 MiB> 1 GiB。 尝试创建Pod： kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace=quota-mem-cpu-example 没有创建第二个Pod。输出显示创建第二个Pod将导致内存请求总数超过内存请求配额。 Error from server (Forbidden): error when creating \"examples/admin/resource/quota-mem-cpu-pod-2.yaml\": pods \"quota-mem-cpu-demo-2\" is forbidden: exceeded quota: mem-cpu-demo, requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi 参考： kubernetes Resource Quotas kubernetes quota-memory-cpu-namespace 为命名空间配置默认的内存请求和限制 Pod开销 Kubernetes Resource Quota Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 07:08:32 "},"策略/kubernetes-policy-QoS.html":{"url":"策略/kubernetes-policy-QoS.html","title":"QoS","keywords":"","body":"kubernetes 资源管理策略 QoS1. 背景2. QoS 类型2.1 创建QoS 类为 Guaranteed 的 Pod2.2 创建QoS 类为 Burstable 的 Pod2.3 创建 QoS 类为 BestEffort 的 Pod4. QOS 的目的5. 总结kubernetes 资源管理策略 QoS tagsstart pod 策略 tagsstop 《为全人类》第三季 1. 背景 作为 Kubernetes 的资源管理与调度部分的基础，我们要从它的资源模型开始说起。在 Kubernetes 里，Pod 是最小的原子调度单位。这也就意味着，所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的 CPU 和内存配置。 像 CPU 这样的资源被称作“可压缩资源”（compressible resources）。它的典型特点是，当可压缩资源不足时，Pod只会“饥饿”，但不会退出。 而像内存这样的资源，则被称作“不可压缩资源（incompressible resources）。当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。 而由于 Pod 可以由多个 Container 组成，所以 CPU 和内存资源的限额，是要配置在每个 Container 的定义上的。这样，Pod 整体的资源配置，就由这些 Container 的配置值累加得到。 其中，Kubernetes 里为 CPU 设置的单位是“CPU 的个数”。比如，cpu=1 指的就是，这个 Pod 的 CPU 限额是 1 个 CPU。当然，具体“1 个 CPU”在宿主机上如何解释，是 1 个 CPU 核心，还是 1 个 vCPU，还是 1 个 CPU 的超线程（Hyperthread），完全取决于宿主机的 CPU 实现方式。Kubernetes 只负责保证 Pod 能够使用到“1 个 CPU”的计算能力。此外，Kubernetes 允许你将 CPU 限额设置为分数，比如在我们的例子里，CPU limits 的值就是 500m。所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 的意思。这样，这个 Pod 就会被分配到 1 个 CPU 一半的计算能力。当然，你也可以直接把这个配置写成 cpu=0.5。但在实际使用时，我还是推荐你使用 500m 的写法，毕竟这才是 Kubernetes 内部通用的 CPU 表示方式。 而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。比如，在我们的例子里，Memory requests 的值就是 64MiB (2 的 26 次方 bytes) 。这里要注意区分 MiB（mebibyte）和 MB（megabyte）的区别。 备注：1Mi=1024*1024；1M=1000*1000 Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况 spec.containers[].resources.limits.cpu spec.containers[].resources.limits.memory spec.containers[].resources.requests.cpu spec.containers[].resources.requests.memory 这两者的区别其实非常简单：在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。 更确切地说，当你指定了 requests.cpu=250m 之后，相当于将 Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024。而当你没有设置 requests.cpu 的时候，cpu.shares 默认则是 1024。这样，Kubernetes 就通过 cpu.shares 完成了对 CPU 时间的按比例分配。 而如果你指定了 limits.cpu=500m 之后，则相当于将 Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms，而 cpu.cfs_period_us 的值始终是 100ms。这样，Kubernetes 就为你设置了这个容器只能用到 CPU 的 50%。 而对于内存来说，当你指定了 limits.memory=128Mi 之后，相当于将 Cgroups 的 memory.limit_in_bytes 设置为 128 * 1024 * 1024。而需要注意的是，在调度的时候，调度器只会使用 requests.memory=64Mi 来进行判断。 Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边界”的定义，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。 基于这种假设，Borg 在作业被提交后，会主动减小它的资源限额配置，以便容纳更多的作业、提升资源利用率。而当作业资源使用量增加到一定阈值时，Borg 会通过“快速恢复”过程，还原作业原始的资源限额，防止出现异常情况。 而 Kubernetes 的 requests+limits 的做法，其实就是上述思路的一个简化版：用户在提交 Pod 时，可以声明一个相对较小的 requests 值供调度器使用，而 Kubernetes 真正设置给容器 Cgroups 的，则是相对较大的 limits 值。不难看到，这跟 Borg 的思路相通的。 2. QoS 类型 在理解了 Kubernetes 资源模型的设计之后，我再来和你谈谈 Kubernetes 里的 QoS 模型。。Kubernetes 使用 QoS 类来决定 Pod 的调度和驱逐策略。 Kubernetes 创建 Pod 时就给它指定了下列一种 QoS 类： Guaranteed Burstable BestEffort 创建命名空间 kubectl create namespace qos-example 2.1 创建QoS 类为 Guaranteed 的 Pod 对于 QoS 类为 Guaranteed 的 Pod： Pod 中的每个容器，包含初始化容器，必须指定内存请求和内存限制，并且两者要相等。 Pod 中的每个容器，包含初始化容器，必须指定 CPU 请求和 CPU 限制，并且两者要相等。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存请求和内存限制，值都是 200 MiB。 容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU： apiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: - name: qos-demo-ctr image: nginx resources: limits: memory: \"200Mi\" cpu: \"700m\" requests: memory: \"200Mi\" cpu: \"700m\" kubectl create -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。 结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。 spec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ... status: qosClass: Guaranteed 说明： 如果容器指定了自己的内存限制，但没有指定内存请求，Kubernetes 会自动为它指定与内存限制匹配的内存请求。同样，如果容器指定了自己的 CPU 限制，但没有指定 CPU 请求，Kubernetes 会自动为它指定与 CPU 限制匹配的 CPU请求，这也属于 Guaranteed 情况。。 2.2 创建QoS 类为 Burstable 的 Pod 如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable： Pod 不符合 Guaranteed QoS 类的标准。 Pod 中至少一个容器具有内存或 CPU 请求。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存限制 200 MiB 和内存请求 100 MiB。 apiVersion: v1 kind: Pod metadata: name: qos-demo-2 namespace: qos-example spec: containers: - name: qos-demo-2-ctr image: nginx resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable。 spec: containers: - image: nginx imagePullPolicy: Always name: qos-demo-2-ctr resources: limits: memory: 200Mi requests: memory: 100Mi ... status: qosClass: Burstable 下面是包含两个容器的 Pod 配置文件。 一个容器指定了内存请求 200 MiB。 另外一个容器没有指定任何请求和限制。 apiVersion: v1 kind: Pod metadata: name: qos-demo-4 namespace: qos-example spec: containers: - name: qos-demo-4-ctr-1 image: nginx resources: requests: memory: \"200Mi\" - name: qos-demo-4-ctr-2 image: redis kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-4 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable： spec: containers: ... name: qos-demo-4-ctr-1 resources: requests: memory: 200Mi ... name: qos-demo-4-ctr-2 resources: {} ... status: qosClass: Burstable 结果 pod 为 Burstable类型。 2.3 创建 QoS 类为 BestEffort 的 Pod 对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求。 下面是包含一个容器的 Pod 配置文件。 容器没有设置内存和 CPU 限制或请求。 apiVersion: v1 kind: Pod metadata: name: qos-demo-3 namespace: qos-example spec: containers: - name: qos-demo-3-ctr image: nginx kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort。 spec: containers: ... resources: {} ... status: qosClass: BestEffort 4. QOS 的目的 那么，Kubernetes 为 Pod 设置这样三种 QoS 类别，具体有什么作用呢？ 实际上，QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。 具体地说，当 Kubernetes 所管理的宿主机上不可压缩资源短缺时，就有可能触发 Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。 目前，Kubernetes 为你设置的 Eviction 的默认阈值如下所示： memory.available 当然，上述各个触发条件在 kubelet 里都是可配置的。比如下面这个例子： kubelet --eviction-hard=imagefs.available 在这个配置中，你可以看到 Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式。 其中，Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。 而 Hard Eviction 模式下，Eviction 过程就会在阈值达到之后立刻开始。 其中，Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。 Kubernetes 计算 Eviction 阈值的数据来源，主要依赖于从 Cgroups 读取到的值，以及使用 cAdvisor 监控到的数据。 当宿主机的 Eviction 阈值达到后，就会进入 MemoryPressure 或者 DiskPressure 状态，从而避免新的 Pod 被调度到这台宿主机上。 而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod 的 QoS 类别了。 首当其冲的，自然是 BestEffort 类别的 Pod。 其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的 Pod。 最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。 当然，对于同 QoS 类别的 Pod 来说，Kubernetes 还会根据 Pod 的优先级来进行进一步地排序和选择。 在理解了 Kubernetes 里的 QoS 类别的设计之后，我再来为你讲解一下Kubernetes 里一个非常有用的特性：cpuset 的设置。 我们知道，在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。 这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。 可是，这样的需求在 Kubernetes 里又该如何实现呢？其实非常简单。 首先，你的 Pod 必须是 Guaranteed 的 QoS 类型； 然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值即可。 比如下面这个例子： spec: containers: - name: nginx image: nginx resources: limits: memory: \"200Mi\" cpu: \"2\" requests: memory: \"200Mi\" cpu: \"2\" 这时候，该 Pod 就会被绑定在 2 个独占的 CPU 核上。当然，具体是哪两个 CPU 核，是由 kubelet 为你分配的。以上，就是 Kubernetes 的资源模型和 QoS 类别相关的主要内容。 5. 总结 讲解了 Kubernetes 里对资源的定义方式和资源模型的设计。 QOS的类型 讲述了 Kubernetes 里对 Pod 进行 Eviction 的具体策略和实践方式 正是基于上述讲述，在实际的使用中，我强烈建议你将 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型。否则，一旦 DaemonSet 的 Pod 被回收，它又会立即在原宿主机上被重建出来，这就使得前面资源回收的动作，完全没有意义了。 参考： kubernetes 配置 Pod 的服务质量 张磊 极客时间 深入浅出 kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 07:27:24 "},"策略/kubernetes-policy-LimitRange.html":{"url":"策略/kubernetes-policy-LimitRange.html","title":"LimitRange","keywords":"","body":"kubernetes 策略 LimitRange1. LimitRange 为 namespace 配置默认的内存请求和限制1.1 LimitRange 限制 namespace 资源1.2 创建没有声明自己的内存请求和限制值pod1.3 声明容器的限制而不声明它的请求会怎么样？1.4 声明容器的内存请求而不声明内存限制会怎么样？2. LimitRange 配置 CPU 最小和最大约束2.1 创建满足配置 CPU 最小和最大约束的 pod2.2 创建超过最大 CPU 限制的 Pod2.3 创建不满足最小 CPU 请求的 Pod2.4 创建没有声明 CPU 请求和 CPU 限制的 Pod3. LimitRange为配置 namespace 内存最小和最大约束3.1 LimitRange 配置最大最小限制值3.2 创建满足声明的最大值与最小值3.3 尝试创建一个超过最大内存限制的 Pod3.4 尝试创建一个不满足最小内存请求的 Pod3.5 创建一个没有声明内存请求和限制的 Podkubernetes 策略 LimitRange tagsstart LimitRange tagsstop 《星际穿越》拍摄花絮 1. LimitRange 为 namespace 配置默认的内存请求和限制 1.1 LimitRange 限制 namespace 资源 创建 namespace kubectl create namespace default-mem-example 创建 LimitRange 和 Pod apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container 在 default-mem-example namespace创建限制范围： kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example 1.2 创建没有声明自己的内存请求和限制值pod 现在，如果在 default-mem-example namespace创建容器，并且该容器没有声明自己的内存请求和限制值， 它将被指定默认的内存请求 256 MiB 和默认的内存限制 512 MiB。 下面是具有一个容器的 Pod 的配置文件。 容器未指定内存请求和限制。 apiVersion: v1 kind: Pod metadata: name: default-mem-demo spec: containers: - name: default-mem-demo-ctr image: nginx kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace=default-mem-example kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example 输出内容显示该 Pod 的容器有 256 MiB 的内存请求和 512 MiB 的内存限制。 这些都是 LimitRange 设置的默认值。 containers: - image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 删除你的 Pod： kubectl delete pod default-mem-demo --namespace=default-mem-example 1.3 声明容器的限制而不声明它的请求会怎么样？ apiVersion: v1 kind: Pod metadata: name: default-mem-demo-2 spec: containers: - name: default-mem-demo-2-ctr image: nginx resources: limits: memory: \"1Gi\" kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace=default-mem-example 查看 Pod 的详情： kubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example 输出结果显示容器的内存请求被设置为它的内存限制相同的值。注意该容器没有被指定默认的内存请求值 256MiB。 resources: limits: memory: 1Gi requests: memory: 1Gi 1.4 声明容器的内存请求而不声明内存限制会怎么样？ apiVersion: v1 kind: Pod metadata: name: default-mem-demo-3 spec: containers: - name: default-mem-demo-3-ctr image: nginx resources: requests: memory: \"128Mi\" kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace=default-mem-example 查看 Pod 声明： kubectl get pod default-mem-demo-3 --output=yaml --namespace=default-mem-example 输出结果显示该容器的内存请求被设置为了容器配置文件中声明的数值。 容器的内存限制被设置为 512MiB，即 namespace的默认内存限制。 resources: limits: memory: 512Mi requests: memory: 128Mi 2. LimitRange 配置 CPU 最小和最大约束 创建 namespace kubectl create namespace constraints-cpu-example 创建 LimitRange 和 Pod apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: - max: cpu: \"800m\" min: cpu: \"200m\" type: Container kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace=constraints-cpu-example 查看 LimitRange 详情： kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。 limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 2.1 创建满足配置 CPU 最小和最大约束的 pod apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo spec: containers: - name: constraints-cpu-demo-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"500m\" kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace=constraints-cpu-example kubectl get pod constraints-cpu-demo --namespace=constraints-cpu-example 查看 Pod 的详情： kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example 输出结果表明容器的 CPU 请求为 500 millicpu，CPU 限制为 800 millicpu。 这些参数满足 LimitRange 规定的限制范围。 resources: limits: cpu: 800m requests: cpu: 500m 2.2 创建超过最大 CPU 限制的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-2 spec: containers: - name: constraints-cpu-demo-2-ctr image: nginx resources: limits: cpu: \"1.5\" requests: cpu: \"500m\" kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example 输出结果表明 Pod 没有创建成功，因为容器声明的 CPU 限制太大了： Error from server (Forbidden): error when creating \"examples/admin/resource/cpu-constraints-pod-2.yaml\": pods \"constraints-cpu-demo-2\" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. 2.3 创建不满足最小 CPU 请求的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-3 spec: containers: - name: constraints-cpu-demo-3-ctr image: nginx resources: limits: cpu: \"800m\" requests: cpu: \"100m\" kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example 输出结果显示 Pod 没有创建成功，因为容器声明的 CPU 请求太小了： Error from server (Forbidden): error when creating \"examples/admin/resource/cpu-constraints-pod-3.yaml\": pods \"constraints-cpu-demo-4\" is forbidden: minimum cpu usage per Container is 200m, but request is 100m. 2.4 创建没有声明 CPU 请求和 CPU 限制的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: - name: constraints-cpu-demo-4-ctr image: vish/stress kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example 查看 Pod 的详情： kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml 输出结果显示 Pod 的容器有个 800 millicpu 的 CPU 请求和 800 millicpu 的 CPU 限制。 容器是怎样得到那些值的呢？ resources: limits: cpu: 800m requests: cpu: 800m 因为你的 Container 没有声明自己的 CPU 请求和限制，LimitRange 给它指定了 默认的 CPU 请求和限制。 3. LimitRange为配置 namespace 内存最小和最大约束 在 LimitRange 对象中指定最小和最大内存值。如果 Pod 不满足 LimitRange 施加的约束，则无法在 namespace 中创建它. 3.1 LimitRange 配置最大最小限制值 创建 namespace kubectl create namespace constraints-mem-example 创建 LimitRange 和 Pod apiVersion: v1 kind: LimitRange metadata: name: mem-min-max-demo-lr spec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace=constraints-mem-example 查看 LimitRange 的详情： kubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml 输出显示预期的最小和最大内存约束。 但请注意，即使你没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。 limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container 3.2 创建满足声明的最大值与最小值 apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo spec: containers: - name: constraints-mem-demo-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"600Mi\" kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace=constraints-mem-example 确认下 Pod 中的容器在运行： kubectl get pod constraints-mem-demo --namespace=constraints-mem-example 查看 Pod 详情： kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example 输出结果显示容器的内存请求为600 MiB，内存限制为800 MiB。这些满足了 LimitRange 设定的限制范围。 resources: limits: memory: 800Mi requests: memory: 600Mi 3.3 尝试创建一个超过最大内存限制的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-2 spec: containers: - name: constraints-mem-demo-2-ctr image: nginx resources: limits: memory: \"1.5Gi\" requests: memory: \"800Mi\" kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace=constraints-mem-example 输出结果显示 Pod 没有创建成功，因为容器声明的内存限制太大了： Error from server (Forbidden): error when creating \"examples/admin/resource/memory-constraints-pod-2.yaml\": pods \"constraints-mem-demo-2\" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. 3.4 尝试创建一个不满足最小内存请求的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-3 spec: containers: - name: constraints-mem-demo-3-ctr image: nginx resources: limits: memory: \"800Mi\" requests: memory: \"100Mi\" kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace=constraints-mem-example 输出结果显示 Pod 没有创建成功，因为容器声明的内存请求太小了： Error from server (Forbidden): error when creating \"examples/admin/resource/memory-constraints-pod-3.yaml\": pods \"constraints-mem-demo-3\" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. 3.5 创建一个没有声明内存请求和限制的 Pod apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-4 spec: containers: - name: constraints-mem-demo-4-ctr image: nginx kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace=constraints-mem-example 查看 Pod 详情： kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml 输出结果显示 Pod 的内存请求为1 GiB，内存限制为1 GiB。容器怎样获得哪些数值呢？ resources: limits: memory: 1Gi requests: memory: 1Gi 因为你的容器没有声明自己的内存请求和限制，它从 LimitRange 那里获得了 默认的内存请求和限制。 参考： kubernetes Limit Ranges kubernetes资源调度之LimitRange Restrict resource consumption with limit ranges Limit Ranges in Kubernetes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 03:00:03 "},"组件/":{"url":"组件/","title":"组件","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-05 06:47:55 "},"组件/Kubeadm/":{"url":"组件/Kubeadm/","title":"Kubeadm","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"组件/Kubeadm/kubeadm-command.html":{"url":"组件/Kubeadm/kubeadm-command.html","title":"命令","keywords":"","body":"kubeadm 命令1. kubeadm 概述2. 安装kubeadm3. kubeadm任务4. kubeadm init 流程5. kubeadm init 初始化参数5.1 kubeadm init phase分段执行5.2 Kubeadm自定义组件配置参数5.3 kubeadm自定义镜像5.4 kubeadm将证书上载到集群6. kubeadmin join6.1 流程详解6.2 参数7. kubeadm config8. kubeadm tokenkubeadm 命令 tagsstart kubeadm 命令 tagsstop 1. kubeadm 概述 Kubeadm 是一个工具，它提供了 kubeadm init 以及 kubeadm join 这两个命令作为快速创建 kubernetes 集群的最佳实践。 2. 安装kubeadm 官方参考： kubeadm安装 3. kubeadm任务 kubeadm init 启动引导一个 Kubernetes 主节点 kubeadm join 启动引导一个 Kubernetes 工作节点并且将其加入到集群 kubeadm upgrade 更新 Kubernetes 集群到新版本 kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadmupgrade 命令 kubeadm token 使用 kubeadm join 来管理令牌 kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点所作改变 kubeadm version 打印出 kubeadm 版本 kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈 4. kubeadm init 流程 \"init\" 命令执行以下阶段： preflight Run pre-flight checks kubelet-start Write kubelet settings and (re)start the kubelet certs Certificate generation /ca Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components /apiserver Generate the certificate for serving the Kubernetes API /apiserver-kubelet-client Generate the certificate for the API server to connect to kubelet /front-proxy-ca Generate the self-signed CA to provision identities for front proxy /front-proxy-client Generate the certificate for the front proxy client /etcd-ca Generate the self-signed CA to provision identities for etcd /etcd-server Generate the certificate for serving etcd /etcd-peer Generate the certificate for etcd nodes to communicate with each other /etcd-healthcheck-client Generate the certificate for liveness probes to healthcheck etcd /apiserver-etcd-client Generate the certificate the apiserver uses to access etcd /sa Generate a private key for signing service account tokens along with its public key kubeconfig Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file /admin Generate a kubeconfig file for the admin to use and for kubeadm itself /kubelet Generate a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes /controller-manager Generate a kubeconfig file for the controller manager to use /scheduler Generate a kubeconfig file for the scheduler to use control-plane Generate all static Pod manifest files necessary to establish the control plane /apiserver Generates the kube-apiserver static Pod manifest /controller-manager Generates the kube-controller-manager static Pod manifest /scheduler Generates the kube-scheduler static Pod manifest etcd Generate static Pod manifest file for local etcd /local Generate the static Pod manifest file for a local, single-node local etcd instance upload-config Upload the kubeadm and kubelet configuration to a ConfigMap /kubeadm Upload the kubeadm ClusterConfiguration to a ConfigMap /kubelet Upload the kubelet component config to a ConfigMap upload-certs Upload certificates to kubeadm-certs mark-control-plane Mark a node as a control-plane bootstrap-token Generates bootstrap tokens used to join a node to a cluster kubelet-finalize Updates settings relevant to the kubelet after TLS bootstrap /experimental-cert-rotation Enable kubelet client certificate rotation addon Install required addons for passing Conformance tests /coredns Install the CoreDNS addon to a Kubernetes cluster /kube-proxy Install the kube-proxy addon to a Kubernetes cluster 描述： 在进行更改之前，kubeadm 运行一系列检查以验证系统状态。一些检查只会触发警告，有些检查会被视为错误并会退出kubeadm，直到问题得到解决或用户指定了 --skip-preflight-checks。 kubeadm 将生成一个 token，以便其它 node 可以用来注册到 master 中。用户也可以选择自己提供一个 token。 kubeadm 将生成一个自签名 CA 来为每个组件设置身份（包括node）。它也生成客户端证书以便各种组件可以使用。如果用户已经提供了自己的 CA 并将其放入 cert 目（通过--cert-dir 配置，默认路径为 /etc/kubernetes/pki），则跳过此步骤。 输出一个 kubeconfig 文件以便 kubelet 能够使用这个文件来连接到 API server，以及一个额外的kubeconfig 文件以作管理用途。 kubeadm 将会为 API server、controller manager 和 scheduler 生成 Kubernetes 的静态 Pod manifest 文件，并将这些文件放入 /etc/kubernetes/manifests 中。Kubelet将会监控这个目录，以便在启动时创建 pod。这些都是 Kubernetes 的关键组件，一旦它们启动并正常运行后，kubeadm就能启动和管理其它额外的组件了。 kubeadm 将会给 master 节点 “taint” 标签，以让控制平面组件只运行在这个节点上。它还建立了 RBAC授权系统，并创建一个特殊的 ConfigMap 用来引导与 kubelet 的互信连接。 kubeadm 通过 API server 安装插件组件。目前这些组件有内部的 DNS server 和 kube-proxy DaemonSet。 5. kubeadm init 初始化参数 kubeadm init [flags] 参数说明： --apiserver-advertise-address string API Server将要广播的监听地址。如指定为 `0.0.0.0` 将使用缺省的网卡地址。 --apiserver-bind-port int32 缺省值: 6443 API Server绑定的端口 --apiserver-cert-extra-sans stringSlice 可选的额外提供的证书主题别名（SANs）用于指定API Server的服务器证书。可以是IP地址也可以是DNS名称。 --cert-dir string 缺省值: \"/etc/kubernetes/pki\" 证书的存储路径。 --config string kubeadm配置文件的路径。警告：配置文件的功能是实验性的。 --cri-socket string 缺省值: \"/var/run/dockershim.sock\" 指明要连接的CRI socket文件 --dry-run 不会应用任何改变；只会输出将要执行的操作。 --feature-gates string 键值对的集合，用来控制各种功能的开关。可选项有: Auditing=true|false (当前为ALPHA状态 - 缺省值=false) CoreDNS=true|false (缺省值=true) DynamicKubeletConfig=true|false (当前为BETA状态 - 缺省值=false) -h, --help 获取init命令的帮助信息 --ignore-preflight-errors stringSlice 忽视检查项错误列表，列表中的每一个检查项如发生错误将被展示输出为警告，而非错误。 例如: 'IsPrivilegedUser,Swap'. 如填写为 'all' 则将忽视所有的检查项错误。 --kubernetes-version string 缺省值: \"stable-1\" 为control plane选择一个特定的Kubernetes版本。 --node-name string 指定节点的名称。 --pod-network-cidr string 指明pod网络可以使用的IP地址段。 如果设置了这个参数，control plane将会为每一个节点自动分配CIDRs。 --service-cidr string 缺省值: \"10.96.0.0/12\" 为service的虚拟IP地址另外指定IP地址段 --service-dns-domain string 缺省值: \"cluster.local\" 为services另外指定域名, 例如： \"myorg.internal\". --skip-token-print 不打印出由 `kubeadm init` 命令生成的默认令牌。 --token string 这个令牌用于建立主从节点间的双向受信链接。格式为 [a-z0-9]{6}\\.[a-z0-9]{16} - 示例： abcdef.0123456789abcdef --token-ttl duration 缺省值: 24h0m0s 令牌被自动删除前的可用时长 (示例： 1s, 2m, 3h). 如果设置为 '0', 令牌将永不过期。 5.1 kubeadm init phase分段执行 --skip-phases 可用于跳过某些阶段 sudo kubeadm init phase control-plane all --config=configfile.yaml sudo kubeadm init phase etcd local --config=configfile.yaml sudo kubeadm init --skip-phases=control-plane,etcd --config=configfile.yaml 5.2 Kubeadm自定义组件配置参数 kubeadm ClusterConfiguration对象公开了extraArgs可以覆盖传递给控制平面组件（如APIServer，ControllerManager和Scheduler）的默认标志的字段。使用以下字段定义组件： apiServer controllerManager scheduler 该extraArgs字段由key: value对组成。覆盖控制平面组件的标志： 5.2.1 APIServer配置 apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 apiServer: extraArgs: advertise-address: 192.168.0.103 anonymous-auth: \"false\" enable-admission-plugins: AlwaysPullImages,DefaultStorageClass audit-log-path: /home/johndoe/audit.log 5.2.2 ControllerManager配置 apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 controllerManager: extraArgs: cluster-signing-key-file: /home/johndoe/keys/ca.key bind-address: 0.0.0.0 deployment-controller-sync-period: \"50\" 5.2.3 Scheduler配置 apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 scheduler: extraArgs: address: 0.0.0.0 config: /home/johndoe/schedconfig.yaml kubeconfig: /home/johndoe/kubeconfig.yaml 覆盖默认配置 kubeadm init --config=component.yaml 5.3 kubeadm自定义镜像 默认情况下，kubeadm从中提取图像k8s.gcr.io。如果请求的Kubernetes版本是CI标签（例如ci/latest） gcr.io/kubernetes-ci-images 允许的自定义为： 提供替代imageRepository的方法 k8s.gcr.io。 设置useHyperKubeImage为true使用HyperKube图像。 为了提供一个具体的imageRepository和imageTag为ETCD或DNS插件。 请注意，配置字段kubernetesVersion或命令行标志 --kubernetes-version会影响图像的版本 5.4 kubeadm将证书上载到集群 此机密将在2小时后自动过期。证书使用32字节密钥加密，可以使用进行指定--certificate-key。相同的密钥可以被用来下载时附加的控制平面节点通过使接合，证书 --control-plane和--certificate-key通过kubeadm join上传。 在到期后重新上传证书： kubeadm init phase upload-certs --upload-certs --certificate-key=SOME_VALUE --config=SOME_YAML_FILE 如果该标志--certificate-key未传递到kubeadm init， kubeadm init phase upload-certs则会自动生成一个新密钥。 以下命令可用于按需生成新密钥： kubeadm alpha certs certificate-key 6. kubeadmin join 6.1 流程详解 请参考 6.2 参数 --apiserver-advertise-address string #如果该节点应托管一个新的控制平面实例，则API Server的IP地址将通告其正在侦听的地址。如果未设置，将使用默认网络接口。 --apiserver-bind-port int32 Default: 6443 #如果该节点应承载一个新的控制平面实例，则该API服务器要绑定到的端口。 --certificate-key string ##使用此密钥可以解密由init上传的证书secret --config string #kubeadm配置文件的路径。 --control-plane #在此节点上创建一个新的控制平面实例 --cri-socket string #要连接的CRI套接字的路径。如果为空，则kubeadm将尝试自动检测此值；仅当您安装了多个CRI或具有非标准CRI插槽时，才使用此选项。 --discovery-file string #从中加载集群信息的文件或URL --discovery-token string #发现token,从中加载集群信息的文件或URL --discovery-token-ca-cert-hash stringSlice #对于基于令牌的发现，请验证根CA公共密钥是否与此哈希匹配（格式：“ ：”）。 --discovery-token-unsafe-skip-ca-verification #对于基于令牌的发现，允许加入时不使用--discovery-token-ca-cert-hash固定 -k, --experimental-kustomize string #kustomize静态pod清单的补丁的存储路径。 --ignore-preflight-errors stringSlice #检查清单，其错误将显示为警告。例如：“ IsPrivilegedUser，Swap”。值“ all”忽略所有检查的错误。 --node-name string #指定节点名称。 --skip-phases stringSlice #要跳过的阶段列表 --tls-bootstrap-token string #指定用于在加入节点时临时通过Kubernetes控制平面进行身份验证的令牌。 --token string #如果未提供这些值，则将它们用于发现令牌和tls-bootstrap令牌。 $ kubeadm join --skip-phases=preflight --config=config.yaml $ kubeadm join 192.168.211.40:6443 --token 5zw7z4.qjzipdh89aguvzn5 --discovery-token-ca-cert-hash sha256:167d0176ccd1c90b7373917940620fb7a48b245913eb25a05726345902f6213c 7. kubeadm config $ kubeadm config upload from-file $ kubeadm config view #查看集群中 kubeadm 配置所在的 ConfigMap $ kubeadm config print init-defaults #打印初始化配置 $ kubeadm config print join-defaults #打印join配置 $ kubeadm config images pull #根据配置文件拉取镜像 $ kubeadm config images list #显示需要拉取的镜像 k8s.gcr.io/kube-apiserver:v1.20.9 k8s.gcr.io/kube-controller-manager:v1.20.9 k8s.gcr.io/kube-scheduler:v1.20.9 k8s.gcr.io/kube-proxy:v1.20.9 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0 $ kubeadm config print init-defaults > kubeadm.conf 8. kubeadm token kubeadm init或输出的命令中返回的kubeadm join.. kubeadm token create --print-join-command 参考： github kubeadm 云原生圣经 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:59:04 "},"组件/Kubernetes-Kube-Proxy.html":{"url":"组件/Kubernetes-Kube-Proxy.html","title":"Kube proxy","keywords":"","body":"kubernetes kube-proxy1. 介绍2. 原理3. Service, Endpoints与Pod的关系4. kubernetes服务发现5. kubernetes发布(暴露)服务6. kube-proxy 不足7. 实现方式7.1 userspace mode7.2 Iptables mode7.3 ipvs mode8. 启动 kube-proxy 示例kubernetes kube-proxy tagsstart kube-proxy tagsstop 《地心引力》 1. 介绍 Kube-proxy 是一个简单的网络代理和负载均衡器，它的作用主要是负责Service的实现，具体来说，就是实现了内部从Pod到Service和外部的从NodePort向Service的访问，每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 iptables 等来为服务配置负载均衡（仅支持 TCP 和 UDP）。 kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行。 2. 原理 在k8s中，提供相同服务的一组pod可以抽象成一个service，通过service提供的统一入口对外提供服务，每个service都有一个虚拟IP地址（VIP）和端口号供客户端访问。kube-proxy存在于各个node节点上，主要用于Service功能的实现，具体来说，就是实现集群内的客户端pod访问service，或者是集群外的主机通过NodePort等方式访问service。在当前版本的k8s中，kube-proxy默认使用的是iptables模式，通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。从k8s的1.8版本开始，kube-proxy引入了IPVS模式，IPVS模式与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 kube-proxy负责为Service提供cluster内部的服务发现和负载均衡，它运行在每个Node计算节点上，负责Pod网络代理, 它会定时从etcd服务获取到service信息来做相应的策略，维护网络规则和四层负载均衡工作。在K8s集群中微服务的负载均衡是由Kube-proxy实现的，它是K8s集群内部的负载均衡器，也是一个分布式代理服务器，在K8s的每个节点上都有一个，这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。 service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。 简单来说: -> kube-proxy其实就是管理service的访问入口，包括集群内Pod到Service的访问和集群外访问service。 -> kube-proxy管理sevice的Endpoints，该service对外暴露一个Virtual IP，也成为Cluster IP, 集群内通过访问这个Cluster IP:Port就能访问到集群内对应的serivce下的Pod。 -> service是通过Selector选择的一组Pods的服务抽象，其实就是一个微服务，提供了服务的LB和反向代理的能力，而kube-proxy的主要作用就是负责service的实现。 -> service另外一个重要作用是，一个服务后端的Pods可能会随着生存灭亡而发生IP的改变，service的出现，给服务提供了一个固定的IP，而无视后端Endpoint的变化。 举个例子，比如现在有podA，podB，podC和serviceAB。serviceAB是podA，podB的服务抽象(service)。那么kube-proxy的作用就是可以将pod(不管是podA，podB或者podC)向serviceAB的请求，进行转发到service所代表的一个具体pod(podA或者podB)上。请求的分配方法一般分配是采用轮询方法进行分配。另外，kubernetes还提供了一种在node节点上暴露一个端口，从而提供从外部访问service的方式。比如这里使用这样的一个manifest来创建service 举个例子，比如现在有podA，podB，podC和serviceAB。serviceAB是podA，podB的服务抽象(service)。那么kube-proxy的作用就是可以将pod(不管是podA，podB或者podC)向serviceAB的请求，进行转发到service所代表的一个具体pod(podA或者podB)上。请求的分配方法一般分配是采用轮询方法进行分配。另外，kubernetes还提供了一种在node节点上暴露一个端口，从而提供从外部访问service的方式。比如这里使用这样的一个manifest来创建service apiVersion: v1 kind: Service metadata: labels: name: mysql role: service name: mysql-service spec: ports: - port: 3306 targetPort: 3306 nodePort: 30964 type: NodePort selector: mysql-service: \"true\" 上面配置的含义是在node上暴露出30964端口。当访问node上的30964端口时，其请求会转发到service对应的cluster IP的3306端口，并进一步转发到pod的3306端口。 3. Service, Endpoints与Pod的关系 Kube-proxy进程获取每个Service的Endpoints,实现Service的负载均衡功能 Service的负载均衡转发规则 访问Service的请求，不论是Cluster IP+TargetPort的方式；还是用Node节点IP+NodePort的方式，都被Node节点的Iptables规则重定向到Kube-proxy监听Service服务代理端口。kube-proxy接收到Service的访问请求后，根据负载策略，转发到后端的Pod。 4. kubernetes服务发现 Kubernetes提供了两种方式进行服务发现, 即环境变量和DNS, 简单说明如下: 1) 环境变量： 当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意: 要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为172.16.50.11:6379，则其对应的环境变量为: REDIS_MASTER_SERVICE_HOST=172.16.50.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://172.16.50.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://172.16.50.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=172.16.50.11 2) DNS：这是k8s官方强烈推荐的方式!!! 可以通过cluster add-on方式轻松的创建KubeDNS来对集群内的Service进行服务发现。 5. kubernetes发布(暴露)服务 kubernetes原生的，一个Service的ServiceType决定了其发布服务的方式。 -> ClusterIP：这是k8s默认的ServiceType。通过集群内的ClusterIP在内部发布服务。 -> NodePort：这种方式是常用的，用来对集群外暴露Service，你可以通过访问集群内的每个NodeIP:NodePort的方式，访问到对应Service后端的Endpoint。 -> LoadBalancer: 这也是用来对集群外暴露服务的，不同的是这需要Cloud Provider的支持，比如AWS等。 -> ExternalName：这个也是在集群内发布服务用的，需要借助KubeDNS(version >= 1.7)的支持，就是用KubeDNS将该service和ExternalName做一个Map，KubeDNS返回一个CNAME记录。 6. kube-proxy 不足 kube-proxy 目前仅支持 TCP 和 UDP，不支持 HTTP 路由，并且也没有健康检查机制。这些可以通过自定义 Ingress Controller 的方法来解决。 7. 实现方式 userspace：最早的负载均衡方案，它在用户空间监听一个端口，所有服务通过 iptables转发到这个端口，然后在其内部负载均衡到实际的 Pod。该方式最主要的问题是效率低，有明显的性能瓶颈。 iptables：目前推荐的方案，完全以 iptables 规则的方式来实现 service负载均衡。该方式最主要的问题是在服务多的时候产生太多的 iptables 规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题 ipvs：为解决 iptables 模式的性能问题，v1.11 新增了 ipvs 模式（v1.8 开始支持测试版，并在 v1.11 GA），采用增量式更新，并可以保证 service 更新期间连接保持不断开 winuserspace：同 userspace，但仅工作在 windows 节点上 注意：使用 ipvs 模式时，需要预先在每台 Node 上加载内核模块 nf_conntrack_ipv4, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh 等。 # load module modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 # to check loaded modules, use lsmod | grep -e ip_vs -e nf_conntrack_ipv4 # or cut -f1 -d \" \" /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4 7.1 userspace mode 在k8s v1.2后就已经被淘汰了，userspace的作用就是在proxy的用户空间监听一个端口，所有的svc都转到这个端口，然后proxy的内部应用层对其进行转发。proxy会为每个svc随机监听一个端口，并增加一个iptables规则，从客户端到 ClusterIP:Port 的报文都会被重定向到 Proxy Port，Kube-Proxy 收到报文后，通过 Round Robin (轮询) 或者 Session Affinity（会话亲和力，即同一 Client IP 都走同一链路给同一 Pod 服务）分发给对应的 Pod。所有流量都是在用户空间进行转发的，虽然比较稳定，但是效率不高。如下图为userspace的工作流程。 serspace是在用户空间，通过kube-proxy来实现service的代理服务, 其原理如下: 可见，userspace这种mode最大的问题是，service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode. userspace这种模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。其工作流程大体如下: 由此分析: 该模式请求在到达 iptables 进行处理时就会进入内核，而 kube-proxy 监听则是在用户态, 请求就形成了从用户态到内核态再返回到用户态的传递过程, 一定程度降低了服务性能。 7.2 Iptables mode iptables这种模式是从kubernetes1.2开始并在v1.12之前的默认模式。在这种模式下proxy监控kubernetes对svc和ep对象进行增删改查。并且这种模式使用iptables来做用户态的入口，而真正提供服务的是内核的Netilter，Netfilter采用模块化设计，具有良好的可扩充性。其重要工具模块IPTables从用户态的iptables连接到内核态的Netfilter的架构中，Netfilter与IP协议栈是无缝契合的，并允许使用者对数据报进行过滤、地址转换、处理等操作。这种情况下proxy只作为Controller。Kube-Proxy 监听 Kubernetes Master 增加和删除 Service 以及 Endpoint 的消息。对于每一个 Service，Kube Proxy 创建相应的 IPtables 规则，并将发送到 Service Cluster IP 的流量转发到 Service 后端提供服务的 Pod 的相应端口上。并且流量的转发都是在内核态进行的，所以性能更高更加可靠。 在这种模式下缺点就是在大规模的集群中，iptables添加规则会有很大的延迟。因为使用iptables，每增加一个svc都会增加一条iptables的chain。并且iptables修改了规则后必须得全部刷新才可以生效。 iptables 自定义几条链路：KUBE-SERVICES，KUBE-NODEPORTS，KUBE-POSTROUTING，KUBE-MARK-MASQ和KUBE-MARK-DROP五个链，并主要通过为 KUBE-SERVICES链（附着在PREROUTING和OUTPUT）增加rule来配制traffic routing 规则。 iptabels 模式下正常的通信链路： 在 PREROUTING的 chain里将 将经过PREROUTING里的数据包重定向到KUBE-SERVICES中 在自定义链kube-services里找到dst为目标地址的ip（ps：kube-services对于相同目标地址都有2给target，对于非pod之间的访问进入KUBE-MARK-MASQ，对于pod之间的访问进入KUBE-SVC-里)，找到对应的KUBE-SVC-(ps:只有有对应endpoint信息的才有KUBE-SVC-)，并找到KUBE-SVC-对应的KUBE-SEP-*。在SEP里会对source来自自身ip的打KUBE-MARK-MASQ，对于其他的做DNAT转换 # Masquerade -A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000 -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE # clusterIP and publicIP -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.98.154.163/32 -p tcp -m comment --comment \"default/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.98.154.163/32 -p tcp -m comment --comment \"default/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SERVICES -d 12.12.12.12/32 -p tcp -m comment --comment \"default/nginx: loadbalancer IP\" -m tcp --dport 80 -j KUBE-FW-4N57TFCL4MD7ZTDA # Masq for publicIP -A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx: loadbalancer IP\" -j KUBE-MARK-MASQ -A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx: loadbalancer IP\" -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx: loadbalancer IP\" -j KUBE-MARK-DROP # Masq for nodePort -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/nginx:\" -m tcp --dport 30938 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/nginx:\" -m tcp --dport 30938 -j KUBE-SVC-4N57TFCL4MD7ZTDA # load balance for each endpoints -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-UXHBWR5XIMVGXW3H -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TOYRWPNILILHH3OR -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -j KUBE-SEP-6QCC2MHJZP35QQAR # endpoint #1 -A KUBE-SEP-6QCC2MHJZP35QQAR -s 10.244.3.4/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-6QCC2MHJZP35QQAR -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 10.244.3.4:80 # endpoint #2 -A KUBE-SEP-TOYRWPNILILHH3OR -s 10.244.2.4/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-TOYRWPNILILHH3OR -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 10.244.2.4:80 # endpoint #3 -A KUBE-SEP-UXHBWR5XIMVGXW3H -s 10.244.1.2/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-UXHBWR5XIMVGXW3H -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 10.244.1.2:80 如果服务设置了 externalTrafficPolicy: Local 并且当前 Node 上面没有任何属于该服务的 Pod，那么在 KUBE-XLB-4N57TFCL4MD7ZTDA 中会直接丢掉从公网 IP 请求的包： -A KUBE-XLB-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx: has no local endpoints\" -j KUBE-MARK-DROP iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。这也导致目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。 iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。其工作流程大体如下: 由此分析: 该模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗，而且在大规模场景下，iptables 规则的条目会十分巨大，性能上还要再打折扣。 iptables的方式则是利用了linux的iptables的nat转发进行实现: apiVersion: v1 kind: Service metadata: labels: name: mysql role: service name: mysql-service spec: ports: - port: 3306 targetPort: 3306 nodePort: 30964 type: NodePort selector: mysql-service: \"true\" mysql-service对应的nodePort暴露出来的端口为30964，对应的cluster IP(10.254.162.44)的端口为3306，进一步对应于后端的pod的端口为3306。 mysql-service后端代理了两个pod，ip分别是192.168.125.129和192.168.125.131, 这里先来看一下iptables: $iptables -S -t nat ... -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/mysql-service:\" -m tcp --dport 30964 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/mysql-service:\" -m tcp --dport 30964 -j KUBE-SVC-67RL4FN6JRUPOJYM -A KUBE-SEP-ID6YWIT3F6WNZ47P -s 192.168.125.129/32 -m comment --comment \"default/mysql-service:\" -j KUBE-MARK-MASQ -A KUBE-SEP-ID6YWIT3F6WNZ47P -p tcp -m comment --comment \"default/mysql-service:\" -m tcp -j DNAT --to-destination 192.168.125.129:3306 -A KUBE-SEP-IN2YML2VIFH5RO2T -s 192.168.125.131/32 -m comment --comment \"default/mysql-service:\" -j KUBE-MARK-MASQ -A KUBE-SEP-IN2YML2VIFH5RO2T -p tcp -m comment --comment \"default/mysql-service:\" -m tcp -j DNAT --to-destination 192.168.125.131:3306 -A KUBE-SERVICES -d 10.254.162.44/32 -p tcp -m comment --comment \"default/mysql-service: cluster IP\" -m tcp --dport 3306 -j KUBE-SVC-67RL4FN6JRUPOJYM -A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS -A KUBE-SVC-67RL4FN6JRUPOJYM -m comment --comment \"default/mysql-service:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ID6YWIT3F6WNZ47P -A KUBE-SVC-67RL4FN6JRUPOJYM -m comment --comment \"default/mysql-service:\" -j KUBE-SEP-IN2YML2VIFH5RO2T 7.3 ipvs mode ipvs的模型中有两个角色： 调度器:Director，又称为Balancer。 调度器主要用于接受用户请求。 真实主机:Real Server，简称为RS。用于真正处理用户的请求。 kubernetes从1.8开始增加了IPVS支持，IPVS相对于iptables来说效率会更加高，使用ipvs模式需要在允许proxy的节点上安装ipvsadm，ipset工具包加载ipvs的内核模块。并且ipvs可以轻松处理每秒 10 万次以上的转发请求。 当proxy启动的时候，proxy将验证节点上是否安装了ipvs模块。如果未安装的话将回退到iptables模式。 并在Kubernetes 1.12成为kube-proxy的默认代理模式。ipvs模式也是基于netfilter，对比iptables模式在大规模Kubernetes集群有更好的扩展性和性能，支持更加复杂的负载均衡算法(如：最小负载、最少连接、加权等)，支持Server的健康检查和连接重试等功能。ipvs依赖于iptables，使用iptables进行包过滤、SNAT、masquared。ipvs将使用ipset需要被DROP或MASQUARED的源地址或目标地址，这样就能保证iptables规则数量的固定，我们不需要关心集群中有多少个Service了。 Kube-proxy IPVS mode 列出了各种服务在 IPVS 模式下的工作原理 $ ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.0.0.1:443 rr persistent 10800 -> 192.168.0.1:6443 Masq 1 1 0 TCP 10.0.0.10:53 rr -> 172.17.0.2:53 Masq 1 0 0 UDP 10.0.0.10:53 rr -> 172.17.0.2:53 Masq 1 0 0 注意，IPVS 模式也会使用 iptables 来执行 SNAT 和 IP 伪装（MASQUERADE），并使用 ipset 来简化 iptables 规则的管理： | ipset 名 | 成员 | 用途 | |--------------------------------|-----------------------------------------------------------------|--------------------------------------------------------------------------| | KUBE-CLUSTER-IP | All service IP + port | Mark-Masq for cases that masquerade-all=true or clusterCIDR specified | | KUBE-LOOP-BACK | All service IP + port + IP | masquerade for solving hairpin purpose | | KUBE-EXTERNAL-IP | service external IP + port | masquerade for packages to external IPs | | KUBE-LOAD-BALANCER | load balancer ingress IP + port | masquerade for packages to load balancer type service | | KUBE-LOAD-BALANCER-LOCAL | LB ingress IP + port with externalTrafficPolicy=local | accept packages to load balancer with externalTrafficPolicy=local | | KUBE-LOAD-BALANCER-FW | load balancer ingress IP + port with loadBalancerSourceRanges | package filter for load balancer with loadBalancerSourceRanges specified | | KUBE-LOAD-BALANCER-SOURCE-CIDR | load balancer ingress IP + port + source CIDR | package filter for load balancer with loadBalancerSourceRanges specified | | KUBE-NODE-PORT-TCP | nodeport type service TCP port | masquerade for packets to nodePort(TCP) | | KUBE-NODE-PORT-LOCAL-TCP | nodeport type service TCP port with externalTrafficPolicy=local | accept packages to nodeport service with externalTrafficPolicy=local | | KUBE-NODE-PORT-UDP | nodeport type service UDP port | masquerade for packets to nodePort(UDP) | | KUBE-NODE-PORT-LOCAL-UDP | nodeport type service UDP port withexternalTrafficPolicy=local | accept packages to nodeport service withexternalTrafficPolicy=local | 这种模式，Kube-Proxy 会监视 Kubernetes Service 对象 和 Endpoints，调用 Netlink 接口以相应地创建 IPVS 规则并定期与 Kubernetes Service 对象 和 Endpoints 对象同步 IPVS 规则，以确保 IPVS 状态与期望一致。访问服务时，流量将被重定向到其中一个后端 Pod。 以下情况下IPVS会使用iptables IPVS proxier将使用iptables，在数据包过滤，SNAT和支持NodePort类型的服务这几个场景中。具体来说，ipvs proxier将在以下4个场景中回归iptables。 kube-proxy 启动项设置了 –masquerade-all=true 如果kube-proxy以--masquerade-all = true开头，则ipvs proxier将伪装访问服务群集IP的所有流量，其行为与iptables proxier相同 注：master节点上也需要进行kubelet配置。因为ipvs在有些情况下是依赖iptables的，iptables中KUBE-POSTROUTING，KUBE-MARK-MASQ， KUBE-MARK-DROP这三条链是被 kubelet创建和维护的， ipvs不会创建它们。 在kube-proxy启动中指定集群CIDR 如果kube-proxy以--cluster-cidr = 开头，则ipvs proxier将伪装访问服务群集IP的群集外流量，其行为与iptables proxier相同。 为LB类型服务指定Load Balancer Source Ranges 当服务的LoadBalancerStatus.ingress.IP不为空并且指定了服务的LoadBalancerSourceRanges时，ipvs proxier将安装iptables。 支持 NodePort type service 为了支持NodePort类型的服务，ipvs将在iptables proxier中继续现有的实现。 kubernetes中ipvs实现原理图： 为什么每个svc会在ipvs网卡增加vip地址： 由于 IPVS 的 DNAT 钩子挂在 INPUT 链上，因此必须要让内核识别 VIP 是本机的 IP。这样才会过INPUT 链，要不然就通过OUTPUT链出去了。k8s 通过设置将service cluster ip 绑定到虚拟网卡kube-ipvs0。 ①因为service cluster ip 绑定到虚拟网卡kube-ipvs0上，内核可以识别访问的 VIP 是本机的 IP. ②数据包到达INPUT链. ③ipvs监听到达input链的数据包，比对数据包请求的服务是为集群服务，修改数据包的目标IP地址为对应pod的IP，然后将数据包发至POSTROUTING链. ④数据包经过POSTROUTING链选路，将数据包通过flannel网卡发送出去。从flannel虚拟网卡获得源IP. ⑤pod接收到请求之后，构建响应报文，改变源地址和目的地址，返回给客户端。 模拟ipvs模式： 新增DROP 链路 将svc地址即ipvs的vip地址为10.222.251.98的包丢弃 在指定的node节点执行 iptables -t filter -I INPUT -d 10.222.251.98 -j DROP 当该vip地址进入到这个input的链路的时候直接DROP 如下： 查看链路是否有包 watch -n 0.1 \"iptables --line-number -nvxL INPUT\" 因为当我们访问10.222.251.98这个地址的时候，ipvs会感知到这个ip是本机的vip地址，所以会线进入input的链路。所以我们在input增加了desc为10.222.251.98的DROP链路的话，会有限 之后恢复的话直接删除掉指定规则 iptables -D INPUT 1 ps： 1代表iptables -nL --line-number输出的行号 8. 启动 kube-proxy 示例 kube-proxy --kubeconfig=/var/lib/kubelet/kubeconfig --cluster-cidr=10.240.0.0/12 --feature-gates=ExperimentalCriticalPodAnnotation=true --proxy-mode=iptables 或者 KUBE_PROXY_ARGS=\"--bind-address=0.0.0.0 \\ --hostname-override=node147 \\ --kubeconfig=/etc/kubernetes/kube-proxy.conf \\ --logtostderr=true \\ --v=2 \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs\" 如果kubelet设置了–hostname-override选项，则kube-proxy也需要设置该选项，并且名字一致否则会出现找不到Node的情况。 参考： kube-proxy ipvs模式详解 kubernetes kube-proxy K8s: A Closer Look at Kube-Proxy Managing the kube-proxy add-on How to monitor kube-proxy Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-12 08:57:35 "},"组件/Kubectl/":{"url":"组件/Kubectl/","title":"Kubectl","keywords":"","body":"Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-04 04:54:31 "},"组件/Kubectl/kubectl-command.html":{"url":"组件/Kubectl/kubectl-command.html","title":"命令","keywords":"","body":"kubectl 命令Kubectl 自动补全kubectl runkubectl createkubectl create namespacekubectl create podkubectl create deploymentkubectl create configmapkubectl create quotakubectl create secretkubectl create job/cronjobkubectl create serviceaccountkubectl create rolebinding/role/clusterrolebinding/clusterrolekubectl execkubectl applykubectl replacekubectl exposekubectl explainkubectl annotatekubectl getkubectl get pods--selector-o=custom-columnsjsonpathgo-templatekubectl logskubectl cordonkubectl drainkubectl rolloutkubectl deletekubectl describekubectl labelkubectl setkubectl scalekubectl api-resourceskubectl plugin listkubectl diffkubectl authkubectl certificatekubectl config综合Helpful commands for debuggingRolling updates and rolloutskubectl 命令 tagsstart kubectl 命令 tagsstop Kubectl 自动补全 BASH source > ~/.bashrc # 在您的 bash shell 中永久的添加自动补全 您还可以为 kubectl 使用一个速记别名，该别名也可以与 completion 一起使用： alias k='kubectl' alias kg='k get' alias kd='k describe' alias kl='k logs' alias ke='k explain' alias kr='k replace' alias kc='k create' alias kgp='k get po' alias kgn='k get no' alias kge='k get ev' alias kex='k exec -it' alias kgc='k config get-contexts' alias ksn='k config set-context --current --namespace' alias kuc='k config use-context' alias krun='k run' export do='--dry-run=client -oyaml' export force='--grace-period=0 --force' source ZSH source > ~/.zshrc # 在您的 zsh shel kubectl run 创建并运行一个或多个容器镜像。 创建一个deployment 或job 来管理容器。 语法 $ kubectl run NAME --image=image [--env=\"key=value\"] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...] 示例： 启动nginx实例。 kubectl run nginx --image=nginx kubectl run nginx --image=nginx --restart=Never -n mynamespace kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml > pod.yaml kubectl run nginx --image=nginx --restart=Never --dry-run=client -o yaml | kubectl create -n mynamespace -f - kubectl run busybox --image=busybox --command --restart=Never -it -- env 启动带API组的nginx实例。将来被弃用 kubectl run nginx --image=nginx --generator=run-pod/v1 带有标签function=mantou的pod kubectl run nginx2 --image=nginx --labels function=mantou 多个标签 kubectl run nginx2 --image=nginx --labels function=mantou，disk=ssd # 创建nginx-app的deployment,并记录升级。 kubectl run nginx-app --image=nginx:1.11.0-alpine --record 使用默认命令启动 nginx 容器，但对该命令使用自定义参数（arg1 .. argN） kubectl run nginx --image=nginx -- ... 启动hazelcast实例，暴露容器端口 5701。 kubectl run hazelcast --image=hazelcast --port=5701 启动hazelcast实例，在容器中设置环境变量“DNS_DOMAIN = cluster”和“POD_NAMESPACE = default”。 kubectl run hazelcast --image=hazelcast --env=\"DNS_DOMAIN=cluster\" --env=\"POD_NAMESPACE=default\" 启动nginx实例，设置副本数5。 kubectl run nginx --image=nginx --replicas=5 配置cpu与内存的pod kubectl run nginx --image=nginx --restart=Never --requests='cpu=100m,memory=256Mi' --limits='cpu=200m,memory=512Mi' 运行 Dry 打印相应的API对象而不创建它们。 kubectl run nginx --image=nginx --dry-run 在特定的命令空间的一个pod运行多个容器 kubectl run test --image=nginx --image=redis --image=memcached --image=consul --restart=Nerver -n kube-public 启动一个单一的 nginx 实例，但是使用从 JSON 分析的一部分值来重载部署规格. kubectl run nginx --image=nginx --overrides='{ \"apiVersion\": \"v1\", \"spec\": { ... } }' 启动一个 busybox 的 pod 并将其保留在前台，如果它退出，请不要重新启动它. kubectl run -i -t busybox --image=busybox --restart=Never 启动 cron 作业计算 π 后2000位，每5分钟打印一次. kubectl run pi --schedule=\"0/5 * * * ?\" --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)' 注意： Flag --generator has been deprecated, has no effect and will be removed in the future --schedule= CronJob --restart=Always Deployment --restart=OnFailure Job --restart=Never Pod 如果不指定生成器，kubectl 将按以下顺序考虑其他参数： --schedule --restart kubectl create kubectl create namespace #创建一个命名空间 kubectl create namespace mynamespace kubectl create namespace myns -o yaml --dry-run kubectl create pod #通过pod.json文件创建一个pod。 kubectl create -f ./pod.json 通过stdin的JSON创建一个pod。 cat pod.json | kubectl create -f - API版本为v1的JSON格式的docker-registry.yaml文件创建资源。 kubectl create -f docker-registry.yaml --edit --output-version=v1 -o json kubectl create deployment #创建一个deployment kubectl create deployment nginx --image=nginx --restart=always kubectl create configmap #创建一个configmap echo -e \"foo3=lili\\nfoo4=lele\" > config.txt kubectl create configmap db-config --from-env-file=config.txt kubectl get cm db-config -o yaml kubectl create configmap config --from-literal=foo=lala --from-literal=foo2=lolo kubectl describe cm config echo -e \"var1=val1\\n# this is a comment\\n\\nvar2=val2\\n#anothercomment\" > config.env kubectl create cm configmap3 --from-env-file=config.env kubectl get cm configmap3 -o yaml echo -e \"var3=val3\\nvar4=val4\" > config4.txt kubectl create cm configmap4 --from-file=special=config4.txt kubectl describe cm configmap4 kubectl create quota #创建一个quota kubectl create quota myrq --hard=cpu=1,memory=1G,pods=2 --dry-run -o yaml kubectl create secret #创建一个secret kubectl create secret generic mysecret --from-literal=password=mypass echo -n admin > username kubectl create secret generic mysecret2 --from-file=username kubectl get secret mysecret2 -o yaml echo YWRtaW4K | base64 -d # on MAC it is -D, which decodes the value and shows 'admin' kubectl get secret mysecret2 -o jsonpath='{.data.username}{\"\\n\"}' | base64 -d kc secret generic my-secret --from-literal=APP_SECRET=sdcdcsdcsdcsdc kc secret generic my-secret --from-file=secret.txt kc secret generic my-secret --from-env-file=secret.env kubectl create job/cronjob #创建一个job kubectl create job pi --image=perl -- perl -Mbignum=bpi -wle 'print bpi(2000)' kubectl create job busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c 'while true; do echo hello; sleep 10;done' > job.yaml job.spec.activeDeadlineSeconds=30 #如果执行超过30秒则停止job， job.spec.completions=5 #运行5次 job.spec.parallelism=5 #并行运行5次 #创建一个cronjob，定时任务 kubectl create cronjob busybox --image=busybox --schedule=\"*/1 * * * *\" -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster' kubectl create cronjob time-limited-job --image=busybox --restart=Never --dry-run=client --schedule=\"* * * * *\" -o yaml -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster' > time-limited-job.yaml kubectl create serviceaccount #创建一个serviceaccount kubectl create sa myuser kubectl get sa -A kubectl get sa default -o yaml > sa.yaml kubectl run nginx --image=nginx --restart=Never --serviceaccount=myuser -o yaml --dry-run > pod.yaml kubectl create -f pod.yaml kubectl describe pod nginx # will see that a new secret called myuser-token-***** has been mounted kubectl create rolebinding/role/clusterrolebinding/clusterrole root@master:~/k8slib# k -n red create rolebinding secret-manager --role=secret-manager --user=jane rolebinding.rbac.authorization.k8s.io/secret-manager created root@master:~/k8slib# k -n blue create role secret-manager --verb=get --verb=list --resource=secrets role.rbac.authorization.k8s.io/secret-manager created root@master:~/k8slib# k -n blue create rolebinding secret-manager --role=secret-manager --user=jane rolebinding.rbac.authorization.k8s.io/secret-manager created #测试 root@master:~/k8slib# k -n red auth can-i get secrets --as jane yes root@master:~/k8slib# k -n red auth can-i get secrets --as tom no root@master:~/k8slib# k -n red auth can-i delete secrets --as jane no root@master:~/k8slib# k -n red auth can-i list secrets --as jane no root@master:~/k8slib# k -n blue auth can-i list secrets --as jane yes root@master:~/k8slib# k -n blue auth can-i get secrets --as jane yes root@master:~/k8slib# k -n blue auth can-i get pods --as jane no root@master:~/k8slib# k create clusterrole deploy-deleter --verb delete --resource deployments clusterrole.rbac.authorization.k8s.io/deploy-deleter created root@master:~/k8slib# k create clusterrolebinding deploy-deleter --user jane --clusterrole deploy-deleter clusterrolebinding.rbac.authorization.k8s.io/deploy-deleter created root@master:~/k8slib# k -n red create rolebinding deploy-deleter --user jim --clusterrole deploy-deleter rolebinding.rbac.authorization.k8s.io/deploy-deleter created root@master:~/k8slib# k auth can-i delete deployments --as jane yes root@master:~/k8slib# k auth can-i delete deployments --as jane -n default yes root@master:~/k8slib# k auth can-i delete deployments --as jane -n red yes root@master:~/k8slib# k auth can-i delete pods --as jane -n red no root@master:~/k8slib# k auth can-i delete deployments --as jim -n default no root@master:~/k8slib# k auth can-i delete deployments --as jim -A no root@master:~/k8slib# k auth can-i delete deployments --as jim -n red yes kubectl exec kubectl exec -it $(kubectl get pods -n kube-system| grep kube-apiserver|awk '{print $1}') -n kube-system -- /usr/local/bin/kube-apiserver -h |grep enable-admission-plugins kubectl apply # 将pod.json中的配置应用到pod kubectl apply -f ./pod.json # 将控制台输入的JSON配置应用到Pod cat pod.json | kubectl apply -f - 选项 -f, --filename=[]: 包含配置信息的文件名，目录名或者URL。 --include-extended-apis[=true]: If true, include definitions of new APIs via calls to the API server. [default true] -o, --output=\"\": 输出模式。\"-o name\"为快捷输出(资源/name). --record[=false]: 在资源注释中记录当前 kubectl 命令。 -R, --recursive[=false]: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --schema-cache-dir=\"~/.kube/schema\": 非空则将API schema缓存为指定文件，默认缓存到'$HOME/.kube/schema' --validate[=true]: 如果为true，在发送到服务端前先使用schema来验证输入。 继承自父命令的选项 --alsologtostderr[=false]: 同时输出日志到标准错误控制台和文件。 --certificate-authority=\"\": 用以进行认证授权的.cert文件路径。 --client-certificate=\"\": TLS使用的客户端证书路径。 --client-key=\"\": TLS使用的客户端密钥路径。 --cluster=\"\": 指定使用的kubeconfig配置文件中的集群名。 --context=\"\": 指定使用的kubeconfig配置文件中的环境名。 --insecure-skip-tls-verify[=false]: 如果为true，将不会检查服务器凭证的有效性，这会导致你的HTTPS链接变得不安全。 --kubeconfig=\"\": 命令行请求使用的配置文件路径。 --log-backtrace-at=:0: 当日志长度超过定义的行数时，忽略堆栈信息。 --log-dir=\"\": 如果不为空，将日志文件写入此目录。 --log-flush-frequency=5s: 刷新日志的最大时间间隔。 --logtostderr[=true]: 输出日志到标准错误控制台，不输出到文件。 --match-server-version[=false]: 要求服务端和客户端版本匹配。 --namespace=\"\": 如果不为空，命令将使用此namespace。 --password=\"\": API Server进行简单认证使用的密码。 -s, --server=\"\": Kubernetes API Server的地址和端口号。 --stderrthreshold=2: 高于此级别的日志将被输出到错误控制台。 --token=\"\": 认证到API Server使用的令牌。 --user=\"\": 指定使用的kubeconfig配置文件中的用户名。 --username=\"\": API Server进行简单认证使用的用户名。 --v=0: 指定输出日志的级别。 --vmodule=: 指定输出日志的模块，格式如下：pattern=N，使用逗号分隔。 kubectl replace 使用配置文件或stdin来替换资源。 语法： $ kubectl get TYPE NAME -o yaml 示例： 使用pod.json中的数据替换pod。 kubectl replace -f ./pod.json 基于 stdin 输入的 JSON 替换 pod $ cat pod.json | kubectl replace -f - 更新镜像版本(tag)到v4 kubectl get pod mypod -o yaml | sed 's/\\(image: myimage\\):.*$/\\1:v4/' | kubectl replace -f - 强制替换，删除原有资源，然后重新创建资源 kubectl replace --force -f ./pod.json kubectl expose 将资源暴露为新的Kubernetes Service。 指定deployment、service、replica set、replication controller或pod ，并使用该资源的选择器作为指定端口上新服务的选择器。deployment 或 replica set只有当其选择器可转换为service支持的选择器时，即当选择器仅包含matchLabels组件时才会作为暴露新的Service。 资源包括(不区分大小写)： pod（po），service（svc），replication controller（rc），deployment（deploy），replica set（rs） 语法 $ kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] 示例 为RC的nginx创建service，并通过Service的80端口转发至容器的8000端口上。 kubectl expose rc nginx --port=80 --target-port=8000 为使用副本集RS的复制的 nginx 创建一个服务,该服务使用80端口，并连接到容器的8000端口上 kubectl expose rs nginx --port=80 --target-port=8000 由“nginx-controller.yaml”中指定的type和name标识的RC创建Service，并通过Service的80端口转发至容器的8000端口上。 kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000 为一个 pod 的有效端口创建一个服务，该服务在444的端口使用名为“frontend” kubectl expose pod valid-pod --port=444 --name=frontend 基于上述服务创建第二个服务，将容器端口8443对外暴露为端口443，名称为 “nginx-https” kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https 为端口4100上的复制流应用创建一个服务，平衡UDP流量并命名为“video-stream”. kubectl expose rc streamer --port=4100 --protocol=udp --name=video-stream 为一个 nginx deployment 创建服务，该服务在端口80上运行，并连接到容器的8000端口. kubectl expose deployment nginx --port=80 --target-port=8000 kubectl explain 打印指定的定义资源 能的资源类型包括:pods (po)、services (svc)、replicationcontrollers (rc)、nodes (no)、events (ev)、componentstatus (cs)、limitranges (limits)、persistentvolume (pv)、persistentvolume eclures (pvc)、resourcequotas (quota)、namespaces (ns)、horizontalpodautoscalers (hpa)、endpoints (ep)。 --recursive展示完整的spec递归字段 kubectl explain deployment.spec --recursive $ kubectl explain deployment.spec KIND: Deployment VERSION: apps/v1 RESOURCE: spec DESCRIPTION: Specification of the desired behavior of the Deployment. DeploymentSpec is the specification of the desired behavior of the Deployment. FIELDS: minReadySeconds Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready) paused Indicates that the deployment is paused. progressDeadlineSeconds The maximum time in seconds for a deployment to make progress before it is considered to be failed. The deployment controller will continue to process failed deployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the deployment status. Note that progress will not be estimated during the time a deployment is paused. Defaults to 600s. replicas Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. revisionHistoryLimit The number of old ReplicaSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified. Defaults to 10. selector -required- Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this deployment. It must match the pod template's labels. strategy The deployment strategy to use to replace existing pods with new ones. template -required- Template describes the pods that will be created. kubectl explain deployments.spec # or kubectl explain deployment.spec # or kubectl explain deploy.spec kubectl annotate 更新注释 使用注释 'description' 和值 'my frontend' 更新 pod'foo'. ＃如果相同的注释多次设置，则只会应用最后一个值 kubectl annotate pods foo description='my frontend' 在 “pod.json” 中更新由类型和名称标识的 pod kubectl annotate -f pod.json description='my frontend' 使用注释 'description' 和值 'my frontend running nginx' 更新 pod'foo'，并覆盖任何现有值. kubectl annotate --overwrite pods foo description='my frontend running nginx' 更新命名空间中的所有 pod kubectl annotate pods --all description='my frontend running nginx' 仅当资源与版本1没有变化时才更新 pod'foo'. kubectl annotate pods foo description='my frontend running nginx' --resource-version=1 通过删除名为 “description” 的注释（如果存在）更新 pod'foo'. ＃不需要 --overwrite 标志. kubectl annotate pods foo description- kubectl get kubectl get namespaces kubectl get ns kubectl get event kubectl get pods kubectl get pods pod1 pod2 #获取多个pod kubectl get pods -n logging kubectl get pods -n logging -o wide kubectl get pods -A kubectl get po -A kubectl get po -A -o wide --- kubectl get po nginx -o yaml # or kubectl get po nginx -oyaml # or kubectl get po nginx --output yaml # or kubectl get po nginx --output=yaml --- kubectl get pods pods zongxun-test-1 -o yaml -n #查看完整创建配置信息 kubectl get pods pods zongxun-test-1 -o yaml -n | more #查看部分创建配置信息 kubectl get pods pods zongxun-test-1 -o json -n #查看创建配置信息 kubectl get pods -A --show-labels #显示pod并带label kubectl get pods -A -L controller-revision-hash #获取标为controller-revision-hash的pod kubectl get pods -A --watch #动态查看pod状态，如何杀掉pod，状态由running变为terminting #Get only the 'app=v2' pods kubectl get po -l app=v2 # or kubectl get po -l 'app in (v2)' # or kubectl get po --selector=app=v2 --selector # 获取包含 app=cassandra 标签的所有 Pods 的 version 标签 kubectl get pods --selector=app=cassandra -o \\ jsonpath='{.items[*].metadata.labels.version}' --field-selector 参考资料：https://blog.csdn.net/fly910905/article/details/102572878 metadata.name=my-service metadata.namespace!=default status.phase=Pending kubectl get pods --field-selector status.phase=Running -n kube-system #查看正在运行的pod kubectl get pods --field-selector status.phase=Runnin kubectl get ingress --field-selector foo.bar=baz kubectl get services --field-selector metadata.namespace!=default 链式选择器 kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always --sort-by排序 # 列出当前名字空间下所有 Services，按名称排序 kubectl get services --sort-by=.metadata.name # 列出 Pods，按重启次数排序 kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # 列举所有 PV 持久卷，按容量排序 kubectl get pv --sort-by=.spec.capacity.storage # 列出事件（Events），按时间戳排序 kubectl get events --sort-by=.metadata.creationTimestamp # 根据重启次数排序列出 pod kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns # 集群中运行着的所有镜像 root@master:~# k get pods -A -o=custom-columns='DATA:spec.containers[*].image' DATA nginx nginx calico/node:v3.16.6 calico/node:v3.16.6 calico/node:v3.16.6 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/kube-apiserver:v1.20.7 k8s.gcr.io/kube-controller-manager:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-scheduler:v1.20.7 kubernetesui/metrics-scraper:v1.0.6 # 列举 default 名字空间中运行的所有镜像，按 Pod 分组 kubectl get pods --namespace default --output=custom-columns=\"NAME:.metadata.name,IMAGE:.spec.containers[*].image\" NAME IMAGE frontend nginx # 除 \"k8s.gcr.io/coredns:1.6.2\" 之外的所有镜像 root@master:~# kubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=\"calico/node:v3.16.6\")].image' DATA nginx nginx k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/kube-apiserver:v1.20.7 k8s.gcr.io/kube-controller-manager:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-proxy:v1.20.7 k8s.gcr.io/kube-scheduler:v1.20.7 kubernetesui/metrics-scraper:v1.0.6 # 输出 metadata 下面的所有字段，无论 Pod 名字为何 kubectl get pods -A -o=custom-columns='DATA:metadata.*' kubectl get pods -o custom-columns='NAME:metadata.name' kubectl get pods \\ -o custom-columns='NAME:metadata.name,NODE:spec.nodeName' # Select all elements of a list kubectl get pods -o custom-columns='DATA:spec.containers[*].image' # Select a specific element of a list kubectl get pods -o custom-columns='DATA:spec.containers[0].image' # Select those elements of a list that match a filter expression kubectl get pods -o custom-columns='DATA:spec.containers[?(@.image!=\"nginx\")].image' # Select all fields under a specific location, regardless of their name kubectl get pods -o custom-columns='DATA:metadata.*' # Select all fields with a specific name, regardless of their location kubectl get pods -o custom-columns='DATA:..image' jsonpath # 获取包含 app=cassandra 标签的所有 Pods 的 version 标签 kubectl get pods --selector=app=cassandra -o jsonpath='{.items[*].metadata.labels.version}' # 获取全部节点的 ExternalIP 地址 kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # 假设你的 Pods 有默认的容器和默认的名字空间，并且支持 'env' 命令，可以使用以下脚本为所有 Pods 生成 ENV 变量。 # 该脚本也可用于在所有的 Pods 里运行任何受支持的命令，而不仅仅是 'env'。 for pod in $(kubectl get po --output=jsonpath={.items..metadata.name}); do echo $pod && kubectl exec -it $pod env; done #获取k8s集群节点和k8s版本 $ kubectl get nodes -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\\n{end}' master: v1.20.1 node1: v1.20.1 node2: v1.20.1 #获取k8s集群节点和docker版本 $ kubectl get nodes -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.containerRuntimeVersion}\\n{end}' master: docker://19.3.4 node1: docker://19.3.4 node2: docker://19.3.4 #获取node节点apparmor是否开启 kubectl get nodes -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {.status.conditions[?(@.reason==\"KubeletReady\")].message}\\n{end}' master: kubelet is posting ready status. AppArmor enabled node1: kubelet is posting ready status. AppArmor enabled node2: kubelet is posting ready status. AppArmor enabled go-template kubectl get myslclusters -A -o go-template --template='{{ range.items }}{{ printf \"%s|%s|%s\\n\" .metadata.namespace .metadata.name status.clusterStatus }} {{ end }}' kubectl logs kubectl logs my-pod # 获取 pod 日志（标准输出） kubectl logs my-pod |grep -i error # 获取 pod 日志关键词error（标准输出） kubectl logs -l name=myLabel # 获取含 name=myLabel 标签的 Pods 的日志（标准输出） kubectl logs my-pod --previous # 获取上个容器实例的 pod 日志（标准输出） kubectl logs my-pod -c my-container # 获取 Pod 容器的日志（标准输出, 多容器场景） kubectl logs -l name=myLabel -c my-container # 获取含 name=myLabel 标签的 Pod 容器日志（标准输出, 多容器场景） kubectl logs my-pod -c my-container --previous # 获取 Pod 中某容器的上个实例的日志（标准输出, 多容器场景） kubectl logs -f my-pod # 流式输出 Pod 的日志（标准输出） kubectl logs -f my-pod -c my-container # 流式输出 Pod 容器的日志（标准输出, 多容器场景） kubectl logs -f -l name=myLabel --all-containers # 流式输出含 name=myLabel 标签的 Pod 的所有日志（标准输出） # 返回pod ruby中已经停止的容器web-1的日志快照 $ kubectl logs -p -c ruby web-1 # 持续输出pod ruby中的容器web-1的日志 $ kubectl logs -f -c ruby web-1 # 仅输出pod nginx中最近的20条日志 $ kubectl logs --tail=20 nginx # 输出pod nginx中最近一小时内产生的所有日志 $ kubectl logs --since=1h nginx kubectl describe pods/zongxun-test-1 -n #查看监控信息 kubectl describe pods zongxun-test-1 -n kubeclt exec -n -ti bash kubectl exec -n -ti -c bash kubectl logs -f -n kube-system kubectl get cm -n kube-system kubectl edit cm -n kube-system coredns kubectl edit deployment -n kube-system coredns kubectl scale deployment -n kube-system coredns --replicas=0 kubectl delete pods -n kube-system 允许master节点部署pod，使用命令如下: kubectl taint nodes --all node-role.kubernetes.io/master- kubectl cordon 使node1不能调度pod kubectl cordon Which will cause the node to be in the status: Ready,SchedulingDisabled. 使node1恢复调度pod kubectl uncordon node1 kubectl drain 节点foo不可调度 kubectl cordon foo 使node1节点不可调度，并重新分配该节点上的pod kubectl drain node node1 --ignore-daemonsets --delete-local-data kubectl rollout 查看deployment的历史记录 kubectl rollout history deployment/abc 查看daemonset修订版3的详细信息 kubectl rollout history daemonset/abc --revision=3 查看deployment的状态 kubectl rollout status deployment/nginx 将deployment标记为暂停。＃只要deployment在暂停中，使用deployment更新将不会生效。 kubectl rollout pause deployment/nginx 恢复已暂停的 deployment kubectl rollout resume deployment/nginx 回滚到之前的deployment版本 kubectl rollout undo deployment/abc kubectl rollout undo --dry-run=true deployment/abc 回滚到daemonset 修订3版本 kubectl rollout undo daemonset/abc --to-revision=3 重启deployment kubectl rollout restart deployment xxx kubectl delete 使用 pod.json中指定的资源类型和名称删除pod。 kubectl delete -f ./pod.json 根据传入stdin的JSON所指定的类型和名称删除pod。 cat pod.json | kubectl delete -f - 删除名为“baz”和“foo”的Pod和Service。 kubectl delete pod,service baz foo 删除 Label name = myLabel的pod和Service。 kubectl delete pods,services -l name=myLabel 强制删除dead node上的pod kubectl delete pod foo --grace-period=0 --force 删除所有pod kubectl delete pods --all 优雅下线时间也可以设置成零，零的意思是立即从 Kubernetes 中删除此 Pod $ kubectl -f pod.yaml delete --force --grace-period=0 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"pod\" force deleted kubectl describe # 描述一个node $ kubectl describe nodes kubernetes-minion-emt8.c.myproject.internal # 描述一个pod $ kubectl describe pods/nginx # 描述pod.json中的资源类型和名称指定的pod $ kubectl describe -f pod.json # 描述所有的pod $ kubectl describe pods # 描述所有包含label name=myLabel的pod $ kubectl describe po -l name=myLabel # 描述所有被replication controller “frontend”管理的pod（rc创建的pod都以rc的名字作为前缀） $ kubectl describe pods frontend 选项 -f, --filename=[]: 用来指定待描述资源的文件名，目录名或者URL。 -l, --selector=\"\": 用于过滤资源的Label。 继承自父命令的选项 --alsologtostderr[=false]: 同时输出日志到标准错误控制台和文件。 --api-version=\"\": 和服务端交互使用的API版本。 --certificate-authority=\"\": 用以进行认证授权的.cert文件路径。 --client-certificate=\"\": TLS使用的客户端证书路径。 --client-key=\"\": TLS使用的客户端密钥路径。 --cluster=\"\": 指定使用的kubeconfig配置文件中的集群名。 --context=\"\": 指定使用的kubeconfig配置文件中的环境名。 --insecure-skip-tls-verify[=false]: 如果为true，将不会检查服务器凭证的有效性，这会导致你的HTTPS链接变得不安全。 --kubeconfig=\"\": 命令行请求使用的配置文件路径。 --log-backtrace-at=:0: 当日志长度超过定义的行数时，忽略堆栈信息。 --log-dir=\"\": 如果不为空，将日志文件写入此目录。 --log-flush-frequency=5s: 刷新日志的最大时间间隔。 --logtostderr[=true]: 输出日志到标准错误控制台，不输出到文件。 --match-server-version[=false]: 要求服务端和客户端版本匹配。 --namespace=\"\": 如果不为空，命令将使用此namespace。 --password=\"\": API Server进行简单认证使用的密码。 -s, --server=\"\": Kubernetes API Server的地址和端口号。 --stderrthreshold=2: 高于此级别的日志将被输出到错误控制台。 --token=\"\": 认证到API Server使用的令牌。 --user=\"\": 指定使用的kubeconfig配置文件中的用户名。 --username=\"\": API Server进行简单认证使用的用户名。 --v=0: 指定输出日志的级别。 --vmodule=: 指定输出日志的模块，格式如下：pattern=N，使用逗号分隔。 kubectl label 给名为foo的Pod添加label unhealthy=true。 kubectl label pods foo unhealthy=true 给名为foo的Pod修改label 为 'status' / value 'unhealthy'，且覆盖现有的value。 kubectl label --overwrite pods foo status=unhealthy 给 namespace 中的所有 pod 添加 label kubectl label pods --all status=unhealthy 仅当resource-version=1时才更新 名为foo的Pod上的label。 kubectl label pods foo status=unhealthy --resource-version=1 删除名为“app”的label 。（使用“ - ”减号相连） kubectl label po nginx1 nginx2 nginx3 app- # or kubectl label po nginx{1..3} app- # or kubectl label po -l app app- 给node设置标签 docker label node node1 name=ek8s-node-1 #设置 docker get node --show-labels #查看 删除一个Label，只需在命令行最后指定Label的key名并与一个减号相连即可： $ kubectl label nodes 1.1.1.1 role- 修改一个Label的值，需要加上--overwrite参数： $ kubectl label nodes 1.1.1.1 role=apache --overwrite kubectl set 升级镜像 kubectl set image pod/nginx nginx=nginx:1.7.1 kubectl get po nginx -o jsonpath='{.spec.containers[].image}{\"\\n\"}' kubectl scale kubectl scale deploy nginx --replicas=5 kubectl get po kubectl describe deploy nginx #Autoscale the deployment, pods between 5 and 10, targetting CPU utilization at 80% kubectl autoscale deploy nginx --min=5 --max=10 --cpu-percent=80 kubectl api-resources kubectl api-resources kubectl plugin list kubectl plugin list kubectl diff kubectl diff -f pod.json cat service.yaml | kubectl diff -f - kubectl diff -Rf /deployment/ | grep -v \"kubectl\\|recursive\\|generation\" | tee diff.log kubectl auth root@master:~/k8slib# k auth can-i delete deployments --as jane yes root@master:~/k8slib# k auth can-i delete deployments --as jane -n default yes root@master:~/k8slib# k auth can-i delete deployments --as jane -n red yes root@master:~/k8slib# k auth can-i delete pods --as jane -n red no root@master:~/k8slib# k auth can-i delete deployments --as jim -n default no root@master:~/k8slib# k auth can-i delete deployments --as jim -A no root@master:~/k8slib# k auth can-i delete deployments --as jim -n red yes kubectl certificate root@master:~/cks/RBAC# k get csr NAME AGE SIGNERNAME REQUESTOR CONDITION jane 7s kubernetes.io/kube-apiserver-client kubernetes-admin Pending 手动批准（或拒绝）证书签名请求 root@master:~/cks/RBAC# k certificate approve jane certificatesigningrequest.certificates.k8s.io/jane approved root@master:~/cks/RBAC# k get csr NAME AGE SIGNERNAME REQUESTOR CONDITION jane 73s kubernetes.io/kube-apiserver-client kubernetes-admin Approved,Issued 拒绝证书签名请求 root@master:~/cks/RBAC# k certificate deny jane kubectl config k8s@terminal:~$ k config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE gianna@infra-prod gianna@infra-prod gianna@infra-prod infra-prod infra-prod infra-prod restricted@infra-prod restricted@infra-prod restricted@infra-prod * workload-prod workload-prod workload-prod workload-stage workload-stage workload-stage k8s@terminal:~$ k config get-contexts -o name gianna@infra-prod infra-prod restricted@infra-prod workload-prod workload-stage k8s@terminal:~$ k config view -o jsonpath='{.contexts[*].name}' gianna@infra-prod infra-prod restricted@infra-prod workload-prod workload-stage k8s@terminal:~$ k config view -o jsonpath=\"{.contexts[*].name}\" | tr \" \" \"\\n\" gianna@infra-prod infra-prod restricted@infra-prod workload-prod k8s@terminal:~$ k config view --raw apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1ERXlNakU0TWpBeE5Gb1hEVE14TURFeU1ERTRNakF4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTG5GCmsyL0oyeWRQWVJhMlAwZ0xHRmloTnliNHUvM0N3VjVLOWgzKzd5MkNLbTNPenpIbG9GSFgzL0FjQW1VamxRWCsKSkQ5TU51VU1NR3pQaXFvUWsyVFBwZkVIa3ljaVk0Q0xQK0hpd3hKSExHdlhVazNlZGxGMEtGL3pTUkFWT3IvTAp5dnllOGtOazd4K3M5UUpoK1RsRElyUjY1RXFaNDJNbzJJbE5jYTAyMHZlR1ZEdUpHM1JyMHRoTEhsTkh6MU5sCmZNa1pBRnNtK3ZGYWxGdzJHRE5HdXFqZjgvMEVjQVJYQkhnM3kzMndxN2lwRis4OVlDNnJwcG1MSXVITXdkVVQKNzJpbENabm1XSStVMlh2aDVRT1drblhGdWpGa2xNRGk0cU9XREJXbFJ3NGZLRGNQUnVoRWlMUmZJOUlTOUlFcQovZ3N4Wmk1TTB4aURacVNXUmIwQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZQMS8xMTRQZU5LU3J3cFA3OUwzbDlYdWdCNlVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCaGViODl5aWZWU1hndWtHazJHUkFaZXBDbzRMTDhhaXdMT2ZzT1VwSVplMEFZWFJjQwpycVNyM2dWemJaekRZMmZKMjZTNEtNdzlOZUdWNlZsd2tZNTlvOTZWOGZOMUhlMUZNb2g5TStuUWV3RGFBT1V5CllaMTkzcjhubUxGZGVnR2RJSCtCQXkzYnJkcDJXZVZ0eW56OHRRZW0rSlVBdUFQR0k1OEpRYVNoYTdadXJiT0EKUVlXTHNsc1V6bVJBbkpTQXB1OUNZdU5jTXlUNkxmNTFaaWcyV0NxSHozZEh0SHVXR2NTK0J0NllmVFAvakZZQgpmNHVFTG1DQld0Mjg4bFJiczhhVjdjZXRKckQxOGsrN01nUTBrdWtaYjBMeG5iRndrelhDN0kyUkg0SjN0WTNuClNjSGVHMXQxVjlyLytwMHdDMmdVRUtFbGlQd0wyVGFzbmhHcQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://192.168.100.21:6443 name: gianna@infra-prod 创建 kubelet bootstrapping kubeconfig 文件 设置集群参数 [root@k8smaster ssl]# kubectl config set-cluster kubernetes –certificate-authority=/opt/kubernetes/ssl/ca.pem –embed-certs=true –server=https://192.168.137.171:6443 –kubeconfig=bootstrap.kubeconfig Cluster “kubernetes” set. 设置客户端认证参数 [root@k8smaster ssl]# kubectl config set-credentials kubelet-bootstrap –token=ad6d5bb607a186796d8861557df0d17f –kubeconfig=bootstrap.kubeconfig User “kubelet-bootstrap” set. 设置上下文参数 [root@k8smaster ssl]# kubectl config set-context default –cluster=kubernetes –user=kubelet-bootstrap –kubeconfig=bootstrap.kubeconfig Context “default” created. 选择默认上下文 [root@k8smaster ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig Switched to context “default”. kubectl config 设置别名 # Get current context alias krc='kubectl config current-context' # List all contexts alias klc='kubectl config get-contexts -o name | sed \"s/^/ /;\\|^ $(krc)$|s/ /*/\"' # Change current context alias kcc='kubectl config use-context \"$(klc | fzf -e | sed \"s/^..//\")\"' # Get current namespace alias krn='kubectl config get-contexts --no-headers \"$(krc)\" | awk \"{print \\$5}\" | sed \"s/^$/default/\"' # List all namespaces alias kln='kubectl get -o name ns | sed \"s|^.*/| |;\\|^ $(krn)$|s/ /*/\"' # Change current namespace alias kcn='kubectl config set-context --current --namespace \"$(kln | fzf -e | sed \"s/^..//\")\"' 综合 Helpful commands for debugging # Run busybox container k run busybox --image=busybox:1.28 --rm --restart=Never -it sh # Connect to a specific container in a Pod k exec -it busybox -c busybox2 -- /bin/sh # adding limits and requests in command kubectl run nginx --image=nginx --restart=Never --requests='cpu=100m,memory=256Mi' --limits='cpu=200m,memory=512Mi' # Create a Pod with a service kubectl run nginx --image=nginx --restart=Never --port=80 --expose # Check port nc -z -v -w 2 # NSLookup nslookup nslookup 10-32-0-10.default.pod Rolling updates and rollouts k set image deploy/nginx nginx=nginx:1.17.0 --record k rollout status deploy/nginx k rollout history deploy/nginx # Rollback to previous version k rollout undo deploy/nginx # Rollback to revision number k rollout undo deploy/nginx --to-revision=2 k rollout pause deploy/nginx k rollout resume deploy/nginx k rollout restart deploy/nginx kubectl run nginx-deploy --image=nginx:1.16 --replias=1 --record ✈更多阅读： https://kubernetes.io/zh/docs/tasks/tools/install-kubectl/ kubectl 常用 https://learnk8s.io/blog/kubectl-productivity Be fast with Kubectl 1.19 CKAD/CKA Kubectl Cheatsheet ✈推荐阅读： docker 命令 podman 命令 crictl 命令 kubectl 命令 operator-sdk 命令 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 08:59:22 "},"组件/Kubelet/":{"url":"组件/Kubelet/","title":"Kubelet","keywords":"","body":"kubernetes kubelet Overiview简介架构参数kubernetes kubelet Overiview tagsstart kubelet tagsstop 简介 kubelet 是在每个节点上运行的主要“节点代理”。它可以使用以下之一向 apiserver 注册节点：主机名；覆盖主机名的标志；或云提供商的特定逻辑。 kubelet 根据 PodSpec 工作。PodSpec 是描述 pod 的 YAML 或 JSON 对象。kubelet 采用一组通过各种机制（主要通过 apiserver）提供的 PodSpec，并确保这些 PodSpec 中描述的容器运行且健康。kubelet 不管理不是由 Kubernetes 创建的容器。它是一种负责向控制平面服务（API Server）传递信息的服务。它与 ETCD 交互以读取配置详细信息。它与主节点通信以获取命令并有效地工作。 除了来自 apiserver 的 PodSpec 之外，还可以通过三种方式将容器清单提供给 Kubelet。 File：在命令行上作为标志传递的路径。将定期监视此路径下的文件是否有更新。监控周期默认为 20 秒，可通过标志配置。 HTTP 端点：在命令行上作为参数传递的 HTTP 端点。该端点每 20 秒检查一次（也可以使用标志进行配置）。 HTTP 服务器：kubelet 还可以侦听 HTTP 并响应一个简单的 API（当前未规范）以提交新的清单。 架构 参数 kubelet Options Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 08:52:14 "},"组件/Kubelet/kubernetes-kubelet-configuration.html":{"url":"组件/Kubelet/kubernetes-kubelet-configuration.html","title":"配置","keywords":"","body":"kubernetes kubelet 配置1. 背景2. 配置模式2.1 kubeadm 配置 kubelet2.2 systemd 配置 kubelet drop-in 文件3. 配置注意kubernetes kubelet 配置 tagsstart kubelet tagsstop 1. 背景 kubeadm CLI 工具的生命周期与 kubelet 解耦， kubelet是运行在 Kubernetes 集群内每个节点上的守护进程。kubeadm CLI 工具由用户在 Kubernetes 初始化或升级时执行，而 kubelet 始终在后台运行。 由于 kubelet 是一个守护进程，它需要由某种 init 系统或服务管理器来维护。当使用 DEB 或 RPM 安装 kubelet 时，systemd 被配置为管理 kubelet。您可以改用其他服务管理器，但需要手动配置。 一些 kubelet 配置细节需要在集群中涉及的所有 kubelet 中相同，而其他配置方面需要基于每个 kubelet 进行设置，以适应给定机器的不同特征（例如操作系统、存储和网络） . 您可以手动管理 kubelet 的配置，但 kubeadm 现在提供了一种Kubelet ConfigurationAPI 类型来 集中管理您的 kubelet 配置 2. 配置模式 2.1 kubeadm 配置 kubelet 将集群级别的配置传播到每个 kubelet 您可以为 kubelet 提供kubeadm init和kubeadm join 命令使用的默认值。有趣的示例包括使用不同的容器运行时或设置服务使用的默认子网。 如果您希望您的服务使用子网10.96.0.0/12作为服务的默认子网，您可以将--service-cidr参数传递给 kubeadm： kubeadm init --service-cidr 10.96.0.0/12 现在从该子网分配服务的虚拟 IP。您还需要使用标志设置 kubelet 使用的 DNS 地址--cluster-dns。对于集群中每个管理器和节点上的每个 kubelet，此设置需要相同。kubelet 提供了一个版本化、结构化的 API 对象，可以在 kubelet 中配置大部分参数，并将此配置推送到集群中每个正在运行的 kubelet。这个对象被称为 Kubelet Configuration。允许用户指定标志，Kubelet Configuration例如集群 DNS IP 地址，以骆驼大小写键的值列表表示，如下例所示： apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10 通过调用kubeadm config print init-defaults --component-configs KubeletConfiguration，您可以看到此结构的所有默认值。 2.1.1 使用时的工作流程kubeadm init 当您调用kubeadm init时，kubelet 配置会被编组到磁盘/var/lib/kubelet/config.yaml，同时也会上传到 集群命名空间中的ConfigMap名字为kubelet-config 。 集群中所有 kubelet 的基线集群范围配置kube-system也会写入 kubelet 配置文件/etc/kubernetes/kubelet.conf此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。这解决了 将集群级配置传播到每个 kubelet的需要。 为了解决 提供特定于实例的配置详细信息的第二种模式，kubeadm 将环境文件写入到/var/lib/kubelet/kubeadm-flags.env，其中包含一个标志列表，以便在 kubelet 启动时传递给它。标志在文件中显示如下： KUBELET_KUBEADM_ARGS=\"--flag1=value1 --flag2=value2 ...\" 除了启动 kubelet 时使用的标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用不同的容器运行时套接字（--cri-socket）。 将这两个文件编组到磁盘后，如果您使用的是 systemd，kubeadm 会尝试运行以下两个命令： systemctl daemon-reload && systemctl restart kubelet 如果重新加载和重新启动成功，则kubeadm init继续正常的工作流程。 2.1.2 使用时的工作流程kubeadm join 当你运行时kubeadm join，kubeadm 使用 Bootstrap Token 凭证来执行 TLS 引导，它会获取下载 kubelet-configConfigMap 所需的凭证并将其写入/var/lib/kubelet/config.yaml. 动态环境文件的生成方式与kubeadm init. 接下来，kubeadm运行以下两条命令将新配置加载到 kubelet 中： systemctl daemon-reload && systemctl restart kubelet kubelet 加载新配置后，kubeadm 写入 KubeConfig 文件/etc/kubernetes/bootstrap-kubelet.conf，其中包含 CA 证书和 Bootstrap Token。kubelet 使用这些来执行 TLS Bootstrap 并获得一个唯一的凭证，该凭证存储在/etc/kubernetes/kubelet.conf. 当/etc/kubernetes/kubelet.conf文件被写入时，kubelet 已经完成了 TLS Bootstrap。Kubeadm在完成 TLS Bootstrap 后删除该文件/etc/kubernetes/bootstrap-kubelet.conf。 /etc/kubernetes/kubelet.conf内容示例： 在这里插入代码片[Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # This is a file that \"kubeadm init\" and \"kubeadm join\" generate at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, # the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. # KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 此文件为 kubelet 的 kubeadm 管理的所有文件指定默认位置。 用于 TLS Bootstrap 的 KubeConfig 文件是/etc/kubernetes/bootstrap-kubelet.conf，但仅在/etc/kubernetes/kubelet.conf不存在时使用。 具有唯一 kubelet 身份的 KubeConfig 文件是/etc/kubernetes/kubelet.conf. 包含 kubelet 的 Component Config 的文件是/var/lib/kubelet/config.yaml. 包含的动态环境文件KUBELET_KUBEADM_ARGS来自/var/lib/kubelet/kubeadm-flags.env. 可以包含用户指定的标志覆盖的文件KUBELET_EXTRA_ARGS来自 /etc/default/kubelet（对于 DEB）或/etc/sysconfig/kubelet（对于 RPM）。KUBELET_EXTRA_ARGS 在标志链中是最后一个，并且在设置冲突的情况下具有最高优先级。 2.2 systemd 配置 kubelet drop-in 文件 kubeadm附带有关 systemd 应如何运行 kubelet 的配置。请注意，kubeadm CLI 命令永远不会触及这个插入文件。 这个由kubeadm DEB或 RPM 包安装的配置文件被写入 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 并被 systemd 使用。它增强了 kubelet.service RPM或 kubelet.service DEB的基础： 3. 配置注意 由于硬件、操作系统、网络或其他主机特定参数的差异，一些主机需要特定的 kubelet 配置： 由 kubelet 配置标志指定的 DNS 解析文件的路径--resolv-conf可能因操作系统而异，或者取决于您是否使用 systemd-resolved. 如果此路径错误，则 kubelet 配置错误的 Node 上的 DNS 解析将失败。 Node API 对象.metadata.name默认设置为机器的主机名，除非您使用云提供商。--hostname-override如果您需要指定与机器主机名不同的节点名称，您可以使用该标志来覆盖默认行为。 目前，kubelet 无法自动检测容器运行时使用的 cgroup 驱动，但 的值--cgroup-driver必须与容器运行时使用的 cgroup 驱动相匹配，以保证 kubelet 的健康。 --container-runtime-endpoint=要指定容器运行时，您必须使用标志设置其端点 。 您可以通过在服务管理器中配置单个 kubelet 的配置来指定这些标志，例如 systemd。 参考： Configuring each kubelet in your cluster using kubeadm Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 10:23:27 "},"组件/Kubelet/kubernetes-kubelet-trash-recycling.html":{"url":"组件/Kubelet/kubernetes-kubelet-trash-recycling.html","title":"垃圾回收","keywords":"","body":"kubelet 垃圾回收机制1. Tips2. Experiments2.1 镜像回收（使用docker默认 --graph 参数：/var/lib/docker）2.2 镜像回收（使用自定义docker --graph 参数：/opt/docker）2.3 容器回收之 --maximum-dead-containers-per-container 参数2.4 --maximum-dead-containers-per-container 针对容器还是容器集2.5 容器回收之 --maximum-dead-containers 参数2.6 --maximum-dead-containers 对于非kubelet管理的容器是否计数kubelet 垃圾回收机制 tagsstart kubelet tagsstop 1. Tips Kubernetes的垃圾回收由kubelet进行管理，每分钟会查询清理一次容器，每五分钟查询清理一次镜像。在kubelet刚启动时并不会立即进行GC，即第一次进行容器回收为kubelet启动一分钟后，第一次进行镜像回收为kubelet启动五分钟后。 不推荐使用其它管理工具或手工进行容器和镜像的清理，因为kubelet需要通过容器来判断pod的运行状态，如果使用其它方式清除容器有可能影响kubelet的正常工作。 镜像的回收针对node结点上由docker管理的所有镜像，无论该镜像是否是在创建pod时pull的。而容器的回收策略只应用于通过kubelet管理的容器。 Kubernetes通过kubelet集成的cadvisor进行镜像的回收，有两个参数可以设置：--image-gc-high-threshold和--image-gc-low-threshold。当用于存储镜像的磁盘使用率达到百分之--image-gc-high-threshold时将触发镜像回收，删除最近最久未使用（LRU，Least Recently Used）的镜像直到磁盘使用率降为百分之--image-gc-low-threshold或无镜像可删为止。默认--image-gc-high-threshold为90，--image-gc-low-threshold为80。 容器的回收有三个参数可设置：--minimum-container-ttl-duration，--maximum-dead-containers-per-container和--maximum-dead-containers。从容器停止运行时起经过--minimum-container-ttl-duration时间后，该容器标记为已过期将来可以被回收（只是标记，不是回收），默认值为1m0s。一般情况下每个pod最多可以保留--maximum-dead-containers-per-container个已停止运行的容器集，默认值为2。整个node节点可以保留--maximum-dead-containers个已停止运行的容器，默认值为100。 如果需要关闭容器的垃圾回收策略，可以将--minimum-container-ttl-duration设为0（表示无限制），--maximum-dead-containers-per-container和--maximum-dead-containers设为负数。 --minimum-container-ttl-duration的值可以使用单位后缀，如h表示小时，m表示分钟，s表示秒。 当--maximum-dead-containers-per-container和--maximum-dead-containers冲突时，--maximum-dead-containers优先考虑。 对于那些由kubelet创建的但由于某些原因导致无名字（）的容器，会在到达GC时间点时被删除。 回收容器时，按创建时间排序，优先删除那些创建时间最早的容器。 到达GC时间点时，具体的GC过程如下： 1）遍历所有pod，使其满足--maximum-dead-containers-per-container； 2）经过上一步后如果不满足--maximum-dead-containers，计算值X=（--maximum-dead-containers）/（pod总数），再遍历所有pod，使其满足已停止运行的容器集个数不大于X且至少为1； 3）经过以上两步后如果还不满足--maximum-dead-containers，则对所有已停止的容器排序，优先删除创建时间最早的容器直到满足--maximum-dead-containers为止。 当某个镜像重新pull或启动某个pod用到该镜像时，该镜像的最近使用时间都会被更新。 Kubernetes的垃圾回收在1.1.4版本开始才渐渐完善，之前的版本存在比较多bug甚至不能发挥作用。 关于容器的回收需要特别注意pod的概念，比如，通过同一个yaml文件create一个pod，再delete这个pod，然后再create这个pod，此时之前的那个pod对应的容器并不会作为新创建pod的已停止容器集，因为这两个pod虽然同名，但已经不是同一个pod了。只有同一个pod中在运行过程中由于意外或其它情况停止的容器才算是这个pod的已停止容器集。 2. Experiments 2.1 镜像回收（使用docker默认 --graph 参数：/var/lib/docker） 结点上运行的docker设置的参数 --graph 使用默认的/var/lib/docker，指向/var文件系统，通过df -lh查看目前 /var 磁盘使用率为30%，启动kubelet设置镜像回收相关参数如下： --image-gc-high-threshold=40 --image-gc-low-threshold=35 此时任意创建两个使用不同镜像的pod，在node节点上可以看到新pull了三个images（pause镜像是启动pod必需的）： $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.11.150.76:5000/openxxs/iperf 1.2 1783511c56f8 3 months ago 279 MB 10.11.150.76:5000/centos 7 5ddf34d4d69b 8 months ago 172.2 MB pub.domeos.org/kubernetes/pause latest f9d5de079539 20 months ago 239.8 kB 此时查看/var磁盘使用率达到了41%，然后将使用10.11.150.76:5000/centos:7镜像的pod删除，等待GC的镜像回收时间点。然而五分钟过去了，什么事情也没有发生=_=!!。还记得 docker rmi 镜像时有个前提条件是什么吗？没错，要求使用该镜像的容器都已经被删除了才可以。前面删除pod只是停止了容器，并没有将容器删除。因此手工将对应的容器docker rm掉，再等待五分钟后，可以看到镜像已经被删除回收了： $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.11.150.76:5000/openxxs/iperf 1.2 1783511c56f8 3 months ago 279 MB pub.domeos.org/kubernetes/pause latest f9d5de079539 20 months ago 239.8 kB 结论：只有相关联的容器都被停止并删除回收后，才能将Kubernetes的镜像垃圾回收策略应用到该镜像上。 2.2 镜像回收（使用自定义docker --graph 参数：/opt/docker） 结点上运行的docker设置的参数--graph指向 /opt 磁盘，通过 df -lh 查看目前 /opt 磁盘使用率为 48% ，启动 kubelet 设置镜像回收相关参数如下： --image-gc-high-threshold=50 --image-gc-low-threshold=40 此时任意创建两个使用不同镜像的pod，在node节点上可以看到新pull了三个images： $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.11.150.76:5000/openxxs/iperf 1.2 1783511c56f8 3 months ago 279 MB 10.11.150.76:5000/centos 7 5ddf34d4d69b 8 months ago 172.2 MB pub.domeos.org/kubernetes/pause latest f9d5de079539 20 months ago 239.8 kB 此时查看/opt磁盘使用率达到了51%，然后将使用10.11.150.76:5000/centos:7镜像的pod删除，手工将对应的容器docker rm掉，等待GC的镜像回收时间点。然而五分钟过去了，十分钟过去了，docker images时centos镜像依旧顽固地坚守在阵地。 结论：目前Kubernetes的镜像垃圾回收策略可以在docker --graph 参数默认为 /var/lib/docker 时正常工作，当 --graph 设置为其它磁盘路径时还存在bug。 问题反馈在 Github 的相关 issue 里：https://github.com/kubernetes/kubernetes/issues/17994，可以继续跟进。 Append: 根据Github上的反馈，这个bug将在后续版本中解决，目前版本需要让设置了--graph的镜像垃圾回收生效，在启动kubelet时还需要加上参数 --docker-root=。 2.3 容器回收之 --maximum-dead-containers-per-container 参数 启动kubelet设置容器回收相关参数如下： --maximum-dead-containers-per-container=1 --minimum-container-ttl-duration=30s --maximum-dead-containers=100 创建一个只包含一个容器且该容器一运行就退出的pod，此时在node节点上可以看到该pod中的容器不断的创建退出创建退出： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fe969499164 10.11.150.76:5000/centos:7 \"/bin/bash\" 4 seconds ago Exited (0) 2 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_68cc6f03 555b5e7a8550 10.11.150.76:5000/centos:7 \"/bin/bash\" 24 seconds ago Exited (0) 22 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_ad4a5e39 94b30a0b32c2 10.11.150.76:5000/centos:7 \"/bin/bash\" 34 seconds ago Exited (0) 32 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_4027e3e1 d458e6a7d396 pub.domeos.org/kubernetes/pause:latest \"/pause\" 34 seconds ago Up 33 seconds k8s_POD.bdb2e1f5_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_09798975 GC的容器回收时间点到达时，可以看到创建时间大于30秒的已退出容器只剩下一个（pause容器不计算），且先创建的容器被优先删除： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5aae6157aeff 10.11.150.76:5000/centos:7 \"/bin/bash\" 46 seconds ago Exited (0) 45 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_f126d2a8 d458e6a7d396 pub.domeos.org/kubernetes/pause:latest \"/pause\" 2 minutes ago Up 2 minutes k8s_POD.bdb2e1f5_test-gc-pod-exit_default_92e8bd05-e9e6-11e5-974c-782bcb2a316a_09798975 结论：Kubernetes容器垃圾回收的--maximum-dead-containers-per-container参数设置可正常工作。 2.4 --maximum-dead-containers-per-container 针对容器还是容器集 启动kubelet设置容器回收相关参数如下： --maximum-dead-containers-per-container=1 --minimum-container-ttl-duration=30s --maximum-dead-containers=100 创建一个包含三个容器且这些容器一运行就退出的pod，此时在node节点上可以看到该pod中的容器不断的创建退出创建退出： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dec04bd28a03 10.11.150.76:5000/centos:7 \"/bin/bash\" 7 seconds ago Exited (0) 6 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_830a9375 7c94d4a963a7 10.11.150.76:5000/centos:7 \"/bin/bash\" 7 seconds ago Exited (0) 6 seconds ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_975d44d3 4f3e7e8ddfd5 10.11.150.76:5000/centos:7 \"/bin/bash\" 8 seconds ago Exited (0) 7 seconds ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_d024eb06 cb48cf2ba133 10.11.150.76:5000/centos:7 \"/bin/bash\" 12 seconds ago Exited (0) 11 seconds ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_b5ff7373 ec2941f046f0 10.11.150.76:5000/centos:7 \"/bin/bash\" 13 seconds ago Exited (0) 12 seconds ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_69b1a996 f831e8ed5687 10.11.150.76:5000/centos:7 \"/bin/bash\" 13 seconds ago Exited (0) 12 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_fbc02e2e ee972a4537fc pub.domeos.org/kubernetes/pause:latest \"/pause\" 14 seconds ago Up 13 seconds k8s_POD.bdb2e1f5_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_85b3c032 GC的容器回收时间点到达时，可以看到创建时间大于30秒的已退出容器剩下三个（pause容器不计算），且这三个容器正好是一组： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e4351e6855ae 10.11.150.76:5000/centos:7 \"/bin/bash\" 51 seconds ago Exited (0) 50 seconds ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_263dd820 990baa6e6a7a 10.11.150.76:5000/centos:7 \"/bin/bash\" 52 seconds ago Exited (0) 51 seconds ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_b16b5eaa c6916fb06d65 10.11.150.76:5000/centos:7 \"/bin/bash\" 53 seconds ago Exited (0) 51 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_1d8ea284 ee972a4537fc pub.domeos.org/kubernetes/pause:latest \"/pause\" About a minute ago Up About a minute k8s_POD.bdb2e1f5_test-gc-pod-exit_default_d1677c09-e9e7-11e5-974c-782bcb2a316a_85b3c032 结论：--maximum-dead-containers-per-container 的计数针对一个pod内的容器集而不是容器的个数。 2.5 容器回收之 --maximum-dead-containers 参数 启动kubelet设置容器回收相关参数如下： --maximum-dead-containers-per-container=2 --minimum-container-ttl-duration=30s --maximum-dead-containers=3 创建一个包含三个容器的pod，再删除该pod，再创建该pod，再删除该pod，这样就产生了8个已退出容器（包括两个pause容器）： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a28625d189df 10.11.150.76:5000/centos:7 \"/bin/bash\" 1 seconds ago Exited (0) Less than a second ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_48c11200 97aca44f0deb 10.11.150.76:5000/centos:7 \"/bin/bash\" 2 seconds ago Exited (0) 1 seconds ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_df34f48d 4e57b6c839ae 10.11.150.76:5000/centos:7 \"/bin/bash\" 3 seconds ago Exited (0) 2 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_afd622b2 12588fce1433 pub.domeos.org/kubernetes/pause:latest \"/pause\" 3 seconds ago Exited (2) Less than a second ago k8s_POD.bdb2e1f5_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_c9d4cbaa 621ed207d452 10.11.150.76:5000/centos:7 \"/bin/bash\" 4 seconds ago Exited (0) 3 seconds ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_c5cbddbb-e9ee-11e5-974c-782bcb2a316a_a91278cd 023c10fad4fd 10.11.150.76:5000/centos:7 \"/bin/bash\" 5 seconds ago Exited (0) 4 seconds ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_c5cbddbb-e9ee-11e5-974c-782bcb2a316a_6cc03f37 756eb7bb4b53 10.11.150.76:5000/centos:7 \"/bin/bash\" 5 seconds ago Exited (0) 4 seconds ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_c5cbddbb-e9ee-11e5-974c-782bcb2a316a_83312ec2 d54bdc22773e pub.domeos.org/kubernetes/pause:latest \"/pause\" 6 seconds ago Exited (2) 3 seconds ago k8s_POD.bdb2e1f5_test-gc-pod-exit_default_c5cbddbb-e9ee-11e5-974c-782bcb2a316a_ccb57220 GC的容器回收时间点到达时，可以看到已退出容器只剩下了三个，pause容器也被回收了： $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a28625d189df 10.11.150.76:5000/centos:7 \"/bin/bash\" 2 minutes ago Exited (0) 2 minutes ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_48c11200 97aca44f0deb 10.11.150.76:5000/centos:7 \"/bin/bash\" 2 minutes ago Exited (0) 2 minutes ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_df34f48d 4e57b6c839ae 10.11.150.76:5000/centos:7 \"/bin/bash\" 2 minutes ago Exited (0) 2 minutes ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_afd622b2 结论：Kubernetes容器垃圾回收的 --maximum-dead-containers参数设置可正常工作；pause容器也作为可回收容器被管理着；Tips第11条第3）点。 2.6 --maximum-dead-containers 对于非kubelet管理的容器是否计数 在第5个实验的基础上，手工创建一个container，等待GC的容器回收时间点到达，一分钟过去了，两分钟过去了，docker ps -a 显示的依然是4个容器： $ docker run -it 10.11.150.76:5000/openxxs/iperf:1.2 /bin/sh sh-4.2# exit exit $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 939b932dc7db 10.11.150.76:5000/openxxs/iperf:1.2 \"/bin/sh\" 2 minutes ago Exited (0) 2 minutes ago backstabbing_aryabhata a28625d189df 10.11.150.76:5000/centos:7 \"/bin/bash\" 12 minutes ago Exited (0) 12 minutes ago k8s_iperf3.5c8de29f_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_48c11200 97aca44f0deb 10.11.150.76:5000/centos:7 \"/bin/bash\" 12 minutes ago Exited (0) 12 minutes ago k8s_iperf2.5a36e29e_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_df34f48d 4e57b6c839ae 10.11.150.76:5000/centos:7 \"/bin/bash\" 12 minutes ago Exited (0) 12 minutes ago k8s_iperf1.57dfe29d_test-gc-pod-exit_default_c7612b59-e9ee-11e5-974c-782bcb2a316a_afd622b2 结论：Kubernetes容器垃圾回收的策略不适用于非kubelet管理的容器。 转自： Kubernetes中的垃圾回收机制 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-15 09:40:03 "},"组件/kubernetes-kube-apiserver.html":{"url":"组件/kubernetes-kube-apiserver.html","title":"kube apiserver","keywords":"","body":"kubernetes api-server Overview1. 简介2. 特征3. 组成4. 原理5. 命令参数6. API 路径7. 管理 Request7.1 Requests 类型7.2 Request 生命周期7.3 特殊 requests7.4 Watch operations7.5 乐观并发更新8. 访问 API Server8.1 kubectl8.2 kubectl proxy8.3 curl9. 调试 API Server9.1 Basic Logs9.2 Audit Logs9.3 激活附加日志9.4 调试 kubectl 请求10. API 版本11. 替代编码12. 常见响应代码13. API Server Internals14. CRD Control Loop15. OpenAPI 规范服务kubernetes api-server Overview tagsstart api-server tagsstop 1. 简介 api server 是 Kubernetes 集群的网关。它是 Kubernetes 集群中的所有用户、自动化和组件都可以访问的中心接触点。API Server通过 HTTP 实现 RESTful API，执行所有 API 操作，并负责将 API 对象存储到持久存储后端。 2. 特征 尽管 Kubernetes API Server很复杂，但从管理的角度来看，实际上管理起来相对简单。因为 API Server的所有持久状态都存储在 API Server外部的数据库中，所以服务器本身是无状态的，可以复制以处理请求负载和容错。通常，在高可用集群中，API Server会被复制 3 次。 API Server输出的日志可能非常健谈。它为接收到的每个请求至少输出一行。因此，将某种形式的日志滚动添加到 API Server非常重要，这样它就不会消耗所有可用的磁盘空间。但是，由于 API Server日志对于理解 API Server的操作至关重要，我们强烈建议将日志从 API Server传送到日志聚合服务，以便后续自省和查询以调试用户或组件对 API 的请求。 3. 组成 操作 Kubernetes API Server涉及三个核心功能： API management 服务器公开和管理 API 的过程 Request processing 处理来自客户端的单个 API 请求的最大功能集 Internal control loops 负责 API Server成功运行所需的后台操作的内部人员。 4. 原理 kube-apiserver 提供了 Kubernetes 的 REST API，实现了认证、授权、准入控制等安全校验功能，同时也负责集群状态的存储操作（通过 etcd）。 以 /apis/batch/v2alpha1/jobs 为例，GET 请求的处理过程如下图所示： POST 请求的处理过程为： （图片来自 OpenShift Blog） 5. 命令参数 kube-apiserver options 查看资源与版本 $ kubectl api-versions admissionregistration.k8s.io/v1beta1 apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1 apiregistration.k8s.io/v1beta1 apps/v1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 events.k8s.io/v1beta1 extensions/v1beta1 metrics.k8s.io/v1beta1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 scheduling.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 $ kubectl api-resources --api-group=storage.k8s.io NAME SHORTNAMES APIGROUP NAMESPACED KIND storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment 尽管 API 的主要用途是为单个客户端请求提供服务，但在处理 API 请求之前，客户端必须知道如何发出 API 请求。最终，API Server是一个 HTTP 服务器——因此，每个 API 请求都是一个 HTTP 请求。但是必须描述这些 HTTP 请求的特征，以便客户端和服务器知道如何通信。出于探索的目的，让 API Server实际启动并运行非常好，这样您就可以对它进行探索。您可以使用您有权访问的现有 Kubernetes 集群，也可以将minikube 工具用于本地 Kubernetes 集群。为了便于使用该curl工具来探索 API Server，请kubectl在proxy使用以下命令在 localhost:8001 上公开未经身份验证的 API Server的模式： kubectl proxy 6. API 路径 对 API Server的每个请求都遵循 RESTful API 模式，其中请求由请求的 HTTP 路径定义。所有 Kubernetes 请求都以前缀/api/（核心 API）或/apis/（按 API 组分组的 API）开头。这两组不同的路径主要是历史性的。API 组最初不存在于 Kubernetes API 中，因此原始或“核心”对象，如 Pod 和Services，在没有 API 组的情况下维护在'/api/'前缀下。后续 API 一般都添加在 API 组下，因此它们遵循'/apis//'路径。例如，该Job对象是batch API 组的一部分，因此可以在/apis/batch/v1/...下找到。 资源路径的另一个问题是资源是否已命名空间。Namespaces在 Kubernetes 中为对象添加了一层分组，命名空间资源只能在命名空间内创建，并且该命名空间的名称包含在命名空间资源的 HTTP 路径中。当然，有些资源并不存在于命名空间中（最明显的例子是NamespaceAPI 对象本身），在这种情况下，它们的 HTTP 路径中没有命名空间组件。 以下是命名空间资源类型的两种不同路径的组成部分： /api/v1/namespaces/// /apis///namespaces/// 以下是非命名空间资源类型的两种不同路径的组成部分： /api/v1// /apis//// 7. 管理 Request Kubernetes 中 API Server的主要用途是接收和处理 HTTP 请求形式的 API 调用。这些请求要么来自 Kubernetes 系统中的其他组件，要么是最终用户请求。无论哪种情况，它们都由 Kubernetes API Server以相同的方式处理。 7.1 Requests 类型 GET 最简单的请求是GET对特定资源的请求。这些请求检索与特定资源关联的数据。例如，对路径/api/v1/namespaces/default/pods/foo 的 HTTP请求检索名为foo的 Pod 的数据。 LIST 一个稍微复杂但仍然相当简单的请求是 acollection GET或LIST。这些是列出许多不同请求的请求。例如，对路径/api/v1/namespaces/default/podsGET的 HTTP请求检索命名空间中所有 Pod 的集合。requests 还可以选择指定标签查询，在这种情况下，仅返回与该标签查询匹配的资源。 POST 要创建资源，需要使用POST请求。请求的主体是应该创建的新资源。在POST请求的情况下，路径是资源类型（例如，/api/v1/namespaces/default/pods）。要更新现有资源，需要PUT向特定资源路径（例如/api/v1/namespaces/default/pods/foo）发出请求。 DELETE 当需要删除请求时，将使用DELETE对资源路径（例如/api/v1/namespaces/default/pods/foo）的 HTTP 请求。请务必注意，此更改是永久性的——在发出 HTTP 请求后，资源将被删除。 所有这些请求的内容类型通常是基于文本的 JSON ( application/json)，但最近发布的 Kubernetes 也支持 Protocol Buffers 二进制编码。一般来说，JSON 更适合客户端和服务器之间网络上的人类可读和可调试流量，但解析起来更加冗长和昂贵。常用工具 curl，可以提高 API 请求的性能和吞吐量。 除了这些标准请求之外，许多请求还使用 WebSocket 协议来启用客户端和服务器之间的流式会话。此类协议的示例类似attach命令。 7.2 Request 生命周期 为了更好地理解 API Server对每个不同的请求做了什么，我们将拆开并描述对 API Server的单个请求的处理。 7.2.1 Authentication（认证） 请求处理的第一阶段是身份验证，它建立与请求关联的身份。API Server支持多种不同的身份建立模式，包括客户端证书、承载令牌和 HTTP 基本身份验证。一般来说，应该使用客户端证书或不记名令牌进行身份验证；不鼓励使用 HTTP 基本身份验证。 除了这些建立身份的本地方法之外，身份验证是可插入的，并且有几个使用远程身份提供者的插件实现。其中包括对 OpenID Connect (OIDC) 协议以及 Azure Active Directory 的支持。这些身份验证插件被编译到 API Server和客户端库中。这意味着您可能需要确保命令行工具和 API Server的版本大致相同或支持相同的身份验证方法。 API Server还支持基于远程 webhook 的身份验证配置，其中身份验证决策通过不记名令牌转发委托给外部服务器。外部服务器验证来自最终用户的不记名令牌，并将身份验证信息返回给 API Server 7.2.2 RBAC/授权 在 API Server确定请求的身份后，它会继续对其进行授权。对 Kubernetes 的每个请求都遵循传统的 RBAC 模型。要访问请求，身份必须具有与请求关联的适当角色。Kubernetes RBAC 是一个丰富而复杂的话题，因此，我们用一整章的时间来详细介绍RBAC它的运作方式。就本 API Server摘要而言，在处理请求时，API Server会确定与请求关联的身份是否可以访问请求中的动词和 HTTP 路径的组合。如果请求的身份具有适当的角色，则允许继续。否则，返回 HTTP 403 响应。 7.2.3 Admission control（准入控制） 在请求经过身份验证和授权后，它会进入准入控制。身份验证和 RBAC 确定是否允许发生请求，这基于请求的 HTTP 属性（标头、方法和路径）。准入控制确定请求是否格式正确，并可能在处理请求之前对其进行修改。准入控制定义了一个可插拔的接口： apply(request): (transformedRequest, error) 如果任何准入控制器发现错误，则拒绝该请求。如果请求被接受，则使用转换后的请求而不是初始请求。准入控制器被串行调用，每个接收前一个的输出。 因为准入控制是一种通用的、可插入的机制，所以它被用于 API Server中的各种不同功能。例如，它用于向对象添加默认值。它还可以用于强制执行策略（例如，要求所有对象都具有特定标签）。此外，它还可以用来做一些事情，比如向每个 Pod 注入一个额外的容器。服务网格 Istio 使用这种方法透明地注入其 sidecar 容器。 准入控制器非常通用，可以通过基于 webhook 的准入控制动态添加到 API Server。 7.2.4 Validation（验证） 请求验证发生在准入控制之后，尽管它也可以作为准入控制的一部分来实现，尤其是对于基于 Webhook 的外部验证。此外，仅对单个对象执行验证。如果它需要更广泛的集群状态知识，则必须将其实现为准入控制器。 请求验证确保请求中包含的特定资源有效。例如，它确保Service对象的名称符合 DNS 名称的规则，因为最终 a 的名称Service将被编程到 KubernetesService发现 DNS 服务器中。通常，验证是作为按资源类型定义的自定义代码来实现的。 7.3 特殊 requests 除了标准的 RESTful 请求之外，API Server还有许多专门的请求模式，为客户端提供扩展功能： /proxy, /exec, /attach, /logs 第一类重要的操作是与 API Server的开放的、长时间运行的连接。这些请求提供流数据而不是立即响应。 该logs操作是我们描述的第一个流式请求，因为它最容易理解。事实上，默认情况下，logs根本不是流式传输请求。/logs客户端通过附加到特定 Pod 的路径末尾（例如 /api/v1/namespaces/default/pods/some-pod/logs）然后指定容器名称来请求获取 Pod 的日志作为 HTTP 查询参数和 HTTP GET请求。给定一个默认请求，API Server会以纯文本形式返回截至当前时间的所有日志，然后关闭 HTTP 请求。但是，如果客户端请求跟踪日志（通过指定follow查询参数），HTTP 响应由 API Server保持打开状态，并且在通过 API Server从 kubelet 接收新日志时将新日志写入 HTTP 响应。这种连接如图 4-1所示。。 logs是最容易理解的流式传输请求，因为它只是让请求保持打开状态并流入更多数据。其余操作利用 WebSocket 协议进行双向流数据。它们实际上还对这些流中的数据进行了多路复用，以通过 HTTP 启用任意数量的双向流。如果这一切听起来有点复杂，它确实是，但它也是 API Server表面区域的一个有价值的部分。 API Server实际上支持两种不同的流协议。它支持 SPDY 协议，以及 HTTP2/WebSocket。SPDY 正在被 HTTP2/WebSocket 取代，因此我们将注意力集中在 WebSocket 协议上。 完整的 WebSocket 协议超出了本书的范围，但它在许多其他地方都有记录。为了理解 API Server，您可以简单地将 WebSocket 视为一种将 HTTP 转换为双向字节流协议的协议。 然而，在这些流之上，Kubernetes API Server实际上引入了一个额外的多路复用流协议。这样做的原因是，对于许多用例，API Server能够为多个独立的字节流提供服务是非常有用的。例如，考虑在容器中执行命令。在这种情况下，实际上需要维护三个流（stdin、stderr和stdout）。 此流的基本协议如下：为每个流分配一个从 0 到 255 的数字。该流编号用于输入和输出，它在概念上模拟单个双向字节流。 对于通过 WebSocket 协议发送的每一帧，第一个字节是流号（例如，0），帧的其余部分是在该流上传输的数据（图 4-2）。 图 4-2。Kubernetes WebSocket 多通道框架的示例 使用此协议和 WebSockets，API Server可以在单个 WebSocket 会话中同时多路复用 256 字节流。 此基本协议用于会话exec，attach具有以下通道： 0 stdin用于写入进程的流。不会从此流中读取数据。 1 用于从进程stdout读取的输出流。stdout不应将数据写入此流。 2 用于从进程stderr读取的输出流。stderr不应将数据写入此流。 endpoint用于在/proxy客户端与集群内运行的容器和服务之间转发网络流量，而这些endpoint不会暴露在外部。为了流式传输这些 TCP 会话，协议稍微复杂一些。除了多路复用各种流之外，流的前两个字节（在流号之后，所以实际上是 WebSockets 帧中的第二和第三个字节）是正在转发的端口号，因此单个 WebSockets 帧用于/proxy查找如图4-3 所示。 7.4 Watch operations 除了流式数据，API Server还支持 watch API。watch监视更改的路径。因此，不是在某个时间间隔轮询可能的更新，这会引入额外的负载（由于快速轮询）或额外的延迟（由于慢速轮询），使用watch使用户能够通过单个连接获得低延迟更新。当用户通过向?watch=true某个 API Server请求添加查询参数来建立到 API Server的监视连接时，API Server切换到监视模式，并保持客户端和服务器之间的连接打开。同样，API Server返回的数据不再只是 API 对象——它是一个Watch包含更改类型（创建、更新、删除）和 API 对象本身的对象。通过这种方式，客户端可以观察和观察对该对象或对象集的所有更改。 7.5 乐观并发更新 API Server支持的另一个高级操作是能够乐观地执行 Kubernetes API 的并发更新。乐观并发背后的想法是能够在不使用锁的情况下执行大多数操作（悲观并发），并检测何时发生并发写入，拒绝两个并发写入中的后者。被拒绝的写入不会重试（由客户端检测冲突并自己重试写入）。 要理解为什么需要这种乐观并发和冲突检测，了解读/更新/写竞争条件的结构很重要。许多 API Server客户端的操作涉及三个操作： 从 API Server读取一些数据。 更新内存中的数据。 将其写回 API Server。 现在想象一下当这两种读/更新/写模式同时发生时会发生什么。 服务器 A 读取对象 O。 服务器 B 读取对象 O。 服务器 A 更新客户端内存中的对象 O。 服务器 B 更新客户端内存中的对象 O。 服务器 A 写入对象 O。 服务器 B 写入对象 O。 最后，服务器 A 所做的更改将丢失，因为它们被服务器 B 的更新覆盖。 解决这场比赛有两种选择。第一个是悲观锁（pessimistic lock），它可以防止在服务器 A 对对象进行操作时发生其他读取。这样做的问题是它会序列化所有操作，这会导致性能和吞吐量问题。 Kubernetes API Server实现的另一个选项是乐观并发（optimistic concurrency），它假设一切都会顺利进行，并且仅在尝试写入冲突时才检测到问题。为此，对象的每个实例都返回其数据和资源版本。此资源版本指示对象的当前迭代。发生写入时，如果设置了对象的资源版本，则只有当前版本与对象的版本匹配，才能写入成功。如果没有，则返回 HTTP 错误 409（冲突）并且客户端发霉重试。要了解这如何解决刚刚描述的读/更新/写竞争，让我们再次看一下操作： 服务器 A 读取版本为 v1 的对象 O。 服务器 B 读取版本为 v1 的对象 O。 服务器 A 在客户端的内存中更新版本为 v1 的对象 O。 服务器 B 更新客户端内存中版本 v1 的对象 O。 服务器 A 以版本 v1 写入对象 O；这是成功的。 服务器B在v1版本写入对象O，但对象在v2；返回 409 冲突。 8. 访问 API Server 有多种方式可以访问 Kubernetes 提供的 REST API： kubectl 命令行工具 SDK，支持多种语言 Go Python Javascript Java CSharp 其他 OpenAPI 支持的语言，可以通过 gen 工具生成相应的 client 8.1 kubectl kubectl get --raw /api/v1/namespaces kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods 8.2 kubectl proxy 当然，为了能够向 API 发出请求，有必要了解客户端可以使用哪些 API 对象。此过程通过客户端的 API 发现进行。要查看此过程的实际效果并以更实际的方式探索 API Server，我们可以自己执行此 API 发现。 首先，为了简化事情，我们使用kubectl命令行工具的内置proxy来为我们的集群提供身份验证 $ kubectl proxy --port=8080 & $ curl http://localhost:8080/api/ { \"versions\": [ \"v1\" ] } 或者 kubectl proxy 这将创建一个在本地计算机上的端口 8001 上运行的简单服务器。 我们可以使用这个服务器来启动 API 发现的过程。我们首先检查/api前缀： $ curl localhost:8001/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"10.0.0.1:6443\" } ] } 您可以看到服务器返回了一个类型为 的 API 对象APIVersions。这个对象为我们提供了一个versions字段，列出了可用的版本。 在这种情况下，只有一个，但对于/apis前缀，有很多。我们可以使用这个版本继续我们的调查： $ curl localhost:8001/api/v1 { \"kind\": \"APIResourceList\", \"groupVersion\": \"v1\", \"resources\": [ { …. { \"name\": \"namespaces\", \"singularName\": \"\", \"namespaced\": false, \"kind\": \"Namespace\", \"verbs\": [ \"create\", \"delete\", \"get\", \"list\", \"patch\", \"update\", \"watch\" ], \"shortNames\": [ \"ns\" ] }, … { \"name\": \"pods\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Pod\", \"verbs\": [ \"create\", \"delete\", \"deletecollection\", \"get\", \"list\", \"patch\", \"proxy\", \"update\", \"watch\" ], \"shortNames\": [ \"po\" ], \"categories\": [ \"all\" ] }, { \"name\": \"pods/attach\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Pod\", \"verbs\": [] }, { \"name\": \"pods/binding\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Binding\", \"verbs\": [ \"create\" ] }, …. ] } 返回的对象包含/api/v1/路径下暴露的资源列表。 描述 API（元 API 对象）的 OpenAPI/Swagger JSON 规范除了资源类型之外还包含各种有趣的信息。考虑Pod对象的 OpenAPI 规范： { \"name\": \"pods\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Pod\", \"verbs\": [ \"create\", \"delete\", \"deletecollection\", \"get\", \"list\", \"patch\", \"proxy\", \"update\", \"watch\" ], \"shortNames\": [ \"po\" ], \"categories\": [ \"all\" ] }, { \"name\": \"pods/attach\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Pod\", \"verbs\": [] } 查看此对象，该name字段提供此资源的名称。它还指示这些资源的子路径。由于推断英语单词的复数形式具有挑战性，因此 API 资源还包含一个singularName字段，该字段指示应用于该资源的单个实例的名称。我们之前讨论过命名空间。对象描述中的namespaced字段指示对象是否被命名空间。该kind字段提供了存在于 API 对象的 JSON 表示中的字符串，以指示它是什么类型的对象。该verbs字段是 API 对象中最重要的字段之一，因为它指示可以对该对象执行哪些类型的操作。这pods宾语包含所有可能的动词。动词的大部分效果从它们的名字就很明显了。需要更多解释的两个是watch和proxy。watch表示可以为资源建立监视。监视是一个长时间运行的操作，它提供有关对象更改的通知。proxy是一种专门的操作，它通过 API Server与网络端口建立代理网络连接。目前只有两种资源（Pod 和Services）支持proxy. 除了可以对对象执行的操作（描述为动词）之外，还有其他操作被建模为资源类型的子资源。例如，attach命令被建模为子资源： { \"name\": \"pods/attach\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Pod\", \"verbs\": [] } 8.3 curl $ TOKEN=$(cat /run/secrets/kubernetes.io/serviceaccount/token) $ CACERT=/run/secrets/kubernetes.io/serviceaccount/ca.crt $ curl --cacert $CACERT --header \"Authorization: Bearer $TOKEN\" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"10.0.1.149:443\" } ] } $ APISERVER=$(kubectl config view | grep server | cut -f 2- -d \":\" | tr -d \" \") $ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token'| cut -f2 -d':'| tr -d '\\t') $ curl $APISERVER/api --header \"Authorization: Bearer $TOKEN\" --insecure { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"10.0.1.149:443\" } ] } 9. 调试 API Server 当然，了解 API Server的实现是很好的，但通常情况下，您真正​​需要的是能够调试 API Server（以及调用 API 的客户端）的实际情况服务器）。实现这一点的主要方式是通过 API Server写入的日志。API Server导出两个日志流——the standard or basic logs，以及更有针对性的审计日志，它们试图捕获发出请求的原因和方式以及更改的 API Server状态。此外，可以打开更详细的日志记录来调试特定问题。 9.1 Basic Logs 默认情况下，API Server记录发送到 API Server的每个请求。此日志包括客户端的 IP 地址、请求的路径以及服务器返回的代码。如果意外错误导致服务器崩溃，服务器也会捕捉到这种恐慌，返回 500，并记录该错误。 I0803 19:59:19.929302 1 trace.go:76] Trace[1449222206]: \"Create /api/v1/namespaces/default/events\" (started: 2018-08-03 19:59:19.001777279 +0000 UTC m=+25.386403121) (total time: 927.484579ms): Trace[1449222206]: [927.401927ms] [927.279642ms] Object stored in database I0803 19:59:20.402215 1 controller.go:537] quota admission added evaluator for: { namespaces} 在此日志中，您可以看到它以I0803 19:59:…发出日志行时的时间戳开始，然后是发出它的行号，trace.go:76最后是日志消息本身。 9.2 Audit Logs 审计日志(Audit Logs)旨在使服务器管理员能够取证恢复服务器的状态以及导致 Kubernetes API 中数据当前状态的一系列客户端交互。例如，它使用户能够回答诸如“为什么要ReplicaSet扩大到 100 个？”、“谁删除了那个 Pod？”等问题。 审计日志有一个可插入的后端，用于记录它们的写入位置。通常，审计日志会写入文件，但也可以将它们写入 webhook。在任何一种情况下，记录的数据都是API 组中类型event的结构化 JSON 对象audit.k8s.io。 审计本身可以通过同一 API 组中的策略对象进行配置。此策略允许您指定将审计事件发送到审计日志的规则。 9.3 激活附加日志 Kubernetes 使用 leveled logging github.com/golang/glog 包进行日志记录。使用 API Server上的标志--v，您可以调整日志记录的详细程度。一般来说，Kubernetes 项目已将日志详细级别 2 ( --v=2) 设置为记录相关但不太垃圾邮件的合理默认值。如果您正在调查特定问题，您可以提高日志记录级别以查看更多（可能是垃圾邮件）消息。由于过多的日志记录会影响性能，我们建议不要在生产中使用详细的日志级别运行。如果您正在寻找更有针对性的日志记录，该--vmodule标志可以提高单个源文件的日志级别。这对于仅限于一小部分文件的非常有针对性的详细日志记录非常有用。 9.4 调试 kubectl 请求 除了通过日志调试 API Server外，还可以通过kubectl命令行工具调试与 API Server的交互。与 API Server一样，kubectl命令行工具通过github.com/golang/glog包记录并支持--v详细度标志。将详细程度设置为级别 10 ( --v=10) 会启用最大程度详细记录。在此模式下，kubectl记录它向服务器发出的所有请求，以及尝试打印curl可用于复制这些请求的命令。请注意，这些curl命令有时不完整。 此外，如果您想直接戳 API Server，我们之前用于探索 API 发现的方法效果很好。Running kubectl proxy在 localhost 上创建一个代理服务器，它会根据本地$HOME/.kube/config文件自动提供您的身份验证和授权凭据。运行代理后，使用该curl命令查看各种 API 请求相当简单。 10. API 版本 在 Kubernetes 中，API 最初是一个 alpha API（例如，v1alpha1）。alpha 名称表示 API 不稳定且不适合生产用例。采用 alpha API 的用户应该预料到 API 表面区域可能会在 Kubernetes 版本之间发生变化，并且 API 本身的实现可能会不稳定，甚至可能会破坏整个 Kubernetes 集群的稳定性。因此，在生产 Kubernetes 集群中禁用了 Alpha API。 一旦 API 成熟，它就会成为 beta API（例如，v1beta1）。Beta 指定表明 API 通常是稳定的，但可能存在错误或最终的 API 表面改进。通常，假设 Kubernetes 版本之间的 beta API 是稳定的，并且向后兼容是一个目标。但是，在特殊情况下，Beta API 可能仍然在 Kubernetes 版本之间不兼容。同样，beta API 旨在保持稳定，但可能仍然存在错误。Beta API 通常在生产 Kubernetes 集群中启用，但应谨慎使用。 最后，API 变得普遍可用（例如，v1）。通用可用性 (GA) 表明 API 是稳定的。这些 API 带有向后兼容性保证和弃用保证。在 API 被标记为计划删除后，Kubernetes 会将该 API 保留至少三个版本或一年，以先到者为准。弃用也不太可能。只有在开发出更好的替代方案后，API 才会被弃用。同样，GA API 是稳定的，适用于所有生产用途。 Kubernetes 的特定版本可以支持多个版本（alpha、beta 和 GA）。为了实现这一点，API Server始终具有三种不同的 API 表示：外部表示，即通过 API 请求进入的表示；内部表示，它是在 API Server中用于处理的对象的内存表示；和存储表示，它被记录到存储层以持久化 API 对象。API Server中的代码知道如何在所有这些表示之间执行各种转换。API 对象可以作为v1alpha1版本提交，作为v1对象存储，然后作为v1beta1对象或任何其他任意支持的版本检索。 11. 替代编码 除了支持请求对象的 JSON 编码外，API Server还支持另外两种请求格式。请求的编码由请求上的 Content-Type HTTP 标头指示。如果缺少此标头，则假定内容为application/json，表示 JSON 编码。第一个替代编码是 YAML，由application/yamlContent Type 指示。YAML 是一种基于文本的格式，通常被认为比 JSON 更易于阅读。几乎没有理由使用 YAML 进行编码以与服务器通信，但在某些情况下它可能很方便（例如，通过 手动将文件发送到服务器curl）。 请求和响应的另一种替代编码是协议缓冲区编码格式。Protocol Buffers 是一个相当有效的二进制对象协议。使用协议缓冲区可以为 API Server带来更高效和更高吞吐量的请求。事实上，许多 Kubernetes 内部工具都使用协议缓冲区作为它们的传输。Protocol Buffers 的主要问题是，由于它们的二进制性质，它们在有线格式中更难可视化/调试。此外，目前并非所有客户端库都支持 Protocol Buffers 请求或响应。协议缓冲区格式由application/vnd.kubernetes.protobufContent-Type 标头指示。 12. 常见响应代码 因为 API Server是作为 RESTful 服务器实现的，所以来自服务器的所有响应都与 HTTP 响应代码保持一致。除了典型的 200 表示 OK 响应和 500 表示内部服务器错误之外，以下是一些常见的响应代码及其含义： 202 公认。已收到创建或删除对象的异步请求。结果以状态对象响应，直到异步请求完成，此时将返回实际对象。 400 错误的请求。服务器无法解析或理解该请求。 401 未经授权。在没有已知身份验证方案的情况下接收到请求。 403 禁止。已收到并理解请求，但禁止访问。 409 冲突。已收到请求，但它是更新旧版本对象的请求。 422 无法处理的实体。请求已正确解析，但未通过某种验证. 13. API Server Internals 除了操作 HTTP RESTful 服务的基础知识之外，API Server还有一些内部服务来实现部分 Kubernetes API。通常，这些类型的控制循环在称为控制器管理器的单独二进制文件中运行。但是有一些控制循环必须在 API Server内运行。在每种情况下，我们都会描述其功能以及它存在于 API Server中的原因。 14. CRD Control Loop 自定义资源定义 (CRD) 是可以添加到正在运行的 API Server的动态 API 对象。因为创建 CRD 的行为本质上会创建 API Server必须知道如何提供服务的新 HTTP 路径，所以负责添加这些路径的控制器位于 API Server内部。随着委托 API Server的增加（在后面的章节中描述），这个控制器实际上已经大部分从 API Server中抽象出来了。它目前仍默认在进程中运行，但也可以在进程外运行。 CRD 控制回路的操作如下： for crd in AllCustomResourceDefinitions: if !RegisteredPath(crd): registerPath for path in AllRegisteredPaths: if !CustomResourceExists(path): markPathInvalid(path) delete custom resource data delete path 自定义资源路径的创建相当简单，但自定义资源的删除稍微复杂一些。这是因为删除自定义资源意味着删除与该类型资源关联的所有数据。这样一来，如果 CRD 被删除，然后在稍后的某个日期被读取，旧数据就不会以某种方式复活。 因此，在可以删除 HTTP 服务路径之前，首先将该路径标记为无效，以便无法创建新资源。然后，与 CRD 关联的所有数据都被删除，最后，路径被删除。 15. OpenAPI 规范服务 当然，了解可用于访问 API Server的资源和路径只是访问 Kubernetes API 所需信息的一部分。除了 HTTP 路径之外，您还需要知道要发送和接收的 JSON 有效负载。API Server还提供路径，为您提供有关 Kubernetes 资源模式的信息。这些模式使用 OpenAPI（以前的 Swagger）语法表示。您可以在以下路径下拉 OpenAPI 规范： /swaggerapi 在 Kubernetes 1.10 之前，服务于 Swagger 1.2 /openapi/v2 Kubernetes 1.10 及更高版本，服务于 OpenAPI (Swagger 2.0) OpenAPI 规范本身就是一个完整的主题，超出了本书的范围。无论如何，您不太可能需要在 Kubernetes 的日常操作中访问它。然而，各种客户端编程语言库是使用这些 OpenAPI 规范生成的（值得注意的例外是 Go 客户端库，它目前是手动编码的）。因此，如果您或用户在通过客户端库访问 Kubernetes API 的某些部分时遇到问题，那么第一站应该是 OpenAPI 规范，以了解 API 对象是如何建模的。 参考： Oreilly The Kubernetes API Server 官方 The Kubernetes API 什么是 Kubernetes API？ Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-16 07:09:52 "},"安全/":{"url":"安全/","title":"安全","keywords":"","body":"kubernetes Security Overview1. 前言2. 节点安全3. Kubernetes API 安全4. Kubernetes 网络安全5. Kubernetes Pod 安全性6. Kubernetes 数据安全7. 其他 Kubernetes 安全资源kubernetes Security Overview tagsstart 安全 tagsstop 1. 前言 保护 Kubernetes 似乎是一项神秘的任务。作为一个由一系列不同组件组成的高度复杂的系统，Kubernetes 不是您可以通过简单地启用安全模块或安装安全工具来保护的东西。 相反，Kubernetes 安全要求团队解决可能影响 Kubernetes 集群内各个层和服务的每种类型的安全风险。例如，团队必须了解如何保护 Kubernetes 节点、网络、Pod、数据等。 此外，Kubernetes 管理员需要了解 Kubernetes 原生提供哪些工具来解决安全问题，以及他们需要将哪些类型的第三方安全工具与其集群集成以填补空白。这也是一个复杂的话题，因为尽管 Kubernetes 不是一个安全平台，但它确实提供了某些类型的原生安全工具，例如基于角色的访问控制 (RBAC)。 如果您是 Kubernetes 新手，并且仍然试图了解整个事情的工作原理，更不用说如何保证它的安全，那么上述所有内容都会让人感到不知所措。但是，如果将它们分解成可消化的部分，这些概念实际上就足够简单了。为此，本文将介绍 Kubernetes 安全性的各个方面，并解释每个方面的基础知识，以及 Kubernetes 安全性在各个层级和服务级别的最佳实践。 2. 节点安全 节点是组成 Kubernetes 集群的服务器。在大多数情况下，节点运行某些版本的 Linux，尽管工作节点可能运行 Windows。节点可以是虚拟机或裸机服务器，但从安全角度来看，区别并不重要。 您应该采用与保护任何类型服务器相同的安全策略来保护 Kubernetes 节点。这些包括： 删除无关的应用程序、库和操作系统的其他组件，以最小化您的攻击面。使用极简 Linux 发行版（例如Alpine Linux）供应节点是最佳实践。 消除不必要的用户帐户。 确保除非绝对必要，否则不会以 root 身份运行。 在可用的情况下，部署操作系统强化框架，如 AppArmor 或 SELinux。 收集和分析操作系统日志以检测可能的违规行为。 如果您在任何类型的环境中都有在操作系统级别保护服务器的经验，那么您可能已经知道如何处理 Kubernetes 节点安全性。在节点级别，当您处理运行 Kubernetes 的节点时，安全考虑与处理任何类型的服务器并没有什么不同。 保护主节点和保护工作节点之间也没有根本区别。主节点的安全性更为重要，因为主节点上的漏洞可能会对您的集群造成更大的损害，但在主节点上保护操作系统的过程与工作节点相同。 3. Kubernetes API 安全 Kubernetes API 将集群的各个部分绑定在一起。因此，它是 Kubernetes 中最重要的保护资源之一。 Kubernetes API 默认设计为安全的。它只会响应它可以正确验证和授权的请求。 也就是说，API 身份验证和授权由您配置的 RBAC 策略管理。因此，API 仅与您的 RBAC 策略一样安全。因此，创建安全的 RBAC 策略来执行最小权限原则并在粒度基础上分配权限是确保 Kubernetes API 安全性的基本最佳实践。 此外，您可以利用准入控制器进一步增强 API 安全性。在 API 服务器已经对请求进行身份验证和授权后，准入控制器会评估请求。通过这种方式，准入控制器为不应该被允许的请求提供了一个可选的第二层防御。通过启用和配置准入控制器，您可以强制执行与 API 请求相关的各种安全规则。此处记录了可用的规则。 最后，通过配置安全证书并要求 API 服务器在安全端口而不是本地主机上服务请求，可以在网络级别保护 API 请求。 4. Kubernetes 网络安全 Kubernetes 网络安全类似于 pod 安全，因为它首先遵循您将用于保护任何网络的最佳实践。 您应该确保尽可能创建一个网络架构，将工作负载与公共 Internet 隔离，除非它们需要与之交互。您应该在网关级别部署防火墙以阻止来自违规主机的流量。您应该监控网络流量是否有违规迹象。这些都是您可以使用 Kubernetes 外部工具（例如服务网格）执行的所有步骤。 然而，Kubernetes 也提供了数量有限的原生工具来帮助保护网络资源。该工具以网络策略的形式出现。虽然网络策略本身并不是一项安全功能，但管理员可以使用它们来控制 Kubernetes 集群内的流量流动方式。 因此，您可以创建网络策略来执行诸如在网络级别隔离 pod 或过滤传入流量之类的事情。 网络策略不能替代保护 Kubernetes 之外的网络配置；相反，请将它们视为补充您构建到整个网络架构中的安全规则的附加资源。 5. Kubernetes Pod 安全性 在 Kubernetes 中，Pod 是一个容器或一组容器，用于运行应用程序。为了保护您的应用程序，您需要保护您的 pod。 pod 安全性的某些方面需要 Kubernetes 外部的实践。您应该在部署之前对应用程序执行安全测试，并在运行容器映像之前对其进行扫描。您应该从 pod 中收集日志并对其进行分析，以检测潜在的违规或滥用行为。 但是，Kubernetes 提供了一些原生工具，可以在 Pod 已经运行后加强 Pod 的安全性。这些包括： RBAC policies，可用于管理集群内的用户和服 务对 Pod 的访问。 Security contexts，它定义了 pod 运行的特权级别。 网络策略，（如上所述）可用于在网络级别隔离 pod。 准入控制器，它可以强制执行附加规则，这些规则基本上扩展了您基于 RBAC 建立的规则。 您使用的 Pod 安全工具的类型以及您配置它们的方式将取决于您的工作负载的性质。Pod 安全性没有一种万能的方法。例如，一些 pod 可以在网络级别完全相互隔离，而另一些则需要能够通信。 但是，无论您的具体要求是什么，您都应该评估可用于保护 Kubernetes pod 的资源，并确保充分利用它们。 6. Kubernetes 数据安全 Kubernetes 不存储任何数据，除了存在于运行中的 pod 中的非持久性数据和存储在节点上的日志数据。通常，您的集群创建和/或访问的数据将存在于某种外部存储系统中，该系统通过存储插件与 Kubernetes 交互。 因此，为了保护与 Kubernetes 相关的数据，您应该遵循用于保护任何大型存储系统内的数据的最佳实践。尽可能加密静态数据。使用访问控制工具来限制谁可以访问数据。确保管理存储池的服务器已正确锁定。备份数据以帮助保护自己免受数据盗窃或勒索软件攻击。 至于原生存在于 Kubernetes pod 和节点中的相对少量数据，Kubernetes 没有提供任何特殊工具来保护这些数据。但是，您可以通过使用上述最佳实践保护您的 pod 和节点来保护它。 7. 其他 Kubernetes 安全资源 除了上述特定于组件的安全实践之外，管理员还应该了解 Kubernetes 的其他安全资源。 审核日志 Kubernetes 可以选择详细记录集群中执行了哪些操作、谁执行了这些操作以及结果如何。使用这些审计日志，您可以全面审计您的集群，实时检测潜在的安全问题以及事后研究安全事件。 要使用审计日志，您必须首先创建一个审计策略，该策略定义 Kubernetes 将如何记录事件。Kubernetes 文档包含有关建立审计策略的完整详细信息。 此外，由于 Kubernetes 不提供帮助您大规模分析审计日志的工具，您可能希望将审计日志流式传输到外部监控或可观察性平台，以帮助您检测异常并提醒您注意违规行为。否则，您只能手动监控审计事件，这在规模上是不切实际的。 命名空间 在 Kubernetes 中，命名空间可用于将不同的工作负载相互隔离。 如果您愿意，您可以在单个命名空间中运行所有内容，但从安全角度来看，为集群中的每个团队和/或工作负载类型创建不同的命名空间是最佳实践。例如，您可能希望使用单独的命名空间将您的开发/测试环境与生产环境分开。 管理多个命名空间在一定程度上增加了 Kubernetes 的管理复杂性，因为在许多（但不是全部）情况下，您需要为每个命名空间创建不同的 RBAC 策略。但是，额外的努力是值得的，因为它可以最大限度地减少违规的潜在影响。 在 Kubernetes 中使用外部安全工具 尽管 Kubernetes 提供了某些类型的工具来帮助强化集群内运行的资源，但 Kubernetes 并非旨在帮助您检测或管理安全事件。 要大规模管理 Kubernetes 安全性，您很可能需要利用外部安全工具。这些工具可以执行几个重要的安全功能，包括： 扫描您的 RBAC 策略、安全上下文和其他配置数据，以识别可能导致安全问题的错误配置。 提供应用程序和容器映像扫描功能，您可以使用它来构建一个自动安全管道，以馈送到您的 Kubernetes 集群中。 收集、汇总和分析应用程序日志和审核日志，以帮助您检测可能预示违规的异常情况。 Kubernetes 有多种外部安全工具——当然包括 Sysdig，它是专门为帮助 DevOps 团队保护 Kubernetes 和其他云原生环境的所有层而构建的。 参考： Kubernetes Security 101: Fundamentals and Best Practices Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-08 03:08:06 "},"安全/Falco/":{"url":"安全/Falco/","title":"Falco","keywords":"","body":"falco 入门1. 简介2. 特点3. 检测4. 规则5. 警报6. 组件7. 架构8. 下载9. 安装9.1 Debian/Ubuntu9.2 CentOS/RHEL/Fedora/Amazon Linux9.3 openSUSE9.4 Linux 通用（二进制包）9.5 minikube 安装 falco9.6 kind 安装 falco10. 升级10.1 Debian/Ubuntu10.2 CentOS/RHEL/Fedora/Amazon Linux10.3 openSUSE11. 部署11.1 Kubernetes11.2 helm11.3 DaemonSet12. 运行12.1 将 Falco 作为 service 运行12.2 Docker 中运行12.3 Hot Reloadfalco 入门 tags： 安全,falco 印度电影已经在《宿敌》中将教材政治渗透问题得到体现。 1. 简介 Falco 项目是最初由Sysdig, Inc构建的开源运行时安全工具。Falco 被捐赠给 CNCF，现在是 CNCF 孵化项目。 Falco 是一个 Linux 安全工具，它使用系统调用来保护和监控系统。Falco 可用于 Kubernetes 运行时安全性。运行 Falco 最安全的方法是将 Falco 直接安装在主机系统上，这样 Falco 与 Kubernetes 隔离，以防万一。然后可以通过在 Kubernetes 中运行的只读代理来使用 Falco 警报。 您还可以使用 Helm 在 Kubernetes 中将 Falco 作为守护程序集直接运行。 Falco 通过以下方式使用系统调用来保护和监控系统： 在运行时从内核解析 Linux 系统调用 针对强大的规则引擎断言流 违反规则时发出警报 2. 特点 Falco 的主要特点： 加强安全性——创建由上下文丰富且灵活的引擎驱动的安全规则，以定义意外的应用程序行为。 降低风险 – 通过将 Falco 插入您当前的安全响应工作流程和流程，立即响应违反政策的警报。 利用最新规则 - 使用来自社区的恶意活动和 CVE 漏洞检测发出警报。 3. 检测 Falco 附带了一组默认规则，用于检查内核是否存在异常行为，例如： 使用特权容器提权 使用诸如setns 读取/写入知名目录，例如/etc, /usr/bin,/usr/sbin等 创建符号链接 所有权和模式更改 意外的网络连接或套接字突变 产生的进程使用execve 执行 shell 二进制文件，例如sh, bash, csh,zsh等 执行 SSH 二进制文件，例如ssh, scp,sftp等 变异 Linux coreutils可执行文件 变异登录二进制文件 变异shadowutil或passwd可执行文件，例如shadowconfig, pwck, chpasswd, getpasswd, change, useradd, etc, 等。 4. 规则 规则是 Falco 反对的项目。它们在 Falco 配置文件中定义，代表您可以在系统上检查的事件。请参阅 Falco规则。 5. 警报 警报是可配置的下游操作，可以像日志记录一样简单，也可以像STDOUT向客户端传递 gRPC 调用一样复杂。有关配置、理解和开发警报的更多信息，请参阅Falco 警报。Falco 可以将警报发送到： 标准输出 一份文件 系统日志 一个衍生的程序 HTTP[s] 端点 通过 gRPC API 的客户端 6. 组件 Falco 由四个主要组件组成： Userspace program - 是falco可用于与 Falco 交互的 CLI 工具。用户空间程序处理信号，解析来自 Falco驱动程序的信息，并发送警报。 Configuration - 定义 Falco 的运行方式、断言的规则以及如何执行警报。有关详细信息，请参阅配置。 Driver - 是一种遵循 Falco 驱动程序规范并发送系统调用信息流的软件。不安装驱动程序就无法运行 Falco。目前，Falco 支持以下驱动程序： （默认）基于C++ 库构建libscap的内核模块libsinsp 从相同模块构建的 BPF 探针 用户空间检测 有关详细信息，请参阅Falco 驱动程序。 Plugins - 允许用户通过添加新的事件源和可以从事件中提取信息的新字段来扩展 falco 库/falco 可执行文件的功能。有关更多信息，请参阅插件。 7. 架构 Falco 可以检测任何涉及进行 Linux 系统调用的行为并发出警报。Falco 警报是根据调用进程的特定系统调用、参数和属性触发的。Falco 在用户空间和内核空间运行。系统调用由 Falco 内核模块解释。然后使用用户空间中的库分析系统调用。然后使用配置了 Falco 规则的规则引擎过滤事件。然后向配置为 Syslog、文件、标准输出等的输出警告可疑事件。 8. 下载 两种下载和运行 Falco 的方式： 直接在 Linux 主机上运行 Falco 在容器中运行 Falco 用户空间程序，并在底层主机上安装驱动程序。 development stable rpm deb binary 下载容器镜像 | 标签 | 拉命令 | 描述 | |-----|---------------------------------------------------------|-------------------------------------------| | 最新的 | docker pull falcosecurity/falco-no-driver:latest | 最新版本 | | 版本 | docker pull falcosecurity/falco-no-driver: | 特定版本的 Falco，例如0.32.1 | | 最新的 | docker pull falcosecurity/falco-driver-loader:latest | falco-driver-loader带有构建工具链的最新版本 | | 版本 | docker pull falcosecurity/falco-driver-loader: | 特定版本，falco-driver-loader例如0.32.1带有构建工具链的 | | 最新的 | docker pull falcosecurity/falco:latest | falco-driver-loader包含的最新版本 | | 版本 | docker pull falcosecurity/falco: | 特定版本的 Falco，例如0.32.1包含falco-driver-loader | 9. 安装 9.1 Debian/Ubuntu 配置信任 falcosecurity GPG 密钥，配置 apt 存储库，并更新软件包列表： curl -s https://falco.org/repo/falcosecurity-3672BA8F.asc | apt-key add - echo \"deb https://download.falco.org/packages/deb stable main\" | tee -a /etc/apt/sources.list.d/falcosecurity.list apt-get update -y Install kernel headers: apt-get -y install linux-headers-$(uname -r) 安装 Falco： apt-get install -y falco 现在安装了 Falco、内核模块驱动程序和默认配置。Falco 作为一个系统单元运行。 有关如何使用 Falco 管理、运行和调试的信息，请参阅运行。 9.2 CentOS/RHEL/Fedora/Amazon Linux 配置信任 falcosecurity GPG 密钥并配置 yum 存储库： rpm --import https://falco.org/repo/falcosecurity-3672BA8F.asc curl -s -o /etc/yum.repos.d/falcosecurity.repo https://falco.org/repo/falcosecurity-rpm.repo Install kernel headers: yum -y install kernel-devel-$(uname -r) 安装 Falco： yum -y install falco 卸载 Falco： yum erase falco 9.3 openSUSE 配置信任 falcosecurity GPG 密钥并配置 zypper 存储库： rpm --import https://falco.org/repo/falcosecurity-3672BA8F.asc curl -s -o /etc/zypp/repos.d/falcosecurity.repo https://falco.org/repo/falcosecurity-rpm.repo Install kernel headers: zypper -n install kernel-default-devel-$(uname -r | sed s/\\-default//g) 注意— 如果上述命令未找到该包，您可能需要运行zypper -n dist-upgrade以修复它。可能需要重新启动系统。 安装 Falco： zypper -n install falco 现在安装了 Falco、内核模块驱动程序和默认配置。Falco 作为一个系统单元运行。 有关如何使用 Falco 管理、运行和调试的信息，请参阅运行。 卸载 Falco： zypper rm falco 9.4 Linux 通用（二进制包） 下载最新的二进制文件： curl -L -O https://download.falco.org/packages/bin/x86_64/falco-0.32.1-x86_64.tar.gz 安装 Falco： tar -xvf falco-0.32.1-x86_64.tar.gz cp -R falco-0.32.1-x86_64/* / 安装以下依赖项： 您的发行版的内核头文件 安装驱动程序后，您可以手动运行falco. 安装驱动程序 安装驱动程序的最简单方法是使用falco-driver-loader脚本。 默认情况下，它首先尝试使用dkms. 如果不可能，那么它会尝试将预建的下载到~/.falco/. 如果找到内核模块，则将其插入。 如果要安装 eBPF 探针驱动程序，请运行falco-driver-loader bpf. 它首先尝试在本地构建 eBPF 探针，否则将预构建下载到~/.falco/. 如果您使用的是 eBPF 探针，为了确保性能不会下降，请确保 您的内核已CONFIG_BPF_JIT启用 net.core.bpf_jit_enable设置为 1（启用 BPF JIT 编译器） 这可以通过验证sysctl -n net.core.bpf_jit_enable 可配置选项： DRIVERS_REPO- 设置此环境变量以覆盖预构建内核模块和 eBPF 探针的默认存储库 URL，不带斜杠。 即，https://myhost.mydomain.com或者如果服务器具有子目录结构https://myhost.mydomain.com/drivers。 驱动程序需要使用以下结构托管： /${driver_version}/falco_${target}_${kernelrelease}_${kernelversion}.[ko|o]whereko和分别o代表内核模块和eBPF探针。 例如，/a259b4bf49c3330d9ad6c3eed9eb1a31954259a6/falco_amazonlinux2_4.14.128-112.105.amzn2.x86_64_1.ko. 该falco-driver-loader脚本使用上述格式获取驱动程序。 9.5 minikube 安装 falco 在本地环境中在 Kubernetes 上使用 Falco 的最简单方法是在Minikube上。 当minikube使用默认--driver参数运行时，Minikube 会创建一个运行各种 Kubernetes 服务的 VM 和一个运行 Pod 等的容器框架。通常，不可能直接在 minikube VM 上构建 Falco 内核模块，因为 VM 没有包括正在运行的内核的内核头文件。 为了解决这个问题，从 Falco 0.13.1 开始，可以在https://s3.amazonaws.com/download.draios.com获得最后 10 个 minikube 版本的预构建内核模块。这允许下载后备步骤通过可加载的内核模块成功。Falco 现在在每个新的 Falco 版本中都支持 10 个最新版本的 minikube。Falco 目前保留以前构建的内核模块供下载，并继续提供有限的历史支持。 使用 VM 驱动程序使用 Minikube 创建集群，在本例中为 Virtualbox： minikube start --driver=virtualbox 检查所有 pod 是否正在运行： kubectl get pods --all-namespaces 将稳定图表添加到 Helm 存储库： helm repo add falcosecurity https://falcosecurity.github.io/charts helm repo update 使用 Helm 安装 Falco： helm install falco falcosecurity/falco 输出： AME: falco LAST DEPLOYED: Wed Jan 20 18:24:08 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Falco agents are spinning up on each node in your cluster. After a few seconds, they are going to start monitoring your containers looking for security issues. No further action should be required. Tip: You can easily forward Falco events to Slack, Kafka, AWS Lambda and more with falcosidekick. Full list of outputs: https://github.com/falcosecurity/charts/falcosidekick. You can enable its deployment with `--set falcosidekick.enabled=true` or in your values.yaml. See: https://github.com/falcosecurity/charts/blob/master/falcosidekick/values.yaml for configuration values. 检查日志以确保 Falco 正在运行： kubectl logs -l app=falco -f 输出： * Trying to dkms install falco module with GCC /usr/bin/gcc-5 DIRECTIVE: MAKE=\"'/tmp/falco-dkms-make'\" * Running dkms build failed, couldn't find /var/lib/dkms/falco/5c0b863ddade7a45568c0ac97d037422c9efb750/build/make.log (with GCC /usr/bin/gcc-5) * Trying to load a system falco driver, if present * Success: falco module found and loaded with modprobe Wed Jan 20 12:55:47 2021: Falco version 0.27.0 (driver version 5c0b863ddade7a45568c0ac97d037422c9efb750) Wed Jan 20 12:55:47 2021: Falco initialized with configuration file /etc/falco/falco.yaml Wed Jan 20 12:55:47 2021: Loading rules from file /etc/falco/falco_rules.yaml: Wed Jan 20 12:55:48 2021: Loading rules from file /etc/falco/falco_rules.local.yaml: Wed Jan 20 12:55:49 2021: Starting internal webserver, listening on port 8765 9.6 kind 安装 falco kind允许您在本地计算机上运行 Kubernetes。此工具要求您 安装和配置Docker 。目前不能直接在带有 Linuxkit 的 Mac 上运行，但这些说明适用于运行kind. 在kind集群上运行 Falco 如下： 创建一个配置文件。例如：kind-config.yaml 将以下内容添加到文件中： kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraMounts: # allow Falco to use devices provided by the kernel module - hostPath: /dev containerPath: /dev # allow Falco to use the Docker unix socket - hostPath: /var/run/docker.sock containerPath: /var/run/docker.sock 通过指定配置文件创建集群： kind create cluster --config=./kind-config.yaml 在 kind 集群中的一个节点上安装Falco。要将 Falco 安装为 Kubernetes 集群上的daemonset，请使用 Helm，方法同上。请参阅https://github.com/falcosecurity/charts/tree/master/falco。 10. 升级 根据您选择的安装方法，在将 Falco 升级到最新版本之前，您首先必须删除活动内核模块： rmmod falco 10.1 Debian/Ubuntu 如果您apt按照 Falco 0.27.0 或更早版本的说明配置了存储库，则可能需要更新存储库 URL： sed -i 's,https://dl.bintray.com/falcosecurity/deb,https://download.falco.org/packages/deb,' /etc/apt/sources.list.d/falcosecurity.list apt-get clean apt-get -y update 检查存在的apt-get update日志https://download.falco.org/packages/deb。 如果您按照提供的说明安装了 Falco ： apt-get --only-upgrade install falco 10.2 CentOS/RHEL/Fedora/Amazon Linux 如果您yum按照 Falco 0.27.0 或更早版本的说明配置了存储库，则可能需要更新存储库 URL sed -i 's,https://dl.bintray.com/falcosecurity/rpm,https://download.falco.org/packages/rpm,' /etc/yum.repos.d/falcosecurity.repo yum clean all 然后检查falcosecurity-rpm存储库是否指向https://download.falco.org/packages/rpm/： yum repolist -v falcosecurity-rpm 如果您按照提供的说明安装了 Falco ： 检查更新： yum check-update 如果有更新的 Falco 版本可用 yum update falco 10.3 openSUSE 如果您zypper按照 Falco 0.27.0 或更早版本的说明配置了存储库，则可能需要更新存储库 URL： sed -i 's,https://dl.bintray.com/falcosecurity/rpm,https://download.falco.org/packages/rpm,' /etc/zypp/repos.d/falcosecurity.repo zypper refresh 然后检查falcosecurity-rpm存储库是否指向https://download.falco.org/packages/rpm/： zypper lr falcosecurity-rpm 如果您按照提供的说明安装了 Falco ： zypper update falco 11. 部署 11.1 Kubernetes Falco 可以作为DaemonSet部署在 Kubernetes 中，以监控集群每个节点中的系统事件。 11.2 helm 在 Kubernetes 中安装 Falco 的最简单方法之一是使用Helm。Falco 社区支持官方 helm chart。 11.3 DaemonSet Falco 也可以手动安装在 Kubernetes 中。在这种情况下，您负责提供 DaemonSet 对象 YAML 定义并将其部署到您的集群中。有关更多详细信息，您可以在此处找到示例。 12. 运行 12.1 将 Falco 作为 service 运行 systemctl enable falco systemctl start falco 您还可以使用 . 查看 Falco 日志journalctl。 journalctl -fu falco 12.2 Docker 中运行 即使使用容器镜像，Falco 也需要在主机上安装内核头文件作为正确构建驱动程序（内核模块或eBPF 探针）的先决条件。当预建驱动程序已经可用时，不需要此步骤。 Falco 提供了一组官方docker 镜像。图像可以通过以下两种方式使用： 最低特权（推荐） 完全特权 12.2.1 最低特权（推荐） a. 安装内核模块： 可以直接在主机上使用官方的安装方式 或者，您可以临时使用特权容器在主机上安装驱动程序： docker pull falcosecurity/falco-driver-loader:latest docker run --rm -i -t \\ --privileged \\ -v /root/.falco:/root/.falco \\ -v /proc:/host/proc:ro \\ -v /boot:/host/boot:ro \\ -v /lib/modules:/host/lib/modules \\ -v /usr:/host/usr:ro \\ -v /etc:/host/etc:ro \\ falcosecurity/falco-driver-loader:latest falcosecurity/falco-driver-loader图像只是包装了脚本falco-driver-loader。 b. 使用 Docker 以最小权限原则在容器中运行 Falco ： docker pull falcosecurity/falco-no-driver:latest docker run --rm -i -t \\ -e HOST_ROOT=/ \\ --cap-add SYS_PTRACE --pid=host $(ls /dev/falco* | xargs -I {} echo --device {}) \\ -v /var/run/docker.sock:/var/run/docker.sock \\ falcosecurity/falco-no-driver:latest 如果您在启用了 AppArmor LSM 的系统（例如 Ubuntu）上运行 Falco，您还需要传递--security-opt apparmor:unconfined给docker run上面的命令。 您可以使用以下命令验证您是否启用了 AppArmor： docker info | grep -i apparmor 请注意，每个 CPU s /dev/falco* | xargs -I {} echo --device {}输出一个--device /dev/falcoX选项（即，仅由 Falco 的内核模块创建的设备）。此外，-e HOST_ROOT=/这是必要的，因为--device无法将设备重新映射到/host/dev/. 要使用 eBPF 驱动程序以最低权限模式运行 Falco，我们列出了所有必需的功能： 在小于 5.8 的内核上，Falco 需要CAP_SYS_ADMIN，CAP_SYS_RESOURCE、CAP_SYS_PTRACE 在kernels >=5.8 上，CAP_BPF并且CAP_PERFMON被分离出来CAP_SYS_ADMIN，因此所需的功能是CAP_BPF, CAP_PERFMON, CAP_SYS_RESOURCE, CAP_SYS_PTRACE. 不幸的是，Docker 还不支持通过该--cap-add选项添加两个新引入的功能。出于这个原因，我们继续使用CAP_SYS_ADMIN，因为它仍然允许执行CAP_BPF和授予的相同操作CAP_PERFMON。在不久的将来，Docker 将支持添加这两个功能，我们将能够替换CAP_SYS_ADMIN. 安装 eBPF 探针 docker pull falcosecurity/falco-driver-loader:latest docker run --rm -i -t \\ --privileged \\ -v /root/.falco:/root/.falco \\ -v /proc:/host/proc:ro \\ -v /boot:/host/boot:ro \\ -v /lib/modules:/host/lib/modules:ro \\ -v /usr:/host/usr:ro \\ -v /etc:/host/etc:ro \\ falcosecurity/falco-driver-loader:latest bpf 然后，运行 Falco docker pull falcosecurity/falco-no-driver:latest docker run --rm -i -t \\ --cap-drop all \\ --cap-add sys_admin \\ --cap-add sys_resource \\ --cap-add sys_ptrace \\ -v /var/run/docker.sock:/host/var/run/docker.sock \\ -e FALCO_BPF_PROBE=\"\" \\ -v /root/.falco:/root/.falco \\ -v /etc:/host/etc \\ -v /proc:/host/proc:ro \\ falcosecurity/falco-no-driver:latest 同样，如果您的系统启用了 AppArmor LSM，您将需要添加--security-opt apparmor:unconfined到最后一个命令。 12.2.2 完全特权 要使用具有完全权限的 Docker 在容器中运行 Falco，请使用以下命令。 如果您想将 Falco 与内核模块驱动程序一起使用： docker pull falcosecurity/falco:latest docker run --rm -i -t \\ --privileged \\ -v /var/run/docker.sock:/host/var/run/docker.sock \\ -v /dev:/host/dev \\ -v /proc:/host/proc:ro \\ -v /boot:/host/boot:ro \\ -v /lib/modules:/host/lib/modules:ro \\ -v /usr:/host/usr:ro \\ -v /etc:/host/etc:ro \\ falcosecurity/falco:latest 或者，您可以使用 eBPF 探针驱动程序： docker pull falcosecurity/falco:latest docker run --rm -i -t \\ --privileged \\ -e FALCO_BPF_PROBE=\"\" \\ -v /var/run/docker.sock:/host/var/run/docker.sock \\ -v /proc:/host/proc:ro \\ -v /boot:/host/boot:ro \\ -v /lib/modules:/host/lib/modules:ro \\ -v /usr:/host/usr:ro \\ -v /etc:/host/etc:ro \\ falcosecurity/falco:latest 也可以在完全特权模式下使用falco-no-driver镜像。falco-driver-loader这在由于空间、资源、安全或策略限制而不允许完整 Falco 映像的环境中可能是可取的。您可以将 eBPF 探针或内核模块加载到其自己的临时容器中，如下所示： docker pull falcosecurity/falco-driver-loader:latest docker run --rm -i -t \\ --privileged \\ -v /var/run/docker.sock:/host/var/run/docker.sock \\ -v /dev:/host/dev \\ -v /proc:/host/proc:ro \\ -v /boot:/host/boot:ro \\ -v /lib/modules:/host/lib/modules:ro \\ -v /usr:/host/usr:ro \\ -v /etc:/host/etc:ro \\ falcosecurity/falco-driver-loader:latest 完成此操作后，或者如果您通过falco-driver-loader脚本而不是 Docker 映像在主机上永久安装了驱动程序，那么您可以简单地falco-no-driver以特权模式加载映像： docker pull falcosecurity/falco-no-driver:latest docker run --rm -i -t \\ --privileged \\ -v /var/run/docker.sock:/host/var/run/docker.sock \\ -v /dev:/host/dev \\ -v /proc:/host/proc:ro \\ falcosecurity/falco-no-driver:latest 要使用eBPF 探针falco-no-driver，falco-driver-loader您必须删除-v /dev:/host/dev（仅内核模块需要）并添加： -e FALCO_BPF_PROBE=\"\" -v /root/.falco:/root/.falco \\ 其他可配置选项： DRIVER_REPO- 请参阅安装驱动程序部分。 SKIP_DRIVER_LOADER-设置此环境变量以避免镜像启动falco-driver-loader时运行。falcosecurity/falco当驱动程序已经通过其他方式安装在主机上时很有用 12.3 Hot Reload 这将重新加载 Falco 配置并重新启动引擎，而不会终止 pid。这对于在不杀死守护进程的情况下传播新的配置更改很有用。 kill -1 $(cat /var/run/falco.pid) 参考： falco getting started sysdig falco Falco case studies Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:13:06 "},"安全/Falco/kubernetes-Security-falco-default-macros.html":{"url":"安全/Falco/kubernetes-Security-falco-default-macros.html","title":"default macros","keywords":"","body":"falco default macros为写入而打开的文件打开文件以供阅读从不真实永远真实进程名称已设置文件系统对象重命名已创建新目录文件系统对象已删除文件系统对象已修改新进程产生二进制文件的公共目录Shell 已启动已知敏感文件新创建的进程出站网络连接入站网络连接对象是一个容器交互过程产生通用 SSH 端口允许的 SSH 主机用户列入白名单的容器允许生成shell的容器允许与 EC2 元数据服务通信的容器Kubernetes API 服务器允许与 Kubernetes API 通信的容器允许与 Kubernetes 服务节点端口通信的容器falco default macros tagsstart falco 安全 tagsstop Falco 规则集定义了许多宏，可以更轻松地开始编写规则。这些宏为许多常见场景提供了快捷方式，并且可以在任何用户定义的规则集中使用。Falco 还提供了应该由用户覆盖的宏，以提供特定于用户环境的设置。提供的宏也可以附加到本地规则文件中。 为写入而打开的文件 - macro: open_write condition: (evt.type=open or evt.type=openat) and evt.is_open_write=true and fd.typechar='f' and fd.num>=0 打开文件以供阅读 - macro: open_read condition: (evt.type=open or evt.type=openat) and evt.is_open_read=true and fd.typechar='f' and fd.num>=0 从不真实 - macro: never_true condition: (evt.num=0) 永远真实 - macro: always_true condition: (evt.num=>0) 进程名称已设置 - macro: proc_name_exists condition: (proc.name!=\"\") 文件系统对象重命名 - macro: proc_name_exists condition: (proc.name!=\"\") 已创建新目录 - macro: mkdir condition: evt.type = mkdir 文件系统对象已删除 - macro: remove condition: evt.type in (rmdir, unlink, unlinkat) 文件系统对象已修改 - macro: modify condition: rename or remove 新进程产生 - macro: spawned_process condition: evt.type = execve and evt.dir= 二进制文件的公共目录 - macro: bin_dir condition: fd.directory in (/bin, /sbin, /usr/bin, /usr/sbin) Shell 已启动 - macro: shell_procs condition: (proc.name in (shell_binaries)) 已知敏感文件 - macro: sensitive_files condition: > fd.name startswith /etc and (fd.name in (sensitive_file_names) or fd.directory in (/etc/sudoers.d, /etc/pam.d)) 新创建的进程 - macro: proc_is_new condition: proc.duration (((evt.type in (accept,listen) and evt.dir== 0 or evt.res = EINPROGRESS)) 出站网络连接 - macro: outbound condition: > (((evt.type = connect and evt.dir== 0 or evt.res = EINPROGRESS)) 入站网络连接 - macro: inbound_outbound condition: > (((evt.type in (accept,listen,connect) and evt.dir== 0 or evt.res = EINPROGRESS)) 对象是一个容器 - macro: container condition: container.id != host 交互过程产生 - macro: interactive condition: > ((proc.aname=sshd and proc.name != sshd) or proc.name=systemd-logind or proc.name=login) 通用 SSH 端口 覆盖此宏以反映环境中提供 SSH 服务的端口。 - macro: ssh_port condition: fd.sport=22 允许的 SSH 主机 覆盖此宏以反映可以连接到已知 SSH 端口（即堡垒或跳转框）的主机。 - macro: allowed_ssh_hosts condition: ssh_port 用户列入白名单的容器 允许在特权模式下运行的白名单容器。 - macro: user_trusted_containers condition: (container.image startswith sysdig/agent) 允许生成shell的容器 将允许生成 shell 的容器列入白名单，如果在 CI/CD 管道中使用容器，则可能需要这样做。 - macro: user_shell_container_exclusions condition: (never_true) 允许与 EC2 元数据服务通信的容器 将允许与 EC2 元数据服务通信的容器列入白名单。默认值：任何容器。 - macro: ec2_metadata_containers condition: container Kubernetes API 服务器 在此处设置 Kubernetes API 服务的 IP。 - macro: k8s_api_server condition: (fd.sip=\"1.2.3.4\" and fd.sport=8080) 允许与 Kubernetes API 通信的容器 将允许与 Kubernetes API 服务通信的容器列入白名单。需要设置 k8s_api_server。 - macro: k8s_containers condition: > (container.image startswith gcr.io/google_containers/hyperkube-amd64 or container.image startswith gcr.io/google_containers/kube2sky or container.image startswith sysdig/agent or container.image startswith sysdig/falco or container.image startswith sysdig/sysdig) 允许与 Kubernetes 服务节点端口通信的容器 - macro: nodeport_containers condition: container 参考： faloc Default Macros Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:13:32 "},"安全/Falco/kubernetes-Security-falco-rules.html":{"url":"安全/Falco/kubernetes-Security-falco-rules.html","title":"规则","keywords":"","body":"falco 规则1. 简介2. 版本控制2.1 Falco 引擎版本控制2.2 Falco rule文件版本控制3. 关键词4. 语法4.1 condition4.2 rule operators4.3 rule macro4.4 rule list4.5 rule output4.6 rule priorities4.7 rule tags4.8 fd.sip.name4.9 rule exceptions5. 转义特殊字符6. rule 文件6.1 默认 rule 文件6.2 本地 rule 文件7. rule 禁用默认7.1 通过现有的宏7.2 通过 Falco 参数7.3 通过自定义rule定义8. rule 陷阱falco 规则 tagsstart falco 安全 tagsstop 上篇文章我们对falco 进行简单的入门，其中涉及到、架构、特点、安装、部署、运行等，接下来我们了解 falco 的 rules。 1. 简介 Falcorule文件是包含三种元素的YAML文件： | 元素 | 描述 | |----|------------------------------------------------| | Rules| 应生成警报的条件。rule伴随着与警报一起发送的描述性输出字符串。 | | Macros | 可以在rule甚至其他宏中重复使用的rule条件片段。宏提供了一种命名常见模式和排除rule冗余的方法。 | | Lists | 可以包含在rule、宏或其他列表中的项目集合。与rule和宏不同，列表不能被解析为过滤表达式 | 还有两个与版本控制相关的可选元素： | 元素 | 描述 | |--------------------------|----------------------------| | required_engine_version | 用于跟踪rule内容和 falco引擎版本之间的兼容性。 | | required_plugin_versions | 用于跟踪rule内容和插件版本之间的兼容性。 | 2. 版本控制 有时，我们会更改不向后兼容旧版本 Falco 的rule文件格式。类似地，libsinsp 和 libscap 可以定义新的 filtercheck 字段、操作符等。我们想要表示一组给定的rule依赖于这些库中的字段/操作符。从 Falco 版本0.14.0 开始，Falco rule支持 Falco 引擎和 Falco rule文件的显式版本控制。 2.1 Falco 引擎版本控制 falco可执行文件和falco_engineC++ 对象现在支持返回版本号。初始版本是 2（暗示之前的版本是 1）。每当我们对rule文件格式进行不兼容的更改或向 Falco 添加新的过滤检查字段/运算符时，我们都会增加此版本。 2.2 Falco rule文件版本控制 Falco 包含的 Falco rule文件包括一个新的顶级对象 ，required_engine_version: N它指定读取此rule文件所需的最低引擎版本。如果不包括在内，则在读取rule文件时不执行版本检查。这是一个例子： # This rules file requires a falco with falco engine version 7. - required_engine_version: 7 如果rule文件的engine_version版本高于 Falco 引擎版本，则会加载rule文件并返回错误。 3. 关键词 Key Required Description Default rule yes rule的简短、唯一名称。 condition yes 应用于事件以检查它们是否与rule匹配的过滤表达式。 desc yes 对rule检测到的内容的更长描述。 output yes 指定发生匹配事件时应输出的消息 priority yes 事件严重性的不区分大小写的表示。应该是以下之一：emergency, alert, critical, error, warning, notice, informational, debug.. exceptions no 一组导致rule不生成警报的异常。 enabled no 如果设置为false，则既不加载rule，也不匹配任何事件。. true tags no 应用于rule的标签列表 warn_evttypes no 如果设置为false，Falco 会抑制与没有事件类型的rule相关的警告 true skip-if-unknown-filter no 如果设置为true，如果rule条件包含过滤检查，例如fd.some_new_field，此版本的 Falco 不知道，Falco 会静默接受该rule但不执行它；如果设置为false，Falco 报告错误并在发现未知过滤检查时存在。 false source no 评估此rule的事件源。典型值为syscall,k8s_audit或源插件公布的源。 4. 语法 4.1 condition rule的关键部分是条件字段。条件是使用条件语法表示的布尔谓词。可以使用它们各自支持的字段来表达所有支持事件的条件。 这是一个在容器内运行 bash shell 时发出警报的条件示例： container.id != host and proc.name = bash 第一个子句检查事件是否发生在容器中（其中container.id等于\"host\"事件是否发生在常规主机上）。第二个子句检查进程名称是否为bash. 由于此条件不包括带有系统调用的子句，它只会检查事件元数据。因此，如果 bash shell 确实在容器中启动，Falco 会为该 shell 执行的每个系统调用输出事件。 如果您只想在容器中每次成功生成 shell 时收到警报，请在条件中添加适当的事件类型和方向： evt.type = execve and evt.dir= 因此，使用上述条件的完整rule可能是： - rule: shell_in_container desc: notice shell activity within a container condition: evt.type = execve and evt.dir= 通常都以evt.type表达式或宏开头，安全rule通常一次只考虑一种系统调用类型。例如，您可能希望在打开文件时考虑open或openat捕获可疑活动、execve检查生成的进程等等。您不必猜测系统调用名称，因为您可以查看支持的系统调用事件的完整列表并了解您可以使用哪些事件。 每个系统调用都有一个进入事件和一个退出事件，它们显示在evt.dir字段中，也称为系统调用的“方向”。值>表示入口，在调用系统调用时触发，而标记退出，表示调用已返回。事实上，通过查看支持的系统调用列表，您可以看到我们的事件有两个条目。例如： > setuid(UID uid) 大多数情况下，引擎会通知您退出事件，因为您想了解事件执行完成后发生的情况。您可以通过使用与打开文件关联的事件来查看。 > open() openat() 每个事件都有一个与之关联的参数列表，您可以使用evt.arg.. 例如，要识别进程何时打开文件以覆盖它，您需要检查标志列表是否包含O_TRUNC. 您可以使用上面显示的和退出事件的evt.arg.flags字段，然后rule将如下所示：open、openat evt.type in (open, openat) and evt.dir = 请注意，参数不一定与 Linux 内核中使用的原始参数匹配，而是由 Falco 解析以更容易编写rule。通过使用这些evt字段。 虽然evt字段允许您编写非常有表现力的条件，但参数和公共字段通常不足以编写完整的安全rule。很多时候，您希望根据事件发生的流程上下文添加条件，或者容器内是否正在发生某些事情，甚至将每个事件与集群​​、pod 等的相关 Kubernetes 元数据相关联。出于这个原因，Falco 用其他字段类丰富了许多事件。 4.2 rule operators =,!= 等式和不等式运算符。 =,> 数值的比较运算符。 contains,icontains 如果一个字符串包含另一个字符串，则对于字符串将评估为真，并且icontains是不区分大小写的版本。对于标志，如果设置了标志，它将评估为真。例子：proc.cmdline contains \"-jar\", evt.arg.flags contains O_TRUNC. startswith,endswith 检查字符串的前缀或后缀。 glob 评估标准 glob 模式。示例：fd.name glob \"/home//.ssh/\"。 in,intersects 设置操作。 pmatch 将文件路径与一组文件或目录前缀进行比较。示例：fd.name pmatch (/tmp/hello)将针对 评估为真/tmp/hello，/tmp/hello/world但不是/tmp/hello_world。 exists 检查是否设置了字段。示例：k8s.pod.name exists。 bcontains,bstartswith 这些运算符的工作方式与原始字节字符串类似，contains并且startswith允许对原始字节字符串执行字节匹配，接受十六进制字符串作为输入。例子：evt.buffer bcontains CAFEBABE, evt.buffer bstartswith 012AB3CC. 4.3 rule macro 如上所述，宏提供了一种以可重用方式定义rule的公共子部分的方法。通过查看上面的条件，它看起来像两者evt.type = execve and evt.dir=，并且container.id != host会被其他rule使用很多，所以为了使我们的工作更容易，我们可以轻松地为两者定义宏： - macro: container condition: container.id != host - macro: spawned_process condition: evt.type = execve and evt.dir= 定义了这个宏后，我们可以将上述rule的条件重写为spawned_process and container and proc.name = bash falco 默认的 macro集合在这里！ 4.4 rule list 列表是可以包含在rule、宏甚至其他列表中的项目的命名集合。请注意，列表不能被解析为过滤表达式。每个列表节点都有以下键： 钥匙 描述 list 列表的唯一名称（作为 slug） items 值列表 以下是一些示例列表以及使用它们的宏： - list: shell_binaries items: [bash, csh, ksh, sh, tcsh, zsh, dash] - list: userexec_binaries items: [sudo, su] - list: known_binaries items: [shell_binaries, userexec_binaries] - macro: safe_procs condition: proc.name in (known_binaries) 如果您使用多个 Falco rule文件，您可能希望将新项目附加到现有列表、rule或宏中。为此，请定义一个与现有项目同名的项目并将一个append: true属性添加到列表中。追加列表时，项目被添加到列表的末尾。附加rule/宏时，附加文本附加到条件：rule/宏的字段。 请注意，在附加到列表、rule或宏时，rule配置文件的顺序很重要！例如，如果您附加到现有的默认rule（例如Terminal shell in container），您必须确保您的自定义配置文件（/etc/falco/rules.d/custom-rules.yaml）在默认配置文件（/etc/falco/falco_rules.yaml ）之后加载。这可以使用多个参数以正确的顺序配置，直接在 falco 配置文件 (falco.yaml ) 中通过，或者您使用官方 Helm charts。 4.4.1 附加到列表 /etc/falco/falco_rules.yaml - list: my_programs items: [ls, cat, pwd] - rule: my_programs_opened_file desc: track whenever a set of programs opens a file condition: proc.name in (my_programs) and (evt.type=open or evt.type=openat) output: a tracked program opened a file (user=%user.name command=%proc.cmdline file=%fd.name) priority: INFO /etc/falco/falco_rules.local.yaml - list: my_programs append: true items: [cp] 每当打开文件时，该rulemy_programs_opened_file都会触发。ls、cat、pwd、cp 4.4.2 附加到宏 /etc/falco/falco_rules.yaml - macro: access_file condition: evt.type=open - rule: program_accesses_file desc: track whenever a set of programs opens a file condition: proc.name in (cat, ls) and (access_file) output: a tracked program opened a file (user=%user.name command=%proc.cmdline file=%fd.name) priority: INFO /etc/falco/falco_rules.local.yaml - macro: access_file append: true condition: or evt.type=openat 该rule将在/使用/文件program_accesses_file时触发ls、cat、open或者openat 4.4.3 附加到rule /etc/falco/falco_rules.yaml - rule: program_accesses_file desc: track whenever a set of programs opens a file condition: proc.name in (cat, ls) and evt.type=open output: a tracked program opened a file (user=%user.name command=%proc.cmdline file=%fd.name) priority: INFO /etc/falco/falco_rules.local.yaml - rule: program_accesses_file append: true condition: and not user.name=root 该rule将在/用于文件program_accesses_file时触发，但如果用户是 root 则不会触发ls、cat、open 4.5 rule output rule输出是一个字符串，它可以使用条件可以使用的相同字段%来执行插值，类似于printf. 例如： Disallowed SSH Connection (command=%proc.cmdline connection=%fd.name user=%user.name user_loginuid=%user.loginuid container_id=%container.id image=%container.image.repository) 可以输出： Disallowed SSH Connection (command=sshd connection=127.0.0.1:34705->10.0.0.120:22 user=root user_loginuid=-1 container_id=host image=) 请注意，不必在特定事件中设置所有字段。正如您在上面的示例中看到的，如果连接发生在容器外部，则%container.image.repository不会设置该字段，而是显示该字段。 4.6 rule priorities 每条 Falco rule都有一个优先级，表明违反rule的严重程度。优先级包含在消息/JSON 输出/等中。以下是可用的优先级： EMERGENCY ALERT CRITICAL ERROR WARNING NOTICE INFORMATIONAL DEBUG 用于为rule分配优先级的一般准则如下： 如果rule与写入状态（即文件系统等）有关，则其优先级为ERROR. 如果一条rule与未经授权的状态读取有关（即读取敏感文件等），则其优先级为WARNING. 如果rule与意外行为（在容器中生成意外 shell、打开意外网络连接等）相关，则其优先级为NOTICE. 如果rule与违反良好实践的行为（意外的特权容器、具有敏感挂载的容器、以 root 身份运行交互式命令）有关， 则其优先级为INFO. 4.7 rule tags 从 0.6.0 开始，rule具有一组可选标签，用于将rule集分类为相关rule组。这是一个例子： - rule: File Open by Privileged Container desc: Any open by a privileged container. Exceptions are made for known trusted images. condition: (open_read or open_write) and container and container.privileged=true and not trusted_containers output: File opened for read/write by privileged container (user=%user.name command=%proc.cmdline %container.info file=%fd.name) priority: WARNING tags: [container, cis] 标签的使用方法： 您可以使用该-T 参数禁用具有给定标签的rule。-T可以指定多次。例如，要跳过所有带有“filesystem”和“cis”标签的rule，你可以使用falco -T filesystem -T cis ...., -T不能用 指定-t。 您可以使用该-t 参数仅运行具有给定标记的rule。-t可以指定多次。例如，要仅使用“filesystem”和“cis”标签运行这些rule，您可以使用falco -t filesystem -t cis .... ,-t不能用-Tor指定-D （通过rule名称正则表达式禁用rule） 当前 Falco rule集的标签 我们还检查了默认rule集，并用一组初始标签标记了所有rule。以下是我们使用的标签： | 标签 | 描述 | |---------------|-------------------------------| | filesystem | 该rule与读/写文件有关 | | software_mgmt | 该rule涉及任何软件/包管理工具，如 rpm、dpkg 等。 | | process | 该rule与启动新进程或更改当前进程的状态有关 | | database | 该rule与数据库有关 | | host | 该rule仅适用于容器之外 | | shell | 该rule特别涉及启动 shell | | container | 该rule仅适用于容器内 | | cis | 该rule与 CIS Docker 基准相关 | | users | 该rule涉及用户管理或更改正在运行的进程的身份 | | network | 该rule与网络活动有关 | 4.8 fd.sip.name 域的实际查找是在单独的线程上完成的，以避免停止主系统调用事件循环。此外，域的 IP 集会定期刷新，策略如下： 域名的基本刷新时间为 10 秒。 如果在刷新周期后 IP 地址未更改，则该域名的刷新超时时间会加倍，直到 320 秒（约 5 分钟） - list: https_miner_domains items: [ \"ca.minexmr.com\", \"cn.stratum.slushpool.com\", \"de.minexmr.com\", \"fr.minexmr.com\", \"mine.moneropool.com\", \"mine.xmrpool.net\", \"pool.minexmr.com\", \"sg.minexmr.com\", \"stratum-eth.antpool.com\", \"stratum-ltc.antpool.com\", \"stratum-zec.antpool.com\", \"stratum.antpool.com\", \"xmr.crypto-pool.fr\" ] # Add rule based on crypto mining IOCs - macro: minerpool_https condition: (fd.sport=\"443\" and fd.sip.name in (https_miner_domains)) - rule: Connect to Yahoo desc: Detect Connects to yahoo.com IPs condition: evt.type=connect and fd.sip.name=yahoo.com output: Some connect to yahoo (command=%proc.cmdline connection=%fd.name IP=%fd.sip.name) priority: INFO 相反，此rule将永远不会显示有意义的输出...IP=%fd.sip.name，因为比较使用否定匹配： - rule: Connect to Anything but Yahoo desc: Detect Connects to anything other than yahoo.com IPs condition: evt.type=connect and fd.sip.name!=yahoo.com output: Some connect to something other than yahoo (command=%proc.cmdline connection=%fd.name IP=%fd.sip.name) priority: INFO 4.9 rule exceptions - rule: Write below binary dir desc: an attempt to write to any file below a set of binary directories condition: > bin_dir and evt.dir = 以前，这些例外被表示为原始rule条件的串联。例如，查看宏 package_mgmt_procs： - macro: package_mgmt_procs condition: proc.name in (package_mgmt_binaries) 结果附加and not proc.name in (package_mgmt_binaries)到rule的条件。 一个更极端的情况是 Write_below_etc 宏被 Write below etc rule使用。它有几十个例外： ... and not sed_temporary_file and not exe_running_docker_save and not ansible_running_python and not python_running_denyhosts and not fluentd_writing_conf_files and not user_known_write_etc_conditions and not run_by_centrify and not run_by_adclient and not qualys_writing_conf_files and not git_writing_nssdb ... 这些例外通常都遵循相同的结构——命名一个程序和 /etc 下允许该程序写入文件的目录前缀 从 0.28.0 开始，falco 支持exceptionsrule的可选属性。例外键是标识符列表加上过滤检查字段的元组列表。这是一个例子： - rule: Write below binary dir desc: an attempt to write to any file below a set of binary directories condition: > bin_dir and evt.dir = 该rule定义了四种例外情况： proc_writer：使用 proc.name 和 fd.directory 的组合 cmdline_writer：使用 proc.cmeline 和 fd.directory 的组合 container_writer：使用 container.image.repository 和 fd.directory 的组合 proc_filenames：使用进程和文件名列表的组合。 文件名：使用文件名列表 该fields属性包含一个或多个将从事件中提取值的字段。该comps属性包含将 1-1 与 fields 属性中的项目对齐的比较运算符。该values属性包含值的元组。元组中的每个项目都应与相应的字段和比较运算符 1-1 对齐。每个值元组都与字段/组合组合在一起，以修改条件以将排除项添加到rule的条件中。 例如，对于上面的异常“proc_writer”，字段/comps/values 相当于将以下内容添加到rule的条件中： ... and not ((proc.name=my-custom-yum and fd.directory=/usr/bin) or (proc.name=my-custom-apt and fd.directory=/usr/local/bin)) 请注意，当比较运算符为“in”时，对应的值元组项应该是一个列表。上面的“proc_filenames”使用该语法，相当于： ... and not (proc.name=my-custom-dpkg and fd.name in (/usr/bin, /bin)) 4.9.1 Appending Exception Values 异常值通常在rule中使用 append: true 定义。这是一个例子： - list: apt_files items: [/bin/ls, /bin/rm] - rule: Write below binary dir exceptions: - name: proc_writer values: - [apk, /usr/lib/alpine] - [npm, /usr/node/bin] - name: container_writer values: - [docker.io/alpine, /usr/libexec/alpine] - name: proc_filenames values: - [apt, [apt_files]] - [rpm, [/bin/cp, /bin/pwd]] - name: filenames values: [python, go] append: true 在这种情况下，这些值将附加到基本rule的任何值，然后将字段/comps/values 添加到rule的条件中。 综上所述，该rule的有效rule条件为： ... and not ((proc.name=my-custom-yum and fd.directory=/usr/bin) or # proc_writer (proc.name=my-custom-apt and fd.directory=/usr/local/bin) or (proc.name=apk and fd.directory=/usr/lib/alpine) or (proc.name=npm and fd.directory=/usr/node/bin) or (container.image.repository=docker.io/alpine and fd.name=/usr/libexec/alpine) or # container_writer (proc.name=apt and fd.name in (apt_files)) or # proc_filenames (proc.name=rpm and fd.name in (/bin/cp, /bin/pwd)) or (fd.filename in (python, go)) # filenames 定义异常时，请尝试考虑actor、action和target，并尽可能将所有三个项目用于异常。例如，除了简单地使用proc.name or container.image.repository用于基于文件的异常，还包括通过fd.name、fd.directory等操作的文件。类似地，如果rule是特定于容器的，则不仅包括 image container.image.repository，还包括进程名称proc.name 4.9.2 Exceptions Syntax Shortcuts rule可以定义字段和组合，但不能定义值。这允许稍后使用“append：true”的rule将值添加到异常中（更多内容见下文）。上 面的异常“cmdline_writer”具有以下格式： - name: cmdline_writer fields: [proc.cmdline, fd.directory] comps: [startswith, =] 定义异常的另一种方法是让字段包含单个字段，而 comps 包含单个比较运算符（必须是“in”、“pmatch”、“intersects”之一）。在这种格式中，values 是一个值列表，而不是元组列表。上面的异常“文件名”具有以下格式： - name: filenames fields: fd.filename comps: in 在这种情况下，异常相当于： ... and not (fd.filename in (...)) 如果未提供 comps，则填写默认值。当 fields 为列表时，comps 将设置为 = 的等长列表。上面的异常“container_writer”具有该格式，相当于： - name: container_writer fields: [container.image.repository, fd.directory] comps: [=, =] 当 fields 是单个字段时，comps 设置为单个字段“in”。 5. 转义特殊字符 在某些情况下，rule可能需要包含特殊字符，如(、空格等。例如，您可能需要查找 a proc.name of (systemd)，包括周围的括号。 您可以使用\"这些特殊字符来捕获。这是一个例子： - rule: Any Open Activity by Systemd desc: Detects all open events by systemd. condition: evt.type=open and proc.name=\"(systemd)\" or proc.name=systemd output: \"File opened by systemd (user=%user.name command=%proc.cmdline file=%fd.name)\" priority: WARNING 在列表中包含项目时，请确保不会通过用单引号将带引号的字符串从 YAML 文件中解释双引号。这是一个例子： - list: systemd_procs items: [systemd, '\"(systemd)\"'] - rule: Any Open Activity by Systemd desc: Detects all open events by systemd. condition: evt.type=open and proc.name in (systemd_procs) output: \"File opened by systemd (user=%user.name command=%proc.cmdline file=%fd.name)\" priority: WARNING 6. rule 文件 6.1 默认 rule 文件 默认的 falco rule文件安装在/etc/falco/falco_rules.yaml. 它包含一组预定义的rule，旨在在各种情况下提供良好的覆盖。目的是不修改此rule文件，并用每个新的软件版本替换。 6.2 本地 rule 文件 本地 falco rule文件安装在/etc/falco/falco_rules.local.yaml. 除了一些评论之外，它是空的。目的是将主rule文件的添加/覆盖/修改添加到此本地文件中。它不会被每个新的软件版本所取代。 7. rule 禁用默认 7.1 通过现有的宏 大多数默认rule都提供了某种consider_*宏，这些宏已经是rule条件的一部分。这些consider_*宏通常设置为(never_true)或(always_true)基本上启用或禁用相关rule。现在，如果您想启用默认禁用的rule（例如Unexpected outbound connection destination），您只需在自定义 Falco 配置中覆盖rule的consider_*宏（consider_all_outbound_conns在这种情况下）。 您的自定义 Falco 配置示例（注意(always_true)条件）： - macro: consider_all_outbound_conns condition: (always_true) 请再次注意，指定配置文件的顺序很重要！最后定义的同名宏有效。 7.2 通过 Falco 参数 Falco 提供以下参数来限制应该启用/使用哪些默认rule，哪些不应该： -D Disable any rules with names having the substring . Can be specified multiple times. -T Disable any rules with a tag=. Can be specified multiple times. Can not be specified with -t. -t Only run those rules with a tag=. Can be specified multiple times. Can not be specified with -T/-D. extraArgs如果您通过官方 Helm charts 部署 Falco，这些参数也可以指定为 Helm 图表值 ( ). 7.3 通过自定义rule定义 enabled: false最后但并非最不重要的一点是，您可以使用rule 属性禁用默认启用的rule。这对于在默认条件下不提供consider_*宏的rule特别有用。 确保在默认配置文件之后加载自定义配置文件。您可以使用多个-r参数配置正确的顺序，直接在 falco 配置文件中falco.yaml通过rules_file. 如果您使用的是官方 Helm charts，则使用该falco.rules File值配置顺序。 例如，在定义自定义rule时禁用User mgmt binaries默认rule： /etc/falco/falco_rules.yaml /etc/falco/rules.d/custom-rules.yaml - rule: User mgmt binaries enabled: false 同时，可以使用enabled: true rule 属性重新启用禁用的rule。例如，默认情况下禁用的Change thread namespacerule/etc/falco/falco_rules.yaml可以通过以下方式手动启用： - rule: Change thread namespace enabled: true 8. rule 陷阱 带有rule/宏附加和逻辑运算符的陷阱 请记住，在附加rule和宏时，第二个rule/宏的文本只是简单地添加到第一个rule/宏的条件中。如果原始rule/宏具有潜在的模棱两可的逻辑运算符，这可能会导致意外结果。这是一个例子： - rule: my_rule desc: ... condition: evt.type=open and proc.name=apache output: ... - rule: my_rule append: true condition: or proc.name=nginx 应该proc.name=nginx解释为相对于and proc.name=apache，即允许 apache/nginx 打开文件，或者相对于evt.type=open，即允许 apache 打开文件或允许 nginx 做任何事情？ 在这种情况下，请务必尽可能用括号限定原始条件的逻辑运算符范围，或者在不可能时避免附加条件。 参考： falco rules sysdig falco Falco case studies Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:14:29 "},"安全/Apparmor/":{"url":"安全/Apparmor/","title":"Apparmor","keywords":"","body":"Apparmor Overview1. 简介2. 版本3. 包含 AppArmor 的发行版4. 源代码4.1 Kernel4.2 Userspace5. AppArmor 配置文件6. 命令7. 教程Apparmor Overview tagsstart apparmor 安全 tagsstop 1. 简介 AppArmor 是类似于 Linux 的安全系统的强制访问控制 (MAC)。AppArmor 将单个程序限制在一组文件、功能、网络访问和限制中，统称为程序的 AppArmor 策略，或简称为配置文件。无需重新启动即可将新的或修改的策略应用于正在运行的系统。AppArmor 旨在通过以管理员友好的语言呈现其配置文件，使其易于理解和用于大多数常见需求。 AppArmor 的限制是有选择性的，因此系统上的某些程序可能会受到限制，而其他程序则不会。选择性限制允许管理员灵活地关闭有问题的配置文件以进行故障排除，同时限制系统的其他部分。 不受限制的程序在标准 Linux 自由访问控制 (DAC) 安全性下运行。AppArmor 增强了传统 DAC，因为首先在传统 DAC 下评估受限程序，如果 DAC 允许该行为，则咨询 AppArmor 策略。 AppArmor 支持按配置文件学习（投诉）模式，以帮助用户编写和维护策略。学习模式允许通过正常运行程序并学习其行为来创建配置文件。在 AppArmor 充分了解行为之后，配置文件可能会转为强制模式。虽然生成的配置文件可能比为特定环境和应用程序量身定制的手工配置文件更宽松，但学习模式可以大大减少使用 AppArmor 所需的工作量和知识，并增加重要的安全层。 2. 版本 AppArmor 有两个主要版本，即 2.x 系列（当前）和 3.x 系列（开发）。2.x 系列在其生命周期中看到了渐进式的改进，只是在语义兼容性方面出现了渐进式的中断。3.x 系列是 AppArmor 的重大扩展。两个主要系列都使用相同的基本策略语言，只有细微的语义差异。3.x 系列允许更多扩展的策略和细粒度的控制。 3. 包含 AppArmor 的发行版 Annvix Arch Linux, documentation and Arch specific notes CentOs, documentation and CentOS specific notes Debian, documentation and Debian specific notes Gentoo openSUSE (integrated in default install), documentation and Suse specific notes Pardus Linux PLD Ubuntu (integrated in default install), documentation and Ubuntu specific notes 这些发行版的任何衍生产品也应该有 AppArmor 可用。更新的 RPMS可以在openSUSE 构建服务中找到。这些不限于 SUSE 发行版。 4. 源代码 AppArmor 项目源代码分为内核模块（在 Linux 内核和 git 开发树中可用）和启动板中可用的用户空间工具。 4.1 Kernel AppArmor 在 2.6.36 的上游内核中。内核模块 git 树中提供了早期版本： 如何获取 AppArmor 内核源 注意：master 分支不稳定，会时不时地rebase。发布分支将是稳定的，不会被重新定位。 AppArmor v2.4 兼容性补丁在稳定的内核分支中可用。例如 v3.4-aa2.8 或 kernel-patches 目录中的发行版 tarball。 4.2 Userspace 当前稳定版本：3.0.6 支持的版本：2.13.6 支持的版本：2.12.3 支持的版本：2.11.3 生命周期结束版本：2.10.6 用户空间工具 如何获取 AppArmor 用户空间工具 5. AppArmor 配置文件 开发 AppArmor 配置文件位于 Bazaar 存储库中。安装发行版的 Bazaar 软件包后，可以使用以下命令下载它们： git clone git://git.launchpad.net/apparmor-profiles 找到与您的发行版和版本匹配的子目录，并在里面查找当前正在开发的各种配置文件。您可以通过将配置文件复制到 /etc/apparmor.d 并重新启动 AppArmor 来使用配置文件： $ /etc/init.d/apparmor restart restart apparmor # if upstart is being used, with initscripts that have been fully updated to support upstart 如何创建或修改 AppArmor 配置文件 使用工具创建和修改 AppArmor 策略 手动创建和修改 AppArmor 策略 AppArmor 文档 6. 命令 AppArmor 手册 Ubuntu Profile enforcement: apparmor_parser：将 AppArmor 配置文件加载到内核中 aa-audit：将 AppArmor 安全配置文件设置为审核模式 aa-enforce：设置 AppArmor 安全配置文件以强制模式被禁用或抱怨模式。 aa-complain：将 AppArmor 安全配置文件设置为抱怨模式 Ubuntu Monitoring tools: aa-status：显示有关当前 AppArmor 策略的各种信息 aa-notify：显示有关记录的 AppArmor 消息的信息 aa-unconfined：输出带有 tcp 或 udp 端口​​但没有 AppArmor 的进程列表已加载配置文件 Ubuntu Profile development: aa-autodep：猜测基本的 AppArmor 配置文件要求 aa-logprof：用于更新 AppArmor 安全配置文件的实用程序 aa-genprof：AppArmor 的配置文件生成实用程序 mod_apparmor：Apache 的细粒度 AppArmor 限制 aa_change_hat aa_change_profile：更改任务配置文件 PAM plugin：可用于根据在用户或任务级别完成的身份验证附加配置文件 7. 教程 使用工具创建和修改 AppArmor 策略 手动创建和修改 AppArmor 策略 将 mod_apparmor 与 Apache 一起使用来限制 Web 应用程序- DRAFT 使用 AppAmor 和 libvirt 来限制虚拟机 将 AppArmor 与 PAM 集成以实现基于登录的策略 使用 AppArmor 进行基于角色的访问控制 (RBAC) - 草稿 使用 AppArmor 实现多级安全 (MLS) - DRAFT 使用 AppArmor 限制和控制通过 wine 运行的 Windows 应用程序- DRAFT 使用完整的系统策略- DRAFT 如何在 systemd 中使用 AppArmor - DRAFT 参考： AppArmor AppArmor Documentation Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-08 07:28:53 "},"安全/Apparmor/kubernetes-Apparmor-rules.html":{"url":"安全/Apparmor/kubernetes-Apparmor-rules.html","title":"语法","keywords":"","body":"1. 简介2. 注释3. Include Rules包含与评论规则冲突并优先。#和include不能与空格分开，否则将被视为注释4. Child profiles5. Hats6. Capability Rules7. Network Rules8. File rules9. 文件通配9.1 文件通配示例9.2 文件权限9.3 执行权限9.4 AppArmor 文件标签10. Rule 修饰符kubernetes apparmor 语法 上一篇：kubernetes apparmor 入门 1. 简介 tagsstart apparmor 安全 tagsstop AppArmor 策略是使用管理员友好的配置文件语言创建的，然后将其编译为二进制策略以加载到内核中。AppArmor 策略存储在 /etc/apparmor.d/中的一组文件中。AppArmor 策略分为配置文件，这些配置文件通常旨在限制特定的应用程序。配置文件将声明访问规则以允许访问资源，并且当配置文件中没有匹配规则时，通过日志记录隐式拒绝访问。 配置文件以配置文件名称开头，后跟可选标志字段，然后是开头{后跟配置文件规则，并以结束}结束如果配置文件名称不以/开头，则应在前面加上关键字配置文件. 例如： /usr/bin/firefox { # profile contents } /usr/bin/firefox flags=(complain) { # profile contents } profile /usr/bin/ { # profile contents } profile user1 { # profile contents } 配置文件名称可以包含文件规则通配符，以允许它们应用于多个可执行文件。 2. 注释 # Comment 1 # Comment 2 profile example { # comment 3 # comment 4 /home/foo rw, # comment at the end of a file rule } 3. Include Rules #include #include “a/relative/path/file” include 包含与评论规则冲突并优先。#和include不能与空格分开，否则将被视为注释 # include is a comment #include 4. Child profiles 配置文件可以包含子配置文件。子配置文件可用于以特殊方式限制应用程序，或者当您希望子在系统上不受限制，但在从父调用时被限制。例如： /parent/profile { /path/to/child1 cx -> child1, /path/to/child2 cx, /path/to/* cx, # for * matching child3 will transition to child3, # child4, child5, ... will transition to /path/to/child* # if matching child profile does not exist will fail /another/path/to/* cx -> child1, # send all matching execs to child1 profile child1 { } profile /path/to/child2 { } profile /path/to/child3 { } # generic fall back profile profile /path/to/child* { } } 5. Hats Hats 是一个特殊的子配置文件，可以与 change_hat API 调用一起使用。要表示帽子，请在帽子名称前加上^，不要有空格。例如： /parent/profile { ^hat { } } 6. Capability Rules AppArmor 支持粗粒度访问 Linux 的 POSIX 风格的功能（请参阅“man 功能”），并且功能规则用于允许访问这些功能。例如，删除权限的 setuid 应用程序可能需要. /profile { capability setuid, capability setgid, } 7. Network Rules AppArmor 目前支持通过网络规则对网络进行粗粒度访问。例如，网络守护程序可能需要： /profile { network inet dgram, network inet stream, } 或者数据包分析器可能需要： /profile { network raw, network packet, } 8. File rules 文件规则控制文件的访问方式，并且只出现在配置文件中。它们由路径名、权限集组成，并以逗号结尾。它们可以先写权限，也可以先写路径名，尽管约定是先列出路径。有效的路径名总是以/开头。例如： /profile { /path/to/file rw, # file rule beginning with a pathname (convention) rw /path/to/file2, # file rule beginning with permissions /path/to/file3 # file rule split over multiple lines rw, } 文件规则可以包含允许匹配多个文件的特殊通配符 9. 文件通配 AppArmor 使用类似于 bash shell 使用的文件通配语法。通配符不是标准的完整正则表达式语法，而是使用一些称为通配符的字符。AppArmor 通配符的语义与 bash 的语义略有不同。 * - 在目录级别匹配零个或多个字符。当将路径视为字符串时，它将匹配除/之外的每个字符 这将匹配点文件（文件名以.开头），特殊点文件除外。和..，如果它紧跟在目录 / 例如之后。/目录/* 这将不匹配空目录字符串，例如。/目录// pcre 等价于 (/\\000*) ** - 在多个目录级别上匹配 0 个或多个字符。 这将匹配点文件（文件名以.开头），特殊点文件除外。和..，如果它紧跟在目录 / 例如之后。/目录/** pcre 等价于 (\\000*) ? - 匹配不是/的单个字符 pcre 等价于 / {} - 替代 - 可以匹配的替代字符串的逗号分隔列表。允许使用空字符串，这意味着空字符串是可行的替代方案 pcre 等价于 (|) [] - 字符类 与 pcre 语法相同 [^] - 反转字符类 与 pcre 语法相同 交替嵌套表达式（从 AppArmor 2.3 开始）： 转义字符 \\* 将字符表示为# \\001 以下是对当前文件通配的建议添加，当前未实施： {*^} - 一个类似于 的 glob，具有不允许匹配的事物的交替样式列表。例如。`/etc/{^shadow}与允许/etc/匹配的所有内容相同，除了/etc/shadow例如。/etc/{^shadow,passwd}与/etc/-/etc/{shadow,passwd}相同，例如。/etc/{^shadow}与/etc/相同 -/etc/shadow 请注意，不允许使用空的替代条目，即。{*^shadow,}` {^} - 类似于 的 glob 不允许匹配具有交替样式列表的事物，例如。/etc/{**^shadow} 与 /etc/ 匹配 - /etc/shadow 例如。`/etc/{^shadow,passwd}与/etc/ - /etc/{shadow,passwd}相同，例如。/etc/{^shadow}与/etc/**相同 -/etc/shadow 请注意，不允许使用空的替代条目，即。{^shadow,}` 9.1 文件通配示例 /dir/file - match a specific file /dir/* - match any files in a directory (including dot files) /dir/a* - match any file in a directory starting with 'a' /dir/*.png - match any file in a directory ending with '.png' /dir/[^.]* - 匹配一个目录中除点文件以外的任何文件 /dir/ - match a directory /dir/*/ - match any directory within /dir/ /dir/a*/ - match any directory within /dir/ starting with a /dir/*a/ - match any directory within /dir/ ending with a /dir/** - 匹配/dir/中或之下的任何文件或目录 /dir/**/ - 匹配 /dir/ 中或之下的任何目录 /dir/**[^/] - 匹配/dir/目录中或之下的任何文件 /dir{,1,2}/** - match any file or directory in or below /dir/, /dir1/, and /dir2/ 9.2 文件权限 支持以下文件权限： r - 读 w - 写 a - 附加（由 w 暗示） x - 执行 ux - 执行无限制（保留环境） - 警告：只应在非常特殊的情况下使用 Ux - 无限制执行（清理环境） px - 在特定配置文件下执行（保留环境） - 警告：仅应在特殊情况下使用 Px - 在特定配置文件下执行（清理环境） pix - 为 px，但如果找不到目标配置文件，则回退到继承当前配置文件 Pix - 作为 Px，但如果找不到目标配置文件，则回退到继承当前配置文件 pux - 作为 px 但如果找不到目标配置文件则回退到执行 unconfined Pux - 作为 Px，但如果找不到目标配置文件，则回退到执行 unconfined ix - 执行并继承当前配置文件 cx - 执行并转换到子配置文件（保护环境） Cx - 执行并转换到子配置文件（清理环境） cix - 作为 cx，但如果找不到目标配置文件，则回退到继承当前配置文件 Cix - 作为 Cx，但如果找不到目标配置文件，则回退到继承当前配置文件 cux - 作为 cx 但如果找不到目标配置文件则回退到执行 unconfined Cux - 与 Cx 相同，但如果未找到目标配置文件，则回退到执行 unconfined m - 内存映射可执行文件 k - 锁定（需要 r 或 w，AppArmor 2.1 及更高版本） l - 链接 owner 关键字可以用作限定词，使权限以拥有文件为条件（进程 fsuid == 文件的 uid）。 owner /foo rw, 以下内容正在开发中： 创建（由 w 暗示） 删除（由 w 暗示） chown - 更改所有权（由 w 暗示） chmod - 改变模式（由 w 暗示） 创建和/或删除文件的权限是： /foo/bar w, 复制文件的权限是： /foo/src r, /foo/dst w, 移动文件的权限是： /foo/src rw, /foo/dst w, 9.3 执行权限 AppArmor 区分文件执行的不同方式。因为在执行文件时会创建一个新进程，所以可以说该进程在执行过程中转换到另一个（可能相同）配置文件。 基本执行权限是： ix - 新进程应该在当前配置文件下运行 cx - 新进程应在与可执行文件名称匹配的子配置文件下运行 px - 新进程应在与可执行文件名称匹配的另一个配置文件下运行 ux - 新进程应该不受限制地运行 一个无限制运行的进程实际上是在内置的无限制配置文件中，它允许一切没有日志记录。 使用大写前导字符（ Px、Cx、Ux ）编写的 px、cx 和 ux 权限将触发 libc 的安全执行。开发配置文件时，通常应使用安全执行变体，以便执行的程序在干净的环境中启动。 px 和 cx 规则（及其安全执行变体）也可能具有 ix 或 ux 后备，表示为 pix、pux、cix 或 cux。使用回退表示如果存在配置文件，则该进程应在​​配置文件下运行，否则配置文件转换应使用指定的 ix 或 ux 转换。使用“PUx”而不是“Ux”通常是个好主意，这样当执行的程序稍后添加 AppArmor 配置文件时，您不必更新配置文件。例如，如果应允许受限程序运行“evince”，则配置文件可能具有： /usr/bin/evince PUx, px 和 cx 规则（及其所有变体）也可以修改为按名称指定配置文件，而不是使用与可执行文件名称匹配的配置文件。这是通过提供->转换箭头和配置文件的名称来完成的。 /foo px -> profile1, 对于目录，UNIX 执行权限映射到搜索访问，AppArmor 不会进一步控制目录搜索访问。换句话说，如果 DAC 允许，则允许遍历目录。 9.4 AppArmor 文件标签 AppArmor 将为文件分配一个默认标签（而不是将该标签存储在文件的 inode 中）。当一个进程打开一个文件时，文件对象被分配一个标签，它可以被认为是配置文件的名称。当不同的进程想要访问同一个文件时，一个文件可以有多个标签，以允许不同的进程具有不同的访问控制。实际上，在开发策略时，只需按名称引用文件，内核在幕后处理所有必要的标签。因此，AppArmor 通常被称为“基于路径名”。 由于 AppArmor 按路径名标记文件（而不是在磁盘上标记），因此管理员无需在文件被覆盖或移动后执行重新标记步骤。例如，如果一个进程被授予对 /etc/shadow 的读取权限并且系统管理员将 /etc/shadow 重命名为 /etc/shadow.old 并用一个副本替换它（例如，其中可能有一个额外的用户） ，该进程将有权访问新的 /etc/shadow，而不是 /etc/shadow.old。 10. Rule 修饰符 当某个资源没有对应的规则时，AppArmor 会阻止对该资源的访问并记录下来。当策略中有规则时，允许访问资源而无需记录。可以在规则前添加以下修饰符来更改此行为： audit：强制记录 deny：明确拒绝，不记录 audit deny：明确拒绝的组合，但记录 实例 /profile { /path/to/file* r, # allow read to /path/to/file* /path/to/file1 w, # allow write to /path/to/file1 deny /path/to/file2, w, # deny write to /path/to/file2, without logging audit /path/to/file3 w, # allow write to /path/to/file3, with logging audit deny /path/to/file4 r, # deny read to /path/to/file4, with logging } 重要提示：拒绝规则在允许规则之前进行评估，并且不能被允许规则覆盖。它们通常用于覆盖文件通配规则。例如，在上述策略中，上面的“audit deny /path/to/file4 r”规则会覆盖“/path/to/file* r”规则。 参考： apparmor Documation How to create an AppArmor Profile Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-11 09:49:41 "},"安全/Trivy/":{"url":"安全/Trivy/","title":"Trivy","keywords":"","body":"trivy 漏洞扫描工具安装1. 简介2. 特征3. 安装3.1 RHEL/CentOS3.2 Debian/Ubuntu3.3 Arch Linux3.4 Homebrew3.5 脚本安装3.6 Docker3.7 Helmtrivy 漏洞扫描工具安装 tagsstart 安全 trivy tagsstop 1. 简介 Trivy 是一个简单而全面的漏洞/错误配置/秘密扫描器，用于容器和其他工件。 检测操作系统包（Alpine、RHEL、CentOS 等）和特定语言包（Bundler、Composer、npm、yarn 等）的漏洞。此外，扫描Terraform 和 Kubernetes 等基础架构即代码 (IaC) 文件，以检测使您的部署面临攻击风险的潜在配置问题。 还扫描硬编码的秘密vyTrivyTrivyTrivy比如密码、API 密钥和令牌。 Trivy易于使用。只需安装二进制文件，您就可以扫描了。扫描所需要做的就是指定一个目标，例如容器的image名称 Trivy 检测到两种类型的安全问题： 漏洞 错误配置 Trivy 可以扫描四种不同的工件： 容器镜像 文件系统和Rootfs Git 存储库 Kubernetes Trivy 可以在两种不同的模式下运行： Standalone Client/Server Trivy 可以作为 Kubernetes Operator 运行： Kubernetes Operator 2. 特征 全面的漏洞检测 操作系统包（Alpine、Red Hat Universal Base Image、Red Hat Enterprise Linux、CentOS、AlmaLinux、Rocky Linux、CBL-Mariner、Oracle Linux、Debian、Ubuntu、Amazon Linux、openSUSE Leap、SUSE Enterprise Linux、Photon OS 和 Distroless） 特定于语言的包（Bundler、Composer、Pipenv、Poetry、npm、yarn、pnpm、Cargo、NuGet、Maven 和 Go） 检测 IaC 错误配置 开箱即用地提供了多种内置策略： Kubernetes docker Terraform 支持自定义策略 简单 仅指定镜像名称、包含 IaC 配置的目录或工件名称 快速 第一次扫描将在 10 秒内完成（取决于您的网络）。随后的扫描将在几秒钟内完成。 与其他扫描程序在第一次运行时需要很长时间（约 10 分钟）获取漏洞信息并鼓励您维护持久的漏洞数据库不同，Trivy 是无状态的，不需要维护或准备。 简易安装 apt-get install，yum install并且brew install是可能的（参见安装） 没有先决条件，例如安装数据库、库等。 高准确率 尤其是 Alpine Linux 和 RHEL/CentOS 其他操作系统也很高 DevSecOps 适用于Travis CI、CircleCI、Jenkins、GitLab CI 等 CI。 请参阅CI 示例 支持多种格式 容器图像 Docker Engine 中作为守护进程运行的本地映像 Podman (>=2.0) 中的本地图像暴露了一个套接字 Docker Registry 中的远程镜像，例如 Docker Hub、ECR、GCR 和 ACR 存储在docker save/podman save格式文件中的 tar 存档 符合OCI 图像格式的图像目录 本地文件系统和 rootfs 远程 git 仓库 SBOM（软件物料清单）支持 CycloneDX SPDX GitHub Dependency Snapshots 3. 安装 3.1 RHEL/CentOS yum RELEASE_VERSION=$(grep -Po '(? rpm rpm -ivh https://github.com/aquasecurity/trivy/releases/download/v0.30.4/trivy_0.30.4_Linux-64bit.rpm 3.2 Debian/Ubuntu apt源 sudo apt-get install wget apt-transport-https gnupg lsb-release wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add - echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install trivy deb包 wget https://github.com/aquasecurity/trivy/releases/download/v0.30.4/trivy_0.30.4_Linux-64bit.deb sudo dpkg -i trivy_0.30.4_Linux-64bit.deb 3.3 Arch Linux pikaur pikaur -Sy trivy-bin yay yay -Sy trivy-bin 3.4 Homebrew brew install aquasecurity/trivy/trivy 3.5 脚本安装 curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.30.4 3.6 Docker docker pull aquasec/trivy:0.30.4 docker pull ghcr.io/aquasecurity/trivy:0.30.4 docker pull public.ecr.aws/aquasecurity/trivy:0.30.4 linux docker run --rm -v [YOUR_CACHE_DIR]:/root/.cache/ aquasec/trivy:0.30.4 image [YOUR_IMAGE_NAME] macOS docker run --rm -v $HOME/Library/Caches:/root/.cache/ aquasec/trivy:0.30.4 image [YOUR_IMAGE_NAME 实例 docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ -v $HOME/Library/Caches:/root/.cache/ aquasec/trivy:0.30.4 python:3.4-alpine 3.7 Helm helm repo add aquasecurity https://aquasecurity.github.io/helm-charts/ helm repo update helm search repo trivy helm install my-trivy aquasecurity/trivy 使用发布名称安装图表my-release： helm install my-release . 该命令以默认配置在 Kubernetes 集群上部署 Trivy。参数 部分列出了可以在安装期间配置的参数。 示例 $ helm install my-release . \\ --namespace my-namespace \\ --set \"service.port=9090\" \\ --set \"trivy.vulnType=os\\,library\" 参考： trivy 官方 ` Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:17:48 "},"安全/Trivy/Kubernetes-Security-trivy-scan.html":{"url":"安全/Trivy/Kubernetes-Security-trivy-scan.html","title":"扫描应用","keywords":"","body":"trivy 工具漏洞扫描应用1. 扫描镜像2. 嵌入 Dockerfile 扫描3. 扫描文件系统3.1 独立模式3.2 client/server3. 扫描 Rootfs4. 扫描 git 仓库4.1 扫描您的远程 git 存储库4.2 扫描分支4.3 扫描到 Commit4.4 扫描标签4.5 扫描私有存储库5. 扫描错误配置6. 类型检测trivy 工具漏洞扫描应用 tagsstart trivy 安全 tagsstop Trivy 是一个简单而全面的漏洞/错误配置/秘密扫描器，用于容器和其他工件。 检测操作系统包（Alpine、RHEL、CentOS 等）和特定语言包（Bundler、Composer、npm、yarn 等）的漏洞。此外，扫描Terraform 和 Kubernetes 等基础架构即代码 (IaC) 文件，以检测使您的部署面临攻击风险的潜在配置问题。 还扫描硬编码的秘密vyTrivyTrivyTrivy比如密码、API 密钥和令牌。 Trivy易于使用。 1. 扫描镜像 trivy image nginx:1.18.0 trivy image --severity CRITICAL nginx:1.18.0 trivy image --severity CRITICAL, HIGH nginx:1.18.0 trivy image --ignore-unfixed nginx:1.18.0 # Scanning image tarball docker save nginx:1.18.0 > nginx.tar trivy image --input archive.tar # Scan and output results to file trivy image --output python_alpine.txt python:3.10.0a4-alpine trivy image --severity HIGH --output /root/python.txt python:3.10.0a4-alpine # Scan image tarball trivy image --input alpine.tar --format json --output /root/alpine.json 扫描解压镜像文件系统 $ docker export $(docker create alpine:3.10.2) | tar -C /tmp/rootfs -xvf - $ trivy rootfs /tmp/rootfs 2. 嵌入 Dockerfile 扫描 通过将 Trivy 嵌入 Dockerfile 来扫描您的图像作为构建过程的一部分。这种方法可用于更新当前使用 Aqua 的Microscanner的 Dockerfile $ cat Dockerfile FROM alpine:3.7 RUN apk add curl \\ && curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin \\ && trivy rootfs --exit-code 1 --no-progress / $ docker build -t vulnerable-image . 或者，您可以在多阶段构建中使用 Trivy。从而避免了不安全curl | sh。图像也没有改变。 [...] # Run vulnerability scan on build image FROM build AS vulnscan COPY --from=aquasec/trivy:latest /usr/local/bin/trivy /usr/local/bin/trivy RUN trivy rootfs --exit-code 1 --no-progress / [...] 3. 扫描文件系统 3.1 独立模式 本地项目 trivy fs /path/to/project trivy fs ~/src/github.com/aquasecurity/trivy-ci-test 单个文件 trivy fs ~/src/github.com/aquasecurity/trivy-ci-test/Pipfile.lock 3.2 client/server trivy server trivy fs --server http://localhost:4954 --severity CRITICAL ./integration/testdata/fixtures/fs/pom/ 3. 扫描 Rootfs 扫描根文件系统（例如主机、虚拟机映像或未打包的容器映像文件系统） $ trivy rootfs /path/to/rootfs $ docker run --rm -it alpine:3.11 / # curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin / # trivy rootfs / 4. 扫描 git 仓库 4.1 扫描您的远程 git 存储库 trivy repo https://github.com/knqyf263/trivy-ci-test 4.2 扫描分支 在提供的远程存储库上传递--branch具有有效分支名称的 agrument： $ trivy repo --branch 4.3 扫描到 Commit 在提供的远程存储库上传递--commit具有有效提交哈希的 agrument： $ trivy repo --commit 4.4 扫描标签 在提供的远程存储库上传递--tag带有有效标签的 agrument： $ trivy repo --tag 4.5 扫描私有存储库 为了扫描私有 GitHub 或 GitLab 存储库，必须分别设置环境变量GITHUB_TOKEN或，并使用有权访问正在扫描的私有存储库的有效令牌：GITLAB_TOKEN 环境变量将GITHUB_TOKEN优先于GITLAB_TOKEN，因此如果要扫描私有 GitLab 存储库，则GITHUB_TOKEN必须取消设置。 例如： $ export GITHUB_TOKEN=\"your_private_github_token\" $ trivy repo $ $ # or $ export GITLAB_TOKEN=\"your_private_gitlab_token\" $ trivy repo 5. 扫描错误配置 只需指定一个包含 IaC 文件的目录，例如 Terraform、CloudFormation 和 Dockerfile。 格式：trivy config [YOUR_IaC_DIRECTORY] 实例 $ ls build/ Dockerfile $ trivy config ./build 2022-05-16T13:29:29.952+0100 INFO Detected config files: 1 Dockerfile (dockerfile) ======================= Tests: 23 (SUCCESSES: 22, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) MEDIUM: Specify a tag in the 'FROM' statement for image 'alpine' ══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ When using a 'FROM' statement you should use a specific tag to avoid uncontrolled behavior when the image is updated. See https://avd.aquasec.com/misconfig/ds001 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Dockerfile:1 ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 [ FROM alpine:latest ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 您还可以通过--security-checks config. $ trivy image --security-checks config IMAGE_NAME $ trivy fs --security-checks config /path/to/dir 与config子命令不同image，fs和repo子命令还可以同时扫描漏洞和秘密。您可以指定--security-checks vuln,config,secret启用漏洞和秘密检测以及错误配置检测。 $ ls myapp/ Dockerfile Pipfile.lock $ trivy fs --security-checks vuln,config,secret --severity HIGH,CRITICAL myapp/ 2022-05-16T13:42:21.440+0100 INFO Number of language-specific files: 1 2022-05-16T13:42:21.440+0100 INFO Detecting pipenv vulnerabilities... 2022-05-16T13:42:21.440+0100 INFO Detected config files: 1 Pipfile.lock (pipenv) ===================== Total: 1 (HIGH: 1, CRITICAL: 0) ┌──────────┬────────────────┬──────────┬───────────────────┬───────────────┬───────────────────────────────────────────────────────────┐ │ Library │ Vulnerability │ Severity │ Installed Version │ Fixed Version │ Title │ ├──────────┼────────────────┼──────────┼───────────────────┼───────────────┼───────────────────────────────────────────────────────────┤ │ httplib2 │ CVE-2021-21240 │ HIGH │ 0.12.1 │ 0.19.0 │ python-httplib2: Regular expression denial of service via │ │ │ │ │ │ │ malicious header │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2021-21240 │ └──────────┴────────────────┴──────────┴───────────────────┴───────────────┴───────────────────────────────────────────────────────────┘ Dockerfile (dockerfile) ======================= Tests: 17 (SUCCESSES: 16, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (HIGH: 1, CRITICAL: 0) HIGH: Last USER command in Dockerfile should not be 'root' ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Running containers with 'root' user can lead to a container escape situation. It is a best practice to run containers as non-root users, which can be done by adding a 'USER' statement to the Dockerfile. See https://avd.aquasec.com/misconfig/ds002 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Dockerfile:3 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 3 [ USER root ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 在上面的示例中，Trivy 检测到了 Python 依赖项的漏洞和 Dockerfile 中的错误配置。 6. 类型检测 指定目录可以包含混合类型的 IaC 文件。Trivy 自动检测配置类型并应用相关策略。 例如，以下示例将 Terraform、CloudFormation、Kubernetes、Helm Charts 和 Dockerfile 的 IaC 文件保存在同一目录中。 $ ls iac/ Dockerfile deployment.yaml main.tf mysql-8.8.26.tar $ trivy conf --severity HIGH,CRITICAL ./iac 输出： Dockerfile (dockerfile) ======================= Tests: 23 (SUCCESSES: 22, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (HIGH: 1, CRITICAL: 0) ... deployment.yaml (kubernetes) ============================ Tests: 28 (SUCCESSES: 15, FAILURES: 13, EXCEPTIONS: 0) Failures: 13 (MEDIUM: 4, HIGH: 1, CRITICAL: 0) ... main.tf (terraform) =================== Tests: 23 (SUCCESSES: 14, FAILURES: 9, EXCEPTIONS: 0) Failures: 9 (HIGH: 6, CRITICAL: 1) ... bucket.yaml (cloudformation) ============================ Tests: 9 (SUCCESSES: 3, FAILURES: 6, EXCEPTIONS: 0) Failures: 6 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 4, CRITICAL: 0) ... mysql-8.8.26.tar:templates/primary/statefulset.yaml (helm) ========================================================== Tests: 20 (SUCCESSES: 18, FAILURES: 2, EXCEPTIONS: 0) Failures: 2 (MEDIUM: 2, HIGH: 0, CRITICAL: 0) 参考： Trivy 官方 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:18:13 "},"安全/Trivy/Kubernetes-Security-trivy-defined-Policy.html":{"url":"安全/Trivy/Kubernetes-Security-trivy-defined-Policy.html","title":"自定义策略","keywords":"","body":"trivy 自定义扫描策略1. 简介2. 文件格式3. 配置语言4. Rego 格式5. rego 结构5.1 Package5.2 Metadata5.3 Input5.4 Return value6. 自定义 data6. rego 测试7. debug 策略trivy 自定义扫描策略 tagsstart trivy 安全 tagsstop 1. 简介 您可以在Rego中编写自定义策略。完成编写自定义策略后，您可以通过选项传递存储这些策略的目录--policy。 trivy conf --policy /path/to/custom_policies --namespaces user /path/to/config_dir 2. 文件格式 如果文件名与以下文件模式匹配，Trivy 将解析该文件并将其作为输入传递给您的 Rego 策略。 | 文件格式 | 文件模式 | |------------|--------------------------------------------------| | JSON | .json | | YAML | .yaml和.yml | | Dockerfile | Dockerfile, Dockerfile., 和.Dockerfile | | Containerfile | Containerfile, Containerfile., 和.Containerfile | | Terraform | .tf和*.tf.json | 3. 配置语言 在上述通用文件格式中，Trivy 会自动识别以下类型的配置文件： CloudFormation (JSON/YAML) Kubernetes (JSON/YAML) Helm (YAML) Terraform Plan (JSON) 4. Rego 格式 一个包必须只包含一个策略。 package user.kubernetes.ID001 import lib.result __rego_metadata__ := { \"id\": \"ID001\", \"title\": \"Deployment not allowed\", \"severity\": \"LOW\", \"description\": \"Deployments are not allowed because of some reasons.\", } __rego_input__ := { \"selector\": [ {\"type\": \"kubernetes\"}, ], } deny[res] { input.kind == \"Deployment\" msg := sprintf(\"Found deployment '%s' but deployments are not allowed\", [input.metadata.name]) res := result.new(msg, input) } 在此示例中，ID001“不允许部署”在 下定义user.kubernetes.ID001。如果添加新的自定义策略，则必须在新包下定义它，例如user.kubernetes.ID002. 5. rego 结构 package（必需的） 必须遵循 Rego 的规范 每个政策必须是唯一的 应包含唯一性的策略 ID 可以包括组名，例如kubernetes为了清楚起见 组名对策略评估没有影响 import data.lib.result（可选的） 如果您想用行号和代码突出显示来修饰您的结果，可以定义 __rego_metadata__（可选的） 为了清楚起见，应该定义这些值，因为这些值将显示在扫描结果中 __rego_input__（可选的） 当你想指定输入格式时可以定义 deny（必需的） 应该是deny或开始于deny 尽管warn, `warn*,violation,violation也适用于兼容性，但deny建议使用严重性，因为可以_rego_metadata. 应返回以下之一： 调用的结果result.new(msg, cause)。msg是string描述问题发生的描述，cause是发生问题的属性/对象。提供这一点允许 Trivy 确定行号并突出显示输出中的代码。 A string表示检测到的问题 虽然objectwithmsg字段被接受，但其他字段被丢弃，string如果result.new()不使用，建议使用。 例如{\"msg\": \"deny message\", \"details\": \"something\"}` 5.1 Package user.kubernetes.ID001 默认情况下，只会builtin.*评估包。如果您定义自定义包，则必须通过--namespaces选项指定包前缀。 trivy conf --policy /path/to/custom_policies --namespaces user /path/to/config_dir 在这种情况下，user.*将进行评估。允许使用任何包前缀，例如main和user 5.2 Metadata 元数据有助于用有用的信息丰富 Trivy 的扫描结果。 __rego_metadata__ := { \"id\": \"ID001\", \"title\": \"Deployment not allowed\", \"severity\": \"LOW\", \"description\": \"Deployments are not allowed because of some reasons.\", \"recommended_actions\": \"Remove Deployment\", \"url\": \"https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits\", } 下面的所有字段__rego_metadata__都是可选的。 字段名称 允许值 默认值 在表中 在 JSON 中 id 任何字符 不适用 yes yes title 任何字符 不适用 yes yes severity LOW, MEDIUM, HIGH,CRITICAL 未知 yes yes description 任何字符 no yes recommended_actions 任何字符 no yes url 任何字符 no yes 某些字段会显示在扫描结果中。 k.yaml (kubernetes) ─────────────────── Tests: 32 (SUCCESSES: 31, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0) LOW: Found deployment 'my-deployment' but deployments are not allowed ════════════════════════════════════════════════════════════════════════ Deployments are not allowed because of some reasons. ──────────────────────────────────────────────────────────────────────── k.yaml:1-2 ──────────────────────────────────────────────────────────────────────── 1 ┌ apiVersion: v1 2 └ kind: Deployment ──────────────────────────────────────────────────────────────────────── 5.3 Input 您可以通过 指定输入格式__rego_input__。下面的所有字段__rego_input都是可选的。 __rego_input__ := { \"combine\": false, \"selector\": [ {\"type\": \"kubernetes\"}, ], } combine (boolean)设置为 true 时，指定目录下的所有配置文件合并为一个输入数据结构。 selector (array)此选项按文件格式或配置语言过滤输入。在上面的示例中，Trivy 仅将 Kubernetes 文件传递​​给此策略。即使指定目录中存在 Dockerfile，它也不会作为输入传递给策略。当 Kubernetes 等配置语言未识别时，会使用 JSON 等文件格式作为type. 当识别出一种配置语言时，它将覆盖type. type接受kubernetes, dockerfile, cloudformation, terraform，terraformplan, json, 或yaml. 在“combine”模式下，input文档变成一个数组，其中每个元素都是一个具有两个字段的对象： \"path\": \"path/to/file\"：相应文件的相对文件路径 \"contents\": ...：相应文件的解析内容 现在，您可以确保重复值在整个配置文件中匹配。 5.4 Return value 在“combine”模式下，deny入口点必须返回一个带有两个键的对象 filepath（必需的）：正在评估的文件的相对文件路径 msg（必需的）：描述问题的消息 deny[res] { resource := input[i].contents ... some logic ... res := { \"filepath\": input[i].path, \"msg\": \"something bad\", } } 6. 自定义 data 自定义策略可能需要额外的数据才能确定答案。 例如，允许创建的资源列表。Trivy 允许将路径传递到带有--data标志的数据文件，而不是在策略中硬编码这些信息。 给定以下 yaml 文件 $ cd examples/misconf/custom-data $ cat data/ports.yaml [~/src/github.com/aquasecurity/trivy/examples/misconf/custom-data] services: ports: - \"20\" - \"20/tcp\" - \"20/udp\" - \"23\" - \"23/tcp\" 这可以导入您的策略： import data.services ports := services.ports 然后，您需要通过--data选项传递数据路径。Trivy 递归搜索 JSON ( *.json) 和 YAML ( *.yaml) 文件的指定路径。 $ trivy conf --policy ./policy --data data --namespaces user ./configs 6. rego 测试 为了帮助您验证自定义策略的正确性，OPA 为您提供了一个框架，您可以使用该框架为您的策略编写测试。通过为您的自定义策略编写测试，您可以加快新规则的开发过程，并减少随着需求的发展而修改规则所需的时间。 有关更多详细信息，请参阅策略测试。 Trivy 的核心库Fanal可以作为 Go 库导入。您可以扫描 Go 中的配置文件并使用 Go 的测试方法（例如表驱动测试）测试您的自定义策略。这允许您使用实际的配置文件作为输入，从而轻松准备测试数据并确保您的自定义策略在实践中有效。 特别是 Dockerfile 和 HCL 需要转换为结构化数据作为输入，这可能与预期的输入格式不同。 我们建议同时编写 OPA 和 Go 测试，因为它们具有不同的角色，例如单元测试和集成测试。 以下示例将允许和拒绝的配置文件存储在目录中。 Successes包含成功的结果，也Failures包含失败的结果。 { name: \"disallowed ports\", input: \"configs/\", fields: fields{ policyPaths: []string{\"policy\"}, dataPaths: []string{\"data\"}, namespaces: []string{\"user\"}, }, want: []types.Misconfiguration{ { FileType: types.Dockerfile, FilePath: \"Dockerfile.allowed\", Successes: types.MisconfResults{ { Namespace: \"user.dockerfile.ID002\", PolicyMetadata: types.PolicyMetadata{ ID: \"ID002\", Type: \"Docker Custom Check\", Title: \"Disallowed ports exposed\", Severity: \"HIGH\", }, }, }, }, { FileType: types.Dockerfile, FilePath: \"Dockerfile.denied\", Failures: types.MisconfResults{ { Namespace: \"user.dockerfile.ID002\", Message: \"Port 23 should not be exposed\", PolicyMetadata: types.PolicyMetadata{ ID: \"ID002\", Type: \"Docker Custom Check\", Title: \"Disallowed ports exposed\", Severity: \"HIGH\", }, }, }, }, }, }, Dockerfile.allowed有一个成功的结果Successes，而Dockerfile.denied有一个失败的结果Failures。 7. debug 策略 在处理更复杂的查询时（或在学习 Rego 时），准确了解策略是如何应用的很有用。为此，您可以使用该--trace标志。这将从 Open Policy Agent 输出一个大的跟踪，只有失败的策略才会显示痕迹。如果要调试已通过的策略，则需要故意使其失败。如下所示： $ trivy conf --trace configs/ 2022-05-16T13:47:58.853+0100 INFO Detected config files: 1 Dockerfile (dockerfile) ======================= Tests: 23 (SUCCESSES: 21, FAILURES: 2, EXCEPTIONS: 0) Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0) MEDIUM: Specify a tag in the 'FROM' statement for image 'alpine' ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ When using a 'FROM' statement you should use a specific tag to avoid uncontrolled behavior when the image is updated. See https://avd.aquasec.com/misconfig/ds001 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Dockerfile:1 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 [ FROM alpine:latest ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── HIGH: Last USER command in Dockerfile should not be 'root' ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Running containers with 'root' user can lead to a container escape situation. It is a best practice to run containers as non-root users, which can be done by adding a 'USER' statement to the Dockerfile. See https://avd.aquasec.com/misconfig/ds002 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Dockerfile:3 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 3 [ USER root ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ID: DS001 File: Dockerfile Namespace: builtin.dockerfile.DS001 Query: data.builtin.dockerfile.DS001.deny Message: Specify a tag in the 'FROM' statement for image 'alpine' TRACE Enter data.builtin.dockerfile.DS001.deny = _ TRACE | Eval data.builtin.dockerfile.DS001.deny = _ TRACE | Index data.builtin.dockerfile.DS001.deny (matched 1 rule) TRACE | Enter data.builtin.dockerfile.DS001.deny TRACE | | Eval output = data.builtin.dockerfile.DS001.fail_latest[_] TRACE | | Index data.builtin.dockerfile.DS001.fail_latest (matched 1 rule) TRACE | | Enter data.builtin.dockerfile.DS001.fail_latest TRACE | | | Eval output = data.builtin.dockerfile.DS001.image_tags[_] TRACE | | | Index data.builtin.dockerfile.DS001.image_tags (matched 2 rules) TRACE | | | Enter data.builtin.dockerfile.DS001.image_tags TRACE | | | | Eval from = data.lib.docker.from[_] TRACE | | | | Index data.lib.docker.from (matched 1 rule) TRACE | | | | Enter data.lib.docker.from TRACE | | | | | Eval instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"from\" TRACE | | | | | Exit data.lib.docker.from TRACE | | | | Redo data.lib.docker.from TRACE | | | | | Redo instruction.Cmd = \"from\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"from\" TRACE | | | | | Fail instruction.Cmd = \"from\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"from\" TRACE | | | | | Fail instruction.Cmd = \"from\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | Eval name = from.Value[0] TRACE | | | | Eval not startswith(name, \"$\") TRACE | | | | Enter startswith(name, \"$\") TRACE | | | | | Eval startswith(name, \"$\") TRACE | | | | | Fail startswith(name, \"$\") TRACE | | | | Eval data.builtin.dockerfile.DS001.parse_tag(name, __local505__) TRACE | | | | Index data.builtin.dockerfile.DS001.parse_tag (matched 2 rules) TRACE | | | | Enter data.builtin.dockerfile.DS001.parse_tag TRACE | | | | | Eval split(name, \":\", __local504__) TRACE | | | | | Eval [img, tag] = __local504__ TRACE | | | | | Exit data.builtin.dockerfile.DS001.parse_tag TRACE | | | | Eval [img, tag] = __local505__ TRACE | | | | Eval output = {\"cmd\": from, \"img\": img, \"tag\": tag} TRACE | | | | Exit data.builtin.dockerfile.DS001.image_tags TRACE | | | Redo data.builtin.dockerfile.DS001.image_tags TRACE | | | | Redo output = {\"cmd\": from, \"img\": img, \"tag\": tag} TRACE | | | | Redo [img, tag] = __local505__ TRACE | | | | Redo data.builtin.dockerfile.DS001.parse_tag(name, __local505__) TRACE | | | | Redo data.builtin.dockerfile.DS001.parse_tag TRACE | | | | | Redo [img, tag] = __local504__ TRACE | | | | | Redo split(name, \":\", __local504__) TRACE | | | | Enter data.builtin.dockerfile.DS001.parse_tag TRACE | | | | | Eval tag = \"latest\" TRACE | | | | | Eval not contains(img, \":\") TRACE | | | | | Enter contains(img, \":\") TRACE | | | | | | Eval contains(img, \":\") TRACE | | | | | | Exit contains(img, \":\") TRACE | | | | | Redo contains(img, \":\") TRACE | | | | | | Redo contains(img, \":\") TRACE | | | | | Fail not contains(img, \":\") TRACE | | | | | Redo tag = \"latest\" TRACE | | | | Redo name = from.Value[0] TRACE | | | | Redo from = data.lib.docker.from[_] TRACE | | | Enter data.builtin.dockerfile.DS001.image_tags TRACE | | | | Eval from = data.lib.docker.from[i] TRACE | | | | Index data.lib.docker.from (matched 1 rule) TRACE | | | | Eval name = from.Value[0] TRACE | | | | Eval cmd_obj = input.stages[j][k] TRACE | | | | Eval possibilities = {\"arg\", \"env\"} TRACE | | | | Eval cmd_obj.Cmd = possibilities[l] TRACE | | | | Fail cmd_obj.Cmd = possibilities[l] TRACE | | | | Redo possibilities = {\"arg\", \"env\"} TRACE | | | | Redo cmd_obj = input.stages[j][k] TRACE | | | | Eval possibilities = {\"arg\", \"env\"} TRACE | | | | Eval cmd_obj.Cmd = possibilities[l] TRACE | | | | Fail cmd_obj.Cmd = possibilities[l] TRACE | | | | Redo possibilities = {\"arg\", \"env\"} TRACE | | | | Redo cmd_obj = input.stages[j][k] TRACE | | | | Eval possibilities = {\"arg\", \"env\"} TRACE | | | | Eval cmd_obj.Cmd = possibilities[l] TRACE | | | | Fail cmd_obj.Cmd = possibilities[l] TRACE | | | | Redo possibilities = {\"arg\", \"env\"} TRACE | | | | Redo cmd_obj = input.stages[j][k] TRACE | | | | Redo name = from.Value[0] TRACE | | | | Redo from = data.lib.docker.from[i] TRACE | | | Eval __local752__ = output.img TRACE | | | Eval neq(__local752__, \"scratch\") TRACE | | | Eval __local753__ = output.img TRACE | | | Eval not data.builtin.dockerfile.DS001.is_alias(__local753__) TRACE | | | Enter data.builtin.dockerfile.DS001.is_alias(__local753__) TRACE | | | | Eval data.builtin.dockerfile.DS001.is_alias(__local753__) TRACE | | | | Index data.builtin.dockerfile.DS001.is_alias (matched 1 rule, early exit) TRACE | | | | Enter data.builtin.dockerfile.DS001.is_alias TRACE | | | | | Eval img = data.builtin.dockerfile.DS001.get_aliases[_] TRACE | | | | | Index data.builtin.dockerfile.DS001.get_aliases (matched 1 rule) TRACE | | | | | Enter data.builtin.dockerfile.DS001.get_aliases TRACE | | | | | | Eval from_cmd = data.lib.docker.from[_] TRACE | | | | | | Index data.lib.docker.from (matched 1 rule) TRACE | | | | | | Eval __local749__ = from_cmd.Value TRACE | | | | | | Eval data.builtin.dockerfile.DS001.get_alias(__local749__, __local503__) TRACE | | | | | | Index data.builtin.dockerfile.DS001.get_alias (matched 1 rule) TRACE | | | | | | Enter data.builtin.dockerfile.DS001.get_alias TRACE | | | | | | | Eval __local748__ = values[i] TRACE | | | | | | | Eval lower(__local748__, __local501__) TRACE | | | | | | | Eval \"as\" = __local501__ TRACE | | | | | | | Fail \"as\" = __local501__ TRACE | | | | | | | Redo lower(__local748__, __local501__) TRACE | | | | | | | Redo __local748__ = values[i] TRACE | | | | | | Fail data.builtin.dockerfile.DS001.get_alias(__local749__, __local503__) TRACE | | | | | | Redo __local749__ = from_cmd.Value TRACE | | | | | | Redo from_cmd = data.lib.docker.from[_] TRACE | | | | | Fail img = data.builtin.dockerfile.DS001.get_aliases[_] TRACE | | | | Fail data.builtin.dockerfile.DS001.is_alias(__local753__) TRACE | | | Eval output.tag = \"latest\" TRACE | | | Exit data.builtin.dockerfile.DS001.fail_latest TRACE | | Redo data.builtin.dockerfile.DS001.fail_latest TRACE | | | Redo output.tag = \"latest\" TRACE | | | Redo __local753__ = output.img TRACE | | | Redo neq(__local752__, \"scratch\") TRACE | | | Redo __local752__ = output.img TRACE | | | Redo output = data.builtin.dockerfile.DS001.image_tags[_] TRACE | | Eval __local754__ = output.img TRACE | | Eval sprintf(\"Specify a tag in the 'FROM' statement for image '%s'\", [__local754__], __local509__) TRACE | | Eval msg = __local509__ TRACE | | Eval __local755__ = output.cmd TRACE | | Eval data.lib.docker.result(msg, __local755__, __local510__) TRACE | | Index data.lib.docker.result (matched 1 rule) TRACE | | Enter data.lib.docker.result TRACE | | | Eval object.get(cmd, \"EndLine\", 0, __local470__) TRACE | | | Eval object.get(cmd, \"Path\", \"\", __local471__) TRACE | | | Eval object.get(cmd, \"StartLine\", 0, __local472__) TRACE | | | Eval result = {\"endline\": __local470__, \"filepath\": __local471__, \"msg\": msg, \"startline\": __local472__} TRACE | | | Exit data.lib.docker.result TRACE | | Eval res = __local510__ TRACE | | Exit data.builtin.dockerfile.DS001.deny TRACE | Redo data.builtin.dockerfile.DS001.deny TRACE | | Redo res = __local510__ TRACE | | Redo data.lib.docker.result(msg, __local755__, __local510__) TRACE | | Redo data.lib.docker.result TRACE | | | Redo result = {\"endline\": __local470__, \"filepath\": __local471__, \"msg\": msg, \"startline\": __local472__} TRACE | | | Redo object.get(cmd, \"StartLine\", 0, __local472__) TRACE | | | Redo object.get(cmd, \"Path\", \"\", __local471__) TRACE | | | Redo object.get(cmd, \"EndLine\", 0, __local470__) TRACE | | Redo __local755__ = output.cmd TRACE | | Redo msg = __local509__ TRACE | | Redo sprintf(\"Specify a tag in the 'FROM' statement for image '%s'\", [__local754__], __local509__) TRACE | | Redo __local754__ = output.img TRACE | | Redo output = data.builtin.dockerfile.DS001.fail_latest[_] TRACE | Exit data.builtin.dockerfile.DS001.deny = _ TRACE Redo data.builtin.dockerfile.DS001.deny = _ TRACE | Redo data.builtin.dockerfile.DS001.deny = _ TRACE ID: DS002 File: Dockerfile Namespace: builtin.dockerfile.DS002 Query: data.builtin.dockerfile.DS002.deny Message: Last USER command in Dockerfile should not be 'root' TRACE Enter data.builtin.dockerfile.DS002.deny = _ TRACE | Eval data.builtin.dockerfile.DS002.deny = _ TRACE | Index data.builtin.dockerfile.DS002.deny (matched 2 rules) TRACE | Enter data.builtin.dockerfile.DS002.deny TRACE | | Eval data.builtin.dockerfile.DS002.fail_user_count TRACE | | Index data.builtin.dockerfile.DS002.fail_user_count (matched 1 rule, early exit) TRACE | | Enter data.builtin.dockerfile.DS002.fail_user_count TRACE | | | Eval __local771__ = data.builtin.dockerfile.DS002.get_user TRACE | | | Index data.builtin.dockerfile.DS002.get_user (matched 1 rule) TRACE | | | Enter data.builtin.dockerfile.DS002.get_user TRACE | | | | Eval user = data.lib.docker.user[_] TRACE | | | | Index data.lib.docker.user (matched 1 rule) TRACE | | | | Enter data.lib.docker.user TRACE | | | | | Eval instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"user\" TRACE | | | | | Fail instruction.Cmd = \"user\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"user\" TRACE | | | | | Exit data.lib.docker.user TRACE | | | | Redo data.lib.docker.user TRACE | | | | | Redo instruction.Cmd = \"user\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | | Eval instruction.Cmd = \"user\" TRACE | | | | | Fail instruction.Cmd = \"user\" TRACE | | | | | Redo instruction = input.stages[_][_] TRACE | | | | Eval username = user.Value[_] TRACE | | | | Exit data.builtin.dockerfile.DS002.get_user TRACE | | | Redo data.builtin.dockerfile.DS002.get_user TRACE | | | | Redo username = user.Value[_] TRACE | | | | Redo user = data.lib.docker.user[_] TRACE | | | Eval count(__local771__, __local536__) TRACE | | | Eval lt(__local536__, 1) TRACE | | | Fail lt(__local536__, 1) TRACE | | | Redo count(__local771__, __local536__) TRACE | | | Redo __local771__ = data.builtin.dockerfile.DS002.get_user TRACE | | Fail data.builtin.dockerfile.DS002.fail_user_count TRACE | Enter data.builtin.dockerfile.DS002.deny TRACE | | Eval cmd = data.builtin.dockerfile.DS002.fail_last_user_root[_] TRACE | | Index data.builtin.dockerfile.DS002.fail_last_user_root (matched 1 rule) TRACE | | Enter data.builtin.dockerfile.DS002.fail_last_user_root TRACE | | | Eval stage_users = data.lib.docker.stage_user[_] TRACE | | | Index data.lib.docker.stage_user (matched 1 rule) TRACE | | | Enter data.lib.docker.stage_user TRACE | | | | Eval stage = input.stages[stage_name] TRACE | | | | Eval users = [cmd | cmd = stage[_]; cmd.Cmd = \"user\"] TRACE | | | | Enter cmd = stage[_]; cmd.Cmd = \"user\" TRACE | | | | | Eval cmd = stage[_] TRACE | | | | | Eval cmd.Cmd = \"user\" TRACE | | | | | Fail cmd.Cmd = \"user\" TRACE | | | | | Redo cmd = stage[_] TRACE | | | | | Eval cmd.Cmd = \"user\" TRACE | | | | | Exit cmd = stage[_]; cmd.Cmd = \"user\" TRACE | | | | Redo cmd = stage[_]; cmd.Cmd = \"user\" TRACE | | | | | Redo cmd.Cmd = \"user\" TRACE | | | | | Redo cmd = stage[_] TRACE | | | | | Eval cmd.Cmd = \"user\" TRACE | | | | | Fail cmd.Cmd = \"user\" TRACE | | | | | Redo cmd = stage[_] TRACE | | | | Exit data.lib.docker.stage_user TRACE | | | Redo data.lib.docker.stage_user TRACE | | | | Redo users = [cmd | cmd = stage[_]; cmd.Cmd = \"user\"] TRACE | | | | Redo stage = input.stages[stage_name] TRACE | | | Eval count(stage_users, __local537__) TRACE | | | Eval len = __local537__ TRACE | | | Eval minus(len, 1, __local538__) TRACE | | | Eval last = stage_users[__local538__] TRACE | | | Eval user = last.Value[0] TRACE | | | Eval user = \"root\" TRACE | | | Exit data.builtin.dockerfile.DS002.fail_last_user_root TRACE | | Redo data.builtin.dockerfile.DS002.fail_last_user_root TRACE | | | Redo user = \"root\" TRACE | | | Redo user = last.Value[0] TRACE | | | Redo last = stage_users[__local538__] TRACE | | | Redo minus(len, 1, __local538__) TRACE | | | Redo len = __local537__ TRACE | | | Redo count(stage_users, __local537__) TRACE | | | Redo stage_users = data.lib.docker.stage_user[_] TRACE | | Eval msg = \"Last USER command in Dockerfile should not be 'root'\" TRACE | | Eval data.lib.docker.result(msg, cmd, __local540__) TRACE | | Index data.lib.docker.result (matched 1 rule) TRACE | | Enter data.lib.docker.result TRACE | | | Eval object.get(cmd, \"EndLine\", 0, __local470__) TRACE | | | Eval object.get(cmd, \"Path\", \"\", __local471__) TRACE | | | Eval object.get(cmd, \"StartLine\", 0, __local472__) TRACE | | | Eval result = {\"endline\": __local470__, \"filepath\": __local471__, \"msg\": msg, \"startline\": __local472__} TRACE | | | Exit data.lib.docker.result TRACE | | Eval res = __local540__ TRACE | | Exit data.builtin.dockerfile.DS002.deny TRACE | Redo data.builtin.dockerfile.DS002.deny TRACE | | Redo res = __local540__ TRACE | | Redo data.lib.docker.result(msg, cmd, __local540__) TRACE | | Redo data.lib.docker.result TRACE | | | Redo result = {\"endline\": __local470__, \"filepath\": __local471__, \"msg\": msg, \"startline\": __local472__} TRACE | | | Redo object.get(cmd, \"StartLine\", 0, __local472__) TRACE | | | Redo object.get(cmd, \"Path\", \"\", __local471__) TRACE | | | Redo object.get(cmd, \"EndLine\", 0, __local470__) TRACE | | Redo msg = \"Last USER command in Dockerfile should not be 'root'\" TRACE | | Redo cmd = data.builtin.dockerfile.DS002.fail_last_user_root[_] TRACE | Exit data.builtin.dockerfile.DS002.deny = _ TRACE Redo data.builtin.dockerfile.DS002.deny = _ TRACE | Redo data.builtin.dockerfile.DS002.deny = _ TRACE 参考： trivy 官方 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-10 09:18:00 "},"tags.html":{"url":"tags.html","title":"标签","keywords":"","body":"标签log资源kubernetes部署minikubeDeploymentclient-go原理DaemonSet对象存储StatefulSetjobcronjobRuntimeClasspodPodSecurityPolicyConfigMapPod探针健康检测secretIngressServiceNetworkPolicyAuditingOPA策略ResourceQuotaLimitRangekubeadm命令kube-proxykubectlkubeletapi-server安全falcoapparmortrivy标签 log 日志 资源 Yum kubernetes minikube 部署 minikube 手动 minikube minikube Deployment Deployment 技巧 开发 策略 原理 client-go 开发 原理 原理 DaemonSet DaemonSet 对象 DaemonSet Volumes StatefulSet Job 原理 PodSecurityPolicy ConfigMap SecurityContext secret Ingress Service NetworkPolicy RBAC 原理 Auditing 存储 Volumes FlexVolume CSI 编写 CSI Volumes 原理 StatefulSet StatefulSet job Job cronjob Job RuntimeClass 原理 pod PodSecurityPolicy QoS PodSecurityPolicy PodSecurityPolicy ConfigMap ConfigMap Pod Probes SecurityContext 探针 Probes 健康检测 Probes secret secret Ingress Ingress Service Service NetworkPolicy NetworkPolicy Auditing Auditing 实战 OPA OPA 策略 OPA ResourceQuota QoS ResourceQuota ResourceQuota LimitRange LimitRange kubeadm 命令 命令 命令 命令 kube-proxy Kube proxy kubectl 命令 kubelet Kubelet 配置 垃圾回收 api-server kube apiserver 安全 安全 default macros 规则 Apparmor 语法 Trivy 扫描应用 自定义策略 falco default macros 规则 apparmor Apparmor 语法 trivy Trivy 扫描应用 自定义策略 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-08-09 13:58:08 "}}